{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca76ed4",
   "metadata": {},
   "source": [
    "## Compute entropy and conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68507032",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(f'Air2UG_alltimeseries_Toolik_MERRA2data.csv',header=0)\n",
    "soil_temp_columns = ['T0','T16','T31','T46','T76','T97']\n",
    "\n",
    "# Extract time info\n",
    "df['Date'] = df['Date'].apply(pd.to_datetime, errors='coerce')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "remote_sensing_features = ['Day','Month','Year'] + ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "\n",
    "# Define a function to calculate entropy\n",
    "def calculate_entropy(variable):\n",
    "    # First we discretize the continuous variables into bins\n",
    "    # The number of bins is set to the square root of the number of unique values, a common heuristic\n",
    "    counts, _ = np.histogram(variable, bins='auto')\n",
    "    # Normalize the counts to get a probability distribution\n",
    "    p = counts / counts.sum()\n",
    "    # Calculate the entropy\n",
    "    entropy = -np.sum(p * np.log2(p + 1e-9))  # add a small value to prevent log(0)\n",
    "    return entropy\n",
    "\n",
    "# Calculate entropy for each remote sensing feature\n",
    "entropies = {feature: calculate_entropy(df[feature]) for feature in soil_temp_columns}\n",
    "\n",
    "# Calculate conditional entropy for soil temperatures given each remote sensing feature\n",
    "# We will use mutual information and the formula H(X|Y) = H(X) - I(X;Y)\n",
    "# where H is entropy, I is mutual information\n",
    "conditional_entropies = {}\n",
    "for soil_temp in soil_temp_columns:\n",
    "    soil_temp_entropy = calculate_entropy(df[soil_temp])\n",
    "    for feature in remote_sensing_features:\n",
    "        mutual_info = mutual_info_regression(df[[feature]], df[soil_temp])[0]\n",
    "        conditional_entropy = soil_temp_entropy - mutual_info\n",
    "        conditional_entropies[(soil_temp, feature)] = conditional_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c9de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tair</th>\n",
       "      <th>T0</th>\n",
       "      <th>T8</th>\n",
       "      <th>T16</th>\n",
       "      <th>T23</th>\n",
       "      <th>T31</th>\n",
       "      <th>T38</th>\n",
       "      <th>T46</th>\n",
       "      <th>T61</th>\n",
       "      <th>...</th>\n",
       "      <th>TLML</th>\n",
       "      <th>TSH</th>\n",
       "      <th>EVPSOIL</th>\n",
       "      <th>LWLAND</th>\n",
       "      <th>TS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>SLP</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>-23.570833</td>\n",
       "      <td>-20.590833</td>\n",
       "      <td>-16.600833</td>\n",
       "      <td>-13.566667</td>\n",
       "      <td>-12.496667</td>\n",
       "      <td>-11.880417</td>\n",
       "      <td>-10.997917</td>\n",
       "      <td>-10.276250</td>\n",
       "      <td>-9.123333</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.564551</td>\n",
       "      <td>-28.508063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-24.548771</td>\n",
       "      <td>-28.508673</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>102747.171875</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>-26.929583</td>\n",
       "      <td>-20.388750</td>\n",
       "      <td>-16.039583</td>\n",
       "      <td>-13.046667</td>\n",
       "      <td>-12.066250</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>-10.781667</td>\n",
       "      <td>-10.152500</td>\n",
       "      <td>-9.127500</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.423911</td>\n",
       "      <td>-29.612433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.873007</td>\n",
       "      <td>-29.612051</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>102558.156250</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>-30.194167</td>\n",
       "      <td>-23.720833</td>\n",
       "      <td>-17.707500</td>\n",
       "      <td>-13.665000</td>\n",
       "      <td>-12.415417</td>\n",
       "      <td>-11.740000</td>\n",
       "      <td>-10.872083</td>\n",
       "      <td>-10.207083</td>\n",
       "      <td>-9.167917</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.514777</td>\n",
       "      <td>-42.817953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.266411</td>\n",
       "      <td>-42.819632</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>102112.085938</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>-32.678333</td>\n",
       "      <td>-25.611250</td>\n",
       "      <td>-18.949167</td>\n",
       "      <td>-14.417500</td>\n",
       "      <td>-13.018333</td>\n",
       "      <td>-12.226667</td>\n",
       "      <td>-11.247083</td>\n",
       "      <td>-10.501250</td>\n",
       "      <td>-9.361667</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.379980</td>\n",
       "      <td>-39.687277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.013476</td>\n",
       "      <td>-39.689627</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>101606.335938</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>-33.580000</td>\n",
       "      <td>-25.935417</td>\n",
       "      <td>-19.502917</td>\n",
       "      <td>-14.986667</td>\n",
       "      <td>-13.555000</td>\n",
       "      <td>-12.720417</td>\n",
       "      <td>-11.680417</td>\n",
       "      <td>-10.876250</td>\n",
       "      <td>-9.651667</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.848532</td>\n",
       "      <td>-37.192175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.391212</td>\n",
       "      <td>-37.193289</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>101824.546875</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       Tair         T0         T8        T16        T23  \\\n",
       "0 2000-01-01 -23.570833 -20.590833 -16.600833 -13.566667 -12.496667   \n",
       "1 2000-01-02 -26.929583 -20.388750 -16.039583 -13.046667 -12.066250   \n",
       "2 2000-01-03 -30.194167 -23.720833 -17.707500 -13.665000 -12.415417   \n",
       "3 2000-01-04 -32.678333 -25.611250 -18.949167 -14.417500 -13.018333   \n",
       "4 2000-01-05 -33.580000 -25.935417 -19.502917 -14.986667 -13.555000   \n",
       "\n",
       "         T31        T38        T46       T61  ...       TLML        TSH  \\\n",
       "0 -11.880417 -10.997917 -10.276250 -9.123333  ... -26.564551 -28.508063   \n",
       "1 -11.540000 -10.781667 -10.152500 -9.127500  ... -26.423911 -29.612433   \n",
       "2 -11.740000 -10.872083 -10.207083 -9.167917  ... -26.514777 -42.817953   \n",
       "3 -12.226667 -11.247083 -10.501250 -9.361667  ... -27.379980 -39.687277   \n",
       "4 -12.720417 -11.680417 -10.876250 -9.651667  ... -28.848532 -37.192175   \n",
       "\n",
       "   EVPSOIL     LWLAND         TS      QV2M            SLP  Year  Month  Day  \n",
       "0      0.0 -24.548771 -28.508673  0.000304  102747.171875  2000      1    1  \n",
       "1      0.0 -19.873007 -29.612051  0.000306  102558.156250  2000      1    2  \n",
       "2      0.0 -21.266411 -42.819632  0.000261  102112.085938  2000      1    3  \n",
       "3      0.0 -13.013476 -39.689627  0.000229  101606.335938  2000      1    4  \n",
       "4      0.0 -14.391212 -37.193289  0.000201  101824.546875  2000      1    5  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the original entropies for ease of plotting\n",
    "df_original_entropies = pd.DataFrame(list(entropies.items()), columns=['Soil Temp', 'Original Entropy'])\n",
    "\n",
    "# Create a DataFrame from the conditional entropies provided for ease of plotting\n",
    "df_conditional_entropies = pd.DataFrame({\n",
    "    'Soil Temp': [k[0] for k in conditional_entropies.keys()],\n",
    "    'Feature': [k[1] for k in conditional_entropies.keys()],\n",
    "    'Conditional Entropy': list(conditional_entropies.values())\n",
    "})\n",
    "\n",
    "# Calculate the reduction in entropy\n",
    "df_conditional_entropies = df_conditional_entropies.join(\n",
    "    df_original_entropies.set_index('Soil Temp'), on='Soil Temp')\n",
    "df_conditional_entropies['Entropy Reduction'] = df_conditional_entropies['Original Entropy'] - df_conditional_entropies['Conditional Entropy']\n",
    "# Export file\n",
    "df_conditional_entropies.to_csv('Entropies_ConditionalEntropies_Toolik.csv', index=False)\n",
    "\n",
    "# Now let's plot the original and conditional entropies\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# Original entropy bar plot\n",
    "sns.barplot(ax=axes[0], x='Soil Temp', y='Original Entropy', data=df_original_entropies, color='skyblue')\n",
    "axes[0].set_title('Original Entropy of Soil Temperatures')\n",
    "axes[0].set_ylabel('Entropy')\n",
    "axes[0].set_xlabel('Soil Temperature')\n",
    "\n",
    "# Entropy reduction bar plot\n",
    "sns.barplot(ax=axes[1], x='Soil Temp', y='Entropy Reduction', hue='Feature', data=df_conditional_entropies, palette='viridis')\n",
    "axes[1].set_title('Reduction in Entropy of Soil Temperatures by Remote Sensing Feature')\n",
    "axes[1].set_ylabel('Entropy Reduction')\n",
    "axes[1].set_xlabel('Soil Temperature')\n",
    "axes[1].legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb46c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a0e77_row8_col1, #T_a0e77_row15_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a0e77\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a0e77_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_a0e77_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a0e77_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_a0e77_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a0e77_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_a0e77_row1_col1\" class=\"data row1 col1\" >T0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a0e77_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_a0e77_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a0e77_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_a0e77_row3_col1\" class=\"data row3 col1\" >(1191, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a0e77_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_a0e77_row4_col1\" class=\"data row4 col1\" >(1191, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a0e77_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_a0e77_row5_col1\" class=\"data row5 col1\" >(833, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a0e77_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_a0e77_row6_col1\" class=\"data row6 col1\" >(358, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a0e77_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_a0e77_row7_col1\" class=\"data row7 col1\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a0e77_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_a0e77_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a0e77_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_a0e77_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a0e77_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_a0e77_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a0e77_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_a0e77_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a0e77_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_a0e77_row12_col1\" class=\"data row12 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a0e77_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
       "      <td id=\"T_a0e77_row13_col1\" class=\"data row13 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a0e77_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_a0e77_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a0e77_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
       "      <td id=\"T_a0e77_row15_col1\" class=\"data row15 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_a0e77_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_a0e77_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_a0e77_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_a0e77_row17_col1\" class=\"data row17 col1\" >reg-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0e77_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_a0e77_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
       "      <td id=\"T_a0e77_row18_col1\" class=\"data row18 col1\" >6720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1554a7463e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla V100-SXM2-16GB, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 0.500000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade orjson\n",
    "# !pip install --upgrade pip setuptools\n",
    "# ! pip install --upgrade jsonschema\n",
    "# !pip install --upgrade platformdirs\n",
    "# !pip install pycaret\n",
    "\n",
    "loc = 'Toolik'\n",
    "sequence_length = 1\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "target= 'T0'\n",
    "res = 30\n",
    "season = 'Winter'\n",
    "_, _, df = prepare_sequences(loc, sequence_length, target, features, season, res)\n",
    "# Split into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "# Split the data chronologically\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "\n",
    "\n",
    "from pycaret.regression import *\n",
    "s = setup(train_df, target = 'T0', session_id = 123, use_gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ff512a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fe4d0 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fe4d0_row0_col0, #T_fe4d0_row1_col0, #T_fe4d0_row1_col1, #T_fe4d0_row1_col2, #T_fe4d0_row1_col3, #T_fe4d0_row1_col4, #T_fe4d0_row1_col5, #T_fe4d0_row1_col6, #T_fe4d0_row2_col0, #T_fe4d0_row2_col1, #T_fe4d0_row2_col2, #T_fe4d0_row2_col3, #T_fe4d0_row2_col4, #T_fe4d0_row2_col5, #T_fe4d0_row2_col6, #T_fe4d0_row3_col0, #T_fe4d0_row3_col1, #T_fe4d0_row3_col2, #T_fe4d0_row3_col3, #T_fe4d0_row3_col4, #T_fe4d0_row3_col5, #T_fe4d0_row3_col6, #T_fe4d0_row4_col0, #T_fe4d0_row4_col1, #T_fe4d0_row4_col2, #T_fe4d0_row4_col3, #T_fe4d0_row4_col4, #T_fe4d0_row4_col5, #T_fe4d0_row4_col6, #T_fe4d0_row5_col0, #T_fe4d0_row5_col1, #T_fe4d0_row5_col2, #T_fe4d0_row5_col3, #T_fe4d0_row5_col4, #T_fe4d0_row5_col5, #T_fe4d0_row5_col6, #T_fe4d0_row6_col0, #T_fe4d0_row6_col1, #T_fe4d0_row6_col2, #T_fe4d0_row6_col3, #T_fe4d0_row6_col4, #T_fe4d0_row6_col5, #T_fe4d0_row6_col6, #T_fe4d0_row7_col0, #T_fe4d0_row7_col1, #T_fe4d0_row7_col2, #T_fe4d0_row7_col3, #T_fe4d0_row7_col4, #T_fe4d0_row7_col5, #T_fe4d0_row7_col6, #T_fe4d0_row8_col0, #T_fe4d0_row8_col1, #T_fe4d0_row8_col2, #T_fe4d0_row8_col3, #T_fe4d0_row8_col4, #T_fe4d0_row8_col5, #T_fe4d0_row8_col6, #T_fe4d0_row9_col0, #T_fe4d0_row9_col1, #T_fe4d0_row9_col2, #T_fe4d0_row9_col3, #T_fe4d0_row9_col4, #T_fe4d0_row9_col5, #T_fe4d0_row9_col6, #T_fe4d0_row10_col0, #T_fe4d0_row10_col1, #T_fe4d0_row10_col2, #T_fe4d0_row10_col3, #T_fe4d0_row10_col4, #T_fe4d0_row10_col5, #T_fe4d0_row10_col6, #T_fe4d0_row11_col0, #T_fe4d0_row11_col1, #T_fe4d0_row11_col2, #T_fe4d0_row11_col3, #T_fe4d0_row11_col4, #T_fe4d0_row11_col5, #T_fe4d0_row11_col6, #T_fe4d0_row12_col0, #T_fe4d0_row12_col1, #T_fe4d0_row12_col2, #T_fe4d0_row12_col3, #T_fe4d0_row12_col4, #T_fe4d0_row12_col5, #T_fe4d0_row12_col6, #T_fe4d0_row13_col0, #T_fe4d0_row13_col1, #T_fe4d0_row13_col2, #T_fe4d0_row13_col3, #T_fe4d0_row13_col4, #T_fe4d0_row13_col5, #T_fe4d0_row13_col6, #T_fe4d0_row14_col0, #T_fe4d0_row14_col1, #T_fe4d0_row14_col2, #T_fe4d0_row14_col3, #T_fe4d0_row14_col4, #T_fe4d0_row14_col5, #T_fe4d0_row14_col6, #T_fe4d0_row15_col0, #T_fe4d0_row15_col1, #T_fe4d0_row15_col2, #T_fe4d0_row15_col3, #T_fe4d0_row15_col4, #T_fe4d0_row15_col5, #T_fe4d0_row15_col6, #T_fe4d0_row16_col0, #T_fe4d0_row16_col1, #T_fe4d0_row16_col2, #T_fe4d0_row16_col3, #T_fe4d0_row16_col4, #T_fe4d0_row16_col5, #T_fe4d0_row16_col6, #T_fe4d0_row17_col0, #T_fe4d0_row17_col1, #T_fe4d0_row17_col2, #T_fe4d0_row17_col3, #T_fe4d0_row17_col4, #T_fe4d0_row17_col5, #T_fe4d0_row17_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fe4d0_row0_col1, #T_fe4d0_row0_col2, #T_fe4d0_row0_col3, #T_fe4d0_row0_col4, #T_fe4d0_row0_col5, #T_fe4d0_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_fe4d0_row0_col7, #T_fe4d0_row1_col7, #T_fe4d0_row2_col7, #T_fe4d0_row3_col7, #T_fe4d0_row4_col7, #T_fe4d0_row5_col7, #T_fe4d0_row6_col7, #T_fe4d0_row7_col7, #T_fe4d0_row8_col7, #T_fe4d0_row9_col7, #T_fe4d0_row10_col7, #T_fe4d0_row11_col7, #T_fe4d0_row12_col7, #T_fe4d0_row13_col7, #T_fe4d0_row15_col7, #T_fe4d0_row16_col7, #T_fe4d0_row17_col7 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_fe4d0_row14_col7 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fe4d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fe4d0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_fe4d0_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_fe4d0_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_fe4d0_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_fe4d0_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_fe4d0_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_fe4d0_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_fe4d0_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row0\" class=\"row_heading level0 row0\" >et</th>\n",
       "      <td id=\"T_fe4d0_row0_col0\" class=\"data row0 col0\" >Extra Trees Regressor</td>\n",
       "      <td id=\"T_fe4d0_row0_col1\" class=\"data row0 col1\" >0.3640</td>\n",
       "      <td id=\"T_fe4d0_row0_col2\" class=\"data row0 col2\" >0.4349</td>\n",
       "      <td id=\"T_fe4d0_row0_col3\" class=\"data row0 col3\" >0.6354</td>\n",
       "      <td id=\"T_fe4d0_row0_col4\" class=\"data row0 col4\" >0.9821</td>\n",
       "      <td id=\"T_fe4d0_row0_col5\" class=\"data row0 col5\" >0.0331</td>\n",
       "      <td id=\"T_fe4d0_row0_col6\" class=\"data row0 col6\" >0.0209</td>\n",
       "      <td id=\"T_fe4d0_row0_col7\" class=\"data row0 col7\" >0.1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row1\" class=\"row_heading level0 row1\" >lightgbm</th>\n",
       "      <td id=\"T_fe4d0_row1_col0\" class=\"data row1 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_fe4d0_row1_col1\" class=\"data row1 col1\" >0.5934</td>\n",
       "      <td id=\"T_fe4d0_row1_col2\" class=\"data row1 col2\" >0.8694</td>\n",
       "      <td id=\"T_fe4d0_row1_col3\" class=\"data row1 col3\" >0.9133</td>\n",
       "      <td id=\"T_fe4d0_row1_col4\" class=\"data row1 col4\" >0.9614</td>\n",
       "      <td id=\"T_fe4d0_row1_col5\" class=\"data row1 col5\" >0.0502</td>\n",
       "      <td id=\"T_fe4d0_row1_col6\" class=\"data row1 col6\" >0.0350</td>\n",
       "      <td id=\"T_fe4d0_row1_col7\" class=\"data row1 col7\" >0.4890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row2\" class=\"row_heading level0 row2\" >rf</th>\n",
       "      <td id=\"T_fe4d0_row2_col0\" class=\"data row2 col0\" >Random Forest Regressor</td>\n",
       "      <td id=\"T_fe4d0_row2_col1\" class=\"data row2 col1\" >0.6158</td>\n",
       "      <td id=\"T_fe4d0_row2_col2\" class=\"data row2 col2\" >0.9703</td>\n",
       "      <td id=\"T_fe4d0_row2_col3\" class=\"data row2 col3\" >0.9668</td>\n",
       "      <td id=\"T_fe4d0_row2_col4\" class=\"data row2 col4\" >0.9587</td>\n",
       "      <td id=\"T_fe4d0_row2_col5\" class=\"data row2 col5\" >0.0532</td>\n",
       "      <td id=\"T_fe4d0_row2_col6\" class=\"data row2 col6\" >0.0364</td>\n",
       "      <td id=\"T_fe4d0_row2_col7\" class=\"data row2 col7\" >0.1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row3\" class=\"row_heading level0 row3\" >gbr</th>\n",
       "      <td id=\"T_fe4d0_row3_col0\" class=\"data row3 col0\" >Gradient Boosting Regressor</td>\n",
       "      <td id=\"T_fe4d0_row3_col1\" class=\"data row3 col1\" >0.9008</td>\n",
       "      <td id=\"T_fe4d0_row3_col2\" class=\"data row3 col2\" >1.5424</td>\n",
       "      <td id=\"T_fe4d0_row3_col3\" class=\"data row3 col3\" >1.2337</td>\n",
       "      <td id=\"T_fe4d0_row3_col4\" class=\"data row3 col4\" >0.9316</td>\n",
       "      <td id=\"T_fe4d0_row3_col5\" class=\"data row3 col5\" >0.0688</td>\n",
       "      <td id=\"T_fe4d0_row3_col6\" class=\"data row3 col6\" >0.0534</td>\n",
       "      <td id=\"T_fe4d0_row3_col7\" class=\"data row3 col7\" >0.1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row4\" class=\"row_heading level0 row4\" >dt</th>\n",
       "      <td id=\"T_fe4d0_row4_col0\" class=\"data row4 col0\" >Decision Tree Regressor</td>\n",
       "      <td id=\"T_fe4d0_row4_col1\" class=\"data row4 col1\" >0.6397</td>\n",
       "      <td id=\"T_fe4d0_row4_col2\" class=\"data row4 col2\" >2.1443</td>\n",
       "      <td id=\"T_fe4d0_row4_col3\" class=\"data row4 col3\" >1.3491</td>\n",
       "      <td id=\"T_fe4d0_row4_col4\" class=\"data row4 col4\" >0.9107</td>\n",
       "      <td id=\"T_fe4d0_row4_col5\" class=\"data row4 col5\" >0.0716</td>\n",
       "      <td id=\"T_fe4d0_row4_col6\" class=\"data row4 col6\" >0.0369</td>\n",
       "      <td id=\"T_fe4d0_row4_col7\" class=\"data row4 col7\" >0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row5\" class=\"row_heading level0 row5\" >ada</th>\n",
       "      <td id=\"T_fe4d0_row5_col0\" class=\"data row5 col0\" >AdaBoost Regressor</td>\n",
       "      <td id=\"T_fe4d0_row5_col1\" class=\"data row5 col1\" >2.0255</td>\n",
       "      <td id=\"T_fe4d0_row5_col2\" class=\"data row5 col2\" >5.7409</td>\n",
       "      <td id=\"T_fe4d0_row5_col3\" class=\"data row5 col3\" >2.3930</td>\n",
       "      <td id=\"T_fe4d0_row5_col4\" class=\"data row5 col4\" >0.7418</td>\n",
       "      <td id=\"T_fe4d0_row5_col5\" class=\"data row5 col5\" >0.1324</td>\n",
       "      <td id=\"T_fe4d0_row5_col6\" class=\"data row5 col6\" >0.1216</td>\n",
       "      <td id=\"T_fe4d0_row5_col7\" class=\"data row5 col7\" >0.0990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row6\" class=\"row_heading level0 row6\" >lr</th>\n",
       "      <td id=\"T_fe4d0_row6_col0\" class=\"data row6 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_fe4d0_row6_col1\" class=\"data row6 col1\" >2.1483</td>\n",
       "      <td id=\"T_fe4d0_row6_col2\" class=\"data row6 col2\" >7.5805</td>\n",
       "      <td id=\"T_fe4d0_row6_col3\" class=\"data row6 col3\" >2.7474</td>\n",
       "      <td id=\"T_fe4d0_row6_col4\" class=\"data row6 col4\" >0.6615</td>\n",
       "      <td id=\"T_fe4d0_row6_col5\" class=\"data row6 col5\" >0.1436</td>\n",
       "      <td id=\"T_fe4d0_row6_col6\" class=\"data row6 col6\" >0.1236</td>\n",
       "      <td id=\"T_fe4d0_row6_col7\" class=\"data row6 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row7\" class=\"row_heading level0 row7\" >br</th>\n",
       "      <td id=\"T_fe4d0_row7_col0\" class=\"data row7 col0\" >Bayesian Ridge</td>\n",
       "      <td id=\"T_fe4d0_row7_col1\" class=\"data row7 col1\" >2.1493</td>\n",
       "      <td id=\"T_fe4d0_row7_col2\" class=\"data row7 col2\" >7.5845</td>\n",
       "      <td id=\"T_fe4d0_row7_col3\" class=\"data row7 col3\" >2.7479</td>\n",
       "      <td id=\"T_fe4d0_row7_col4\" class=\"data row7 col4\" >0.6615</td>\n",
       "      <td id=\"T_fe4d0_row7_col5\" class=\"data row7 col5\" >0.1436</td>\n",
       "      <td id=\"T_fe4d0_row7_col6\" class=\"data row7 col6\" >0.1238</td>\n",
       "      <td id=\"T_fe4d0_row7_col7\" class=\"data row7 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row8\" class=\"row_heading level0 row8\" >ridge</th>\n",
       "      <td id=\"T_fe4d0_row8_col0\" class=\"data row8 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_fe4d0_row8_col1\" class=\"data row8 col1\" >2.1635</td>\n",
       "      <td id=\"T_fe4d0_row8_col2\" class=\"data row8 col2\" >7.6729</td>\n",
       "      <td id=\"T_fe4d0_row8_col3\" class=\"data row8 col3\" >2.7632</td>\n",
       "      <td id=\"T_fe4d0_row8_col4\" class=\"data row8 col4\" >0.6581</td>\n",
       "      <td id=\"T_fe4d0_row8_col5\" class=\"data row8 col5\" >0.1447</td>\n",
       "      <td id=\"T_fe4d0_row8_col6\" class=\"data row8 col6\" >0.1249</td>\n",
       "      <td id=\"T_fe4d0_row8_col7\" class=\"data row8 col7\" >0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row9\" class=\"row_heading level0 row9\" >lasso</th>\n",
       "      <td id=\"T_fe4d0_row9_col0\" class=\"data row9 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_fe4d0_row9_col1\" class=\"data row9 col1\" >2.5148</td>\n",
       "      <td id=\"T_fe4d0_row9_col2\" class=\"data row9 col2\" >9.8475</td>\n",
       "      <td id=\"T_fe4d0_row9_col3\" class=\"data row9 col3\" >3.1279</td>\n",
       "      <td id=\"T_fe4d0_row9_col4\" class=\"data row9 col4\" >0.5615</td>\n",
       "      <td id=\"T_fe4d0_row9_col5\" class=\"data row9 col5\" >0.1684</td>\n",
       "      <td id=\"T_fe4d0_row9_col6\" class=\"data row9 col6\" >0.1489</td>\n",
       "      <td id=\"T_fe4d0_row9_col7\" class=\"data row9 col7\" >0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row10\" class=\"row_heading level0 row10\" >llar</th>\n",
       "      <td id=\"T_fe4d0_row10_col0\" class=\"data row10 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_fe4d0_row10_col1\" class=\"data row10 col1\" >2.5148</td>\n",
       "      <td id=\"T_fe4d0_row10_col2\" class=\"data row10 col2\" >9.8475</td>\n",
       "      <td id=\"T_fe4d0_row10_col3\" class=\"data row10 col3\" >3.1279</td>\n",
       "      <td id=\"T_fe4d0_row10_col4\" class=\"data row10 col4\" >0.5615</td>\n",
       "      <td id=\"T_fe4d0_row10_col5\" class=\"data row10 col5\" >0.1684</td>\n",
       "      <td id=\"T_fe4d0_row10_col6\" class=\"data row10 col6\" >0.1489</td>\n",
       "      <td id=\"T_fe4d0_row10_col7\" class=\"data row10 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row11\" class=\"row_heading level0 row11\" >en</th>\n",
       "      <td id=\"T_fe4d0_row11_col0\" class=\"data row11 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_fe4d0_row11_col1\" class=\"data row11 col1\" >2.5229</td>\n",
       "      <td id=\"T_fe4d0_row11_col2\" class=\"data row11 col2\" >9.9093</td>\n",
       "      <td id=\"T_fe4d0_row11_col3\" class=\"data row11 col3\" >3.1386</td>\n",
       "      <td id=\"T_fe4d0_row11_col4\" class=\"data row11 col4\" >0.5573</td>\n",
       "      <td id=\"T_fe4d0_row11_col5\" class=\"data row11 col5\" >0.1690</td>\n",
       "      <td id=\"T_fe4d0_row11_col6\" class=\"data row11 col6\" >0.1491</td>\n",
       "      <td id=\"T_fe4d0_row11_col7\" class=\"data row11 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row12\" class=\"row_heading level0 row12\" >huber</th>\n",
       "      <td id=\"T_fe4d0_row12_col0\" class=\"data row12 col0\" >Huber Regressor</td>\n",
       "      <td id=\"T_fe4d0_row12_col1\" class=\"data row12 col1\" >3.0468</td>\n",
       "      <td id=\"T_fe4d0_row12_col2\" class=\"data row12 col2\" >15.9299</td>\n",
       "      <td id=\"T_fe4d0_row12_col3\" class=\"data row12 col3\" >3.8718</td>\n",
       "      <td id=\"T_fe4d0_row12_col4\" class=\"data row12 col4\" >0.3160</td>\n",
       "      <td id=\"T_fe4d0_row12_col5\" class=\"data row12 col5\" >0.2033</td>\n",
       "      <td id=\"T_fe4d0_row12_col6\" class=\"data row12 col6\" >0.1762</td>\n",
       "      <td id=\"T_fe4d0_row12_col7\" class=\"data row12 col7\" >0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row13\" class=\"row_heading level0 row13\" >knn</th>\n",
       "      <td id=\"T_fe4d0_row13_col0\" class=\"data row13 col0\" >K Neighbors Regressor</td>\n",
       "      <td id=\"T_fe4d0_row13_col1\" class=\"data row13 col1\" >3.2991</td>\n",
       "      <td id=\"T_fe4d0_row13_col2\" class=\"data row13 col2\" >17.5854</td>\n",
       "      <td id=\"T_fe4d0_row13_col3\" class=\"data row13 col3\" >4.1835</td>\n",
       "      <td id=\"T_fe4d0_row13_col4\" class=\"data row13 col4\" >0.2210</td>\n",
       "      <td id=\"T_fe4d0_row13_col5\" class=\"data row13 col5\" >0.2192</td>\n",
       "      <td id=\"T_fe4d0_row13_col6\" class=\"data row13 col6\" >0.1925</td>\n",
       "      <td id=\"T_fe4d0_row13_col7\" class=\"data row13 col7\" >0.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row14\" class=\"row_heading level0 row14\" >dummy</th>\n",
       "      <td id=\"T_fe4d0_row14_col0\" class=\"data row14 col0\" >Dummy Regressor</td>\n",
       "      <td id=\"T_fe4d0_row14_col1\" class=\"data row14 col1\" >3.9254</td>\n",
       "      <td id=\"T_fe4d0_row14_col2\" class=\"data row14 col2\" >23.8269</td>\n",
       "      <td id=\"T_fe4d0_row14_col3\" class=\"data row14 col3\" >4.8465</td>\n",
       "      <td id=\"T_fe4d0_row14_col4\" class=\"data row14 col4\" >-0.0268</td>\n",
       "      <td id=\"T_fe4d0_row14_col5\" class=\"data row14 col5\" >0.2548</td>\n",
       "      <td id=\"T_fe4d0_row14_col6\" class=\"data row14 col6\" >0.2338</td>\n",
       "      <td id=\"T_fe4d0_row14_col7\" class=\"data row14 col7\" >0.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row15\" class=\"row_heading level0 row15\" >omp</th>\n",
       "      <td id=\"T_fe4d0_row15_col0\" class=\"data row15 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_fe4d0_row15_col1\" class=\"data row15 col1\" >3.9592</td>\n",
       "      <td id=\"T_fe4d0_row15_col2\" class=\"data row15 col2\" >23.8382</td>\n",
       "      <td id=\"T_fe4d0_row15_col3\" class=\"data row15 col3\" >4.8516</td>\n",
       "      <td id=\"T_fe4d0_row15_col4\" class=\"data row15 col4\" >-0.0309</td>\n",
       "      <td id=\"T_fe4d0_row15_col5\" class=\"data row15 col5\" >0.2554</td>\n",
       "      <td id=\"T_fe4d0_row15_col6\" class=\"data row15 col6\" >0.2359</td>\n",
       "      <td id=\"T_fe4d0_row15_col7\" class=\"data row15 col7\" >0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row16\" class=\"row_heading level0 row16\" >par</th>\n",
       "      <td id=\"T_fe4d0_row16_col0\" class=\"data row16 col0\" >Passive Aggressive Regressor</td>\n",
       "      <td id=\"T_fe4d0_row16_col1\" class=\"data row16 col1\" >4.5610</td>\n",
       "      <td id=\"T_fe4d0_row16_col2\" class=\"data row16 col2\" >34.8483</td>\n",
       "      <td id=\"T_fe4d0_row16_col3\" class=\"data row16 col3\" >5.7787</td>\n",
       "      <td id=\"T_fe4d0_row16_col4\" class=\"data row16 col4\" >-0.4520</td>\n",
       "      <td id=\"T_fe4d0_row16_col5\" class=\"data row16 col5\" >0.3011</td>\n",
       "      <td id=\"T_fe4d0_row16_col6\" class=\"data row16 col6\" >0.2455</td>\n",
       "      <td id=\"T_fe4d0_row16_col7\" class=\"data row16 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe4d0_level0_row17\" class=\"row_heading level0 row17\" >lar</th>\n",
       "      <td id=\"T_fe4d0_row17_col0\" class=\"data row17 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_fe4d0_row17_col1\" class=\"data row17 col1\" >40.0362</td>\n",
       "      <td id=\"T_fe4d0_row17_col2\" class=\"data row17 col2\" >3543.2654</td>\n",
       "      <td id=\"T_fe4d0_row17_col3\" class=\"data row17 col3\" >51.5600</td>\n",
       "      <td id=\"T_fe4d0_row17_col4\" class=\"data row17 col4\" >-160.4182</td>\n",
       "      <td id=\"T_fe4d0_row17_col5\" class=\"data row17 col5\" >1.0151</td>\n",
       "      <td id=\"T_fe4d0_row17_col6\" class=\"data row17 col6\" >2.2940</td>\n",
       "      <td id=\"T_fe4d0_row17_col7\" class=\"data row17 col7\" >0.0110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x155466d9adc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare different models to find the best one\n",
    "best_3models = compare_models(n_select = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54b7ebf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d952a_row10_col0, #T_d952a_row10_col1, #T_d952a_row10_col2, #T_d952a_row10_col3, #T_d952a_row10_col4, #T_d952a_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d952a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d952a_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_d952a_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_d952a_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_d952a_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_d952a_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_d952a_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d952a_row0_col0\" class=\"data row0 col0\" >0.4588</td>\n",
       "      <td id=\"T_d952a_row0_col1\" class=\"data row0 col1\" >0.5132</td>\n",
       "      <td id=\"T_d952a_row0_col2\" class=\"data row0 col2\" >0.7164</td>\n",
       "      <td id=\"T_d952a_row0_col3\" class=\"data row0 col3\" >0.9790</td>\n",
       "      <td id=\"T_d952a_row0_col4\" class=\"data row0 col4\" >0.0414</td>\n",
       "      <td id=\"T_d952a_row0_col5\" class=\"data row0 col5\" >0.0274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d952a_row1_col0\" class=\"data row1 col0\" >0.3429</td>\n",
       "      <td id=\"T_d952a_row1_col1\" class=\"data row1 col1\" >0.2458</td>\n",
       "      <td id=\"T_d952a_row1_col2\" class=\"data row1 col2\" >0.4958</td>\n",
       "      <td id=\"T_d952a_row1_col3\" class=\"data row1 col3\" >0.9894</td>\n",
       "      <td id=\"T_d952a_row1_col4\" class=\"data row1 col4\" >0.0281</td>\n",
       "      <td id=\"T_d952a_row1_col5\" class=\"data row1 col5\" >0.0206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d952a_row2_col0\" class=\"data row2 col0\" >0.4565</td>\n",
       "      <td id=\"T_d952a_row2_col1\" class=\"data row2 col1\" >1.1992</td>\n",
       "      <td id=\"T_d952a_row2_col2\" class=\"data row2 col2\" >1.0951</td>\n",
       "      <td id=\"T_d952a_row2_col3\" class=\"data row2 col3\" >0.9657</td>\n",
       "      <td id=\"T_d952a_row2_col4\" class=\"data row2 col4\" >0.0421</td>\n",
       "      <td id=\"T_d952a_row2_col5\" class=\"data row2 col5\" >0.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d952a_row3_col0\" class=\"data row3 col0\" >0.3132</td>\n",
       "      <td id=\"T_d952a_row3_col1\" class=\"data row3 col1\" >0.2024</td>\n",
       "      <td id=\"T_d952a_row3_col2\" class=\"data row3 col2\" >0.4499</td>\n",
       "      <td id=\"T_d952a_row3_col3\" class=\"data row3 col3\" >0.9900</td>\n",
       "      <td id=\"T_d952a_row3_col4\" class=\"data row3 col4\" >0.0221</td>\n",
       "      <td id=\"T_d952a_row3_col5\" class=\"data row3 col5\" >0.0172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d952a_row4_col0\" class=\"data row4 col0\" >0.3371</td>\n",
       "      <td id=\"T_d952a_row4_col1\" class=\"data row4 col1\" >0.2758</td>\n",
       "      <td id=\"T_d952a_row4_col2\" class=\"data row4 col2\" >0.5252</td>\n",
       "      <td id=\"T_d952a_row4_col3\" class=\"data row4 col3\" >0.9875</td>\n",
       "      <td id=\"T_d952a_row4_col4\" class=\"data row4 col4\" >0.0279</td>\n",
       "      <td id=\"T_d952a_row4_col5\" class=\"data row4 col5\" >0.0196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d952a_row5_col0\" class=\"data row5 col0\" >0.3838</td>\n",
       "      <td id=\"T_d952a_row5_col1\" class=\"data row5 col1\" >0.2902</td>\n",
       "      <td id=\"T_d952a_row5_col2\" class=\"data row5 col2\" >0.5387</td>\n",
       "      <td id=\"T_d952a_row5_col3\" class=\"data row5 col3\" >0.9900</td>\n",
       "      <td id=\"T_d952a_row5_col4\" class=\"data row5 col4\" >0.0257</td>\n",
       "      <td id=\"T_d952a_row5_col5\" class=\"data row5 col5\" >0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d952a_row6_col0\" class=\"data row6 col0\" >0.3566</td>\n",
       "      <td id=\"T_d952a_row6_col1\" class=\"data row6 col1\" >0.3524</td>\n",
       "      <td id=\"T_d952a_row6_col2\" class=\"data row6 col2\" >0.5937</td>\n",
       "      <td id=\"T_d952a_row6_col3\" class=\"data row6 col3\" >0.9830</td>\n",
       "      <td id=\"T_d952a_row6_col4\" class=\"data row6 col4\" >0.0281</td>\n",
       "      <td id=\"T_d952a_row6_col5\" class=\"data row6 col5\" >0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d952a_row7_col0\" class=\"data row7 col0\" >0.3294</td>\n",
       "      <td id=\"T_d952a_row7_col1\" class=\"data row7 col1\" >0.1951</td>\n",
       "      <td id=\"T_d952a_row7_col2\" class=\"data row7 col2\" >0.4417</td>\n",
       "      <td id=\"T_d952a_row7_col3\" class=\"data row7 col3\" >0.9876</td>\n",
       "      <td id=\"T_d952a_row7_col4\" class=\"data row7 col4\" >0.0265</td>\n",
       "      <td id=\"T_d952a_row7_col5\" class=\"data row7 col5\" >0.0206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d952a_row8_col0\" class=\"data row8 col0\" >0.3715</td>\n",
       "      <td id=\"T_d952a_row8_col1\" class=\"data row8 col1\" >0.2711</td>\n",
       "      <td id=\"T_d952a_row8_col2\" class=\"data row8 col2\" >0.5207</td>\n",
       "      <td id=\"T_d952a_row8_col3\" class=\"data row8 col3\" >0.9837</td>\n",
       "      <td id=\"T_d952a_row8_col4\" class=\"data row8 col4\" >0.0323</td>\n",
       "      <td id=\"T_d952a_row8_col5\" class=\"data row8 col5\" >0.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d952a_row9_col0\" class=\"data row9 col0\" >0.3863</td>\n",
       "      <td id=\"T_d952a_row9_col1\" class=\"data row9 col1\" >0.3399</td>\n",
       "      <td id=\"T_d952a_row9_col2\" class=\"data row9 col2\" >0.5830</td>\n",
       "      <td id=\"T_d952a_row9_col3\" class=\"data row9 col3\" >0.9861</td>\n",
       "      <td id=\"T_d952a_row9_col4\" class=\"data row9 col4\" >0.0308</td>\n",
       "      <td id=\"T_d952a_row9_col5\" class=\"data row9 col5\" >0.0222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_d952a_row10_col0\" class=\"data row10 col0\" >0.3736</td>\n",
       "      <td id=\"T_d952a_row10_col1\" class=\"data row10 col1\" >0.3885</td>\n",
       "      <td id=\"T_d952a_row10_col2\" class=\"data row10 col2\" >0.5960</td>\n",
       "      <td id=\"T_d952a_row10_col3\" class=\"data row10 col3\" >0.9842</td>\n",
       "      <td id=\"T_d952a_row10_col4\" class=\"data row10 col4\" >0.0305</td>\n",
       "      <td id=\"T_d952a_row10_col5\" class=\"data row10 col5\" >0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d952a_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_d952a_row11_col0\" class=\"data row11 col0\" >0.0475</td>\n",
       "      <td id=\"T_d952a_row11_col1\" class=\"data row11 col1\" >0.2837</td>\n",
       "      <td id=\"T_d952a_row11_col2\" class=\"data row11 col2\" >0.1825</td>\n",
       "      <td id=\"T_d952a_row11_col3\" class=\"data row11 col3\" >0.0070</td>\n",
       "      <td id=\"T_d952a_row11_col4\" class=\"data row11 col4\" >0.0062</td>\n",
       "      <td id=\"T_d952a_row11_col5\" class=\"data row11 col5\" >0.0027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x155503949e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blnd_models = stack_models(best_3models, choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce9c5826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bb251\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bb251_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_bb251_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_bb251_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_bb251_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_bb251_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_bb251_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_bb251_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bb251_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bb251_row0_col0\" class=\"data row0 col0\" >Stacking Regressor</td>\n",
       "      <td id=\"T_bb251_row0_col1\" class=\"data row0 col1\" >2.5155</td>\n",
       "      <td id=\"T_bb251_row0_col2\" class=\"data row0 col2\" >8.0736</td>\n",
       "      <td id=\"T_bb251_row0_col3\" class=\"data row0 col3\" >2.8414</td>\n",
       "      <td id=\"T_bb251_row0_col4\" class=\"data row0 col4\" >0.3052</td>\n",
       "      <td id=\"T_bb251_row0_col5\" class=\"data row0 col5\" >0.1493</td>\n",
       "      <td id=\"T_bb251_row0_col6\" class=\"data row0 col6\" >0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15546413bd60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNODP</th>\n",
       "      <th>SWGDN</th>\n",
       "      <th>T2M</th>\n",
       "      <th>SWLAND</th>\n",
       "      <th>TLML</th>\n",
       "      <th>SLP</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>T0</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>0.542676</td>\n",
       "      <td>5.747317</td>\n",
       "      <td>-28.040482</td>\n",
       "      <td>2.445236</td>\n",
       "      <td>-25.288418</td>\n",
       "      <td>101819.585938</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-22.919069</td>\n",
       "      <td>-23.750349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>0.543722</td>\n",
       "      <td>6.295037</td>\n",
       "      <td>-27.916798</td>\n",
       "      <td>2.675132</td>\n",
       "      <td>-25.233812</td>\n",
       "      <td>101779.484375</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-22.829681</td>\n",
       "      <td>-23.054210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>0.544885</td>\n",
       "      <td>6.935750</td>\n",
       "      <td>-27.969452</td>\n",
       "      <td>2.945383</td>\n",
       "      <td>-25.434784</td>\n",
       "      <td>101752.000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-22.821472</td>\n",
       "      <td>-24.446161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>0.546115</td>\n",
       "      <td>7.812706</td>\n",
       "      <td>-28.285040</td>\n",
       "      <td>3.320143</td>\n",
       "      <td>-25.904675</td>\n",
       "      <td>101749.203125</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-23.104736</td>\n",
       "      <td>-26.178059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>0.547260</td>\n",
       "      <td>8.807284</td>\n",
       "      <td>-28.893335</td>\n",
       "      <td>3.746693</td>\n",
       "      <td>-26.554148</td>\n",
       "      <td>101720.218750</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-23.492889</td>\n",
       "      <td>-27.909937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNODP     SWGDN        T2M    SWLAND       TLML            SLP  Year   \n",
       "1191  0.542676  5.747317 -28.040482  2.445236 -25.288418  101819.585938  2013  \\\n",
       "1192  0.543722  6.295037 -27.916798  2.675132 -25.233812  101779.484375  2013   \n",
       "1193  0.544885  6.935750 -27.969452  2.945383 -25.434784  101752.000000  2013   \n",
       "1194  0.546115  7.812706 -28.285040  3.320143 -25.904675  101749.203125  2013   \n",
       "1195  0.547260  8.807284 -28.893335  3.746693 -26.554148  101720.218750  2013   \n",
       "\n",
       "      Month  Day         T0  prediction_label  \n",
       "1191      2    2 -22.919069        -23.750349  \n",
       "1192      2    3 -22.829681        -23.054210  \n",
       "1193      2    4 -22.821472        -24.446161  \n",
       "1194      2    5 -23.104736        -26.178059  \n",
       "1195      2    6 -23.492889        -27.909937  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict_model(blnd_models, data=test_df)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f4fa01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 348.4927978515625\n",
      "Epoch 10, Loss: 304.8775329589844\n",
      "Epoch 20, Loss: 249.3015899658203\n",
      "Epoch 30, Loss: 25.729082107543945\n",
      "Epoch 40, Loss: 97.03080749511719\n",
      "Epoch 50, Loss: 19.73333168029785\n",
      "Epoch 60, Loss: 141.3456268310547\n",
      "Epoch 70, Loss: 103.43059539794922\n",
      "Epoch 80, Loss: 51.37699890136719\n",
      "Epoch 90, Loss: 14.568145751953125\n",
      "Epoch 100, Loss: 9.537269592285156\n",
      "Epoch 110, Loss: 14.908127784729004\n",
      "Epoch 120, Loss: 7.3258585929870605\n",
      "Epoch 130, Loss: 6.123633861541748\n",
      "Epoch 140, Loss: 5.243969440460205\n",
      "Epoch 150, Loss: 4.3145670890808105\n",
      "Epoch 160, Loss: 3.4659883975982666\n",
      "Epoch 170, Loss: 3.351386308670044\n",
      "Epoch 180, Loss: 2.673105001449585\n",
      "Epoch 190, Loss: 2.251405954360962\n",
      "Train RMSE: 1.4739702939987183\n",
      "Test RMSE: 6.725282192230225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "loc = 'Toolik'\n",
    "sequence_length = 1\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "target= 'T0'\n",
    "res = 30\n",
    "season = 'Winter'\n",
    "X, y, _ = prepare_sequences(loc, sequence_length, target, features, season, res)\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Attention-based regression model\n",
    "class AttentionRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AttentionRegressor, self).__init__()\n",
    "        self.attention = nn.Linear(input_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=-1)\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        # Feed through network\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Model initialization\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 4192  # Adjust as necessary\n",
    "output_dim = 1\n",
    "model = AttentionRegressor(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Training the model\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train).squeeze()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train).squeeze()\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    print(f'Train RMSE: {train_rmse}')\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).squeeze()\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Test RMSE: {test_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "81533f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n",
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Exception in thread Exception ignored in: <function _ConnectionBase.__del__ at 0x15547b1dd4c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 132, in __del__\n",
      "QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread Exception ignored in: <function _ConnectionBase.__del__ at 0x15547b1dd4c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 132, in __del__\n",
      "QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 239, in _feed\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    reader_close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 177, in close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/cm/local/apps/python39/lib/python3.9/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 518   \n",
      "3  | prescalers                         | ModuleDict                      | 160   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.2 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 4.4 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 4.4 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "25.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.7 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033cdfcf7dcf4a229ad4fd8c4ef24ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 78\u001b[0m\n\u001b[1;32m     64\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[1;32m     65\u001b[0m     training,\n\u001b[1;32m     66\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n\u001b[1;32m     85\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:127\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py:127\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py:56\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py:326\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py:132\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "loc = 'Toolik'\n",
    "sequence_length = 1\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "target= 'T0'\n",
    "res = 30\n",
    "season = 'Winter'\n",
    "_, _, df = prepare_sequences(loc, sequence_length, target, features, season, res)\n",
    "# Split into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "# Split the data chronologically\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:]\n",
    "\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP','Year','Month','Day']\n",
    "\n",
    "# prepare the data\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(df_train)\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols  # type: ignore[arg-type]\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(df_train)\n",
    "\n",
    "# build the model\n",
    "wide = Wide(input_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    ")\n",
    "model = WideDeep(wide=wide, deeptabular=tab_mlp)\n",
    "\n",
    "\n",
    "# Preprocess the data for continuous features\n",
    "tab_preprocessor = TabPreprocessor(continuous_cols=features)\n",
    "train_processed = tab_preprocessor.fit_transform(train_df)\n",
    "test_processed = tab_preprocessor.transform(test_df)\n",
    "\n",
    "# Extract the processed features and target for training and testing\n",
    "X_train_processed = train_processed[:, :-1]\n",
    "y_train = train_processed[:, -1]\n",
    "X_test_processed = test_processed[:, :-1]\n",
    "y_test = test_processed[:, -1]\n",
    "\n",
    "# Define the TabMlp model\n",
    "tab_mlp = TabMlp(column_idx=tab_preprocessor.column_idx, \n",
    "                 continuous_cols=features,\n",
    "                 mlp_hidden_dims=[64, 32])  # Adjust the hidden dimensions as necessary\n",
    "\n",
    "# Configure the trainer\n",
    "trainer = Trainer(model=tab_mlp, objective=\"regression\")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(X_train=X_train_processed, target=y_train, n_epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = trainer.predict(X_test_processed)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a4d25995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNODP</th>\n",
       "      <th>SWGDN</th>\n",
       "      <th>T2M</th>\n",
       "      <th>SWLAND</th>\n",
       "      <th>TLML</th>\n",
       "      <th>SLP</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>T0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.463095</td>\n",
       "      <td>0.438971</td>\n",
       "      <td>-23.080234</td>\n",
       "      <td>0.194240</td>\n",
       "      <td>-20.556509</td>\n",
       "      <td>101039.265885</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>-18.331431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.465450</td>\n",
       "      <td>0.496024</td>\n",
       "      <td>-23.193878</td>\n",
       "      <td>0.218241</td>\n",
       "      <td>-20.721169</td>\n",
       "      <td>100951.714844</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>-18.283194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.468565</td>\n",
       "      <td>0.596560</td>\n",
       "      <td>-23.314572</td>\n",
       "      <td>0.261882</td>\n",
       "      <td>-20.896520</td>\n",
       "      <td>100861.221094</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>-18.231236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.471880</td>\n",
       "      <td>0.662519</td>\n",
       "      <td>-23.136095</td>\n",
       "      <td>0.289177</td>\n",
       "      <td>-20.763220</td>\n",
       "      <td>100785.919271</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>-18.061153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.475214</td>\n",
       "      <td>0.816875</td>\n",
       "      <td>-22.426599</td>\n",
       "      <td>0.356429</td>\n",
       "      <td>-20.121811</td>\n",
       "      <td>100743.483854</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>-17.552069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>0.457069</td>\n",
       "      <td>34.522049</td>\n",
       "      <td>-16.051936</td>\n",
       "      <td>15.207806</td>\n",
       "      <td>-12.377654</td>\n",
       "      <td>100551.669010</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>-12.941386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>0.456912</td>\n",
       "      <td>35.534936</td>\n",
       "      <td>-16.059590</td>\n",
       "      <td>15.632955</td>\n",
       "      <td>-12.484593</td>\n",
       "      <td>100481.438021</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-13.087914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0.456921</td>\n",
       "      <td>36.464579</td>\n",
       "      <td>-15.935733</td>\n",
       "      <td>16.018229</td>\n",
       "      <td>-12.429766</td>\n",
       "      <td>100457.913021</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>-13.002636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>0.457106</td>\n",
       "      <td>37.389386</td>\n",
       "      <td>-16.097779</td>\n",
       "      <td>16.397680</td>\n",
       "      <td>-12.670831</td>\n",
       "      <td>100483.828385</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>-13.406665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0.457523</td>\n",
       "      <td>38.886663</td>\n",
       "      <td>-16.456182</td>\n",
       "      <td>17.030758</td>\n",
       "      <td>-13.177868</td>\n",
       "      <td>100543.263542</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>-14.047164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1489 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNODP      SWGDN        T2M     SWLAND       TLML            SLP   \n",
       "0     0.463095   0.438971 -23.080234   0.194240 -20.556509  101039.265885  \\\n",
       "1     0.465450   0.496024 -23.193878   0.218241 -20.721169  100951.714844   \n",
       "2     0.468565   0.596560 -23.314572   0.261882 -20.896520  100861.221094   \n",
       "3     0.471880   0.662519 -23.136095   0.289177 -20.763220  100785.919271   \n",
       "4     0.475214   0.816875 -22.426599   0.356429 -20.121811  100743.483854   \n",
       "...        ...        ...        ...        ...        ...            ...   \n",
       "1484  0.457069  34.522049 -16.051936  15.207806 -12.377654  100551.669010   \n",
       "1485  0.456912  35.534936 -16.059590  15.632955 -12.484593  100481.438021   \n",
       "1486  0.456921  36.464579 -15.935733  16.018229 -12.429766  100457.913021   \n",
       "1487  0.457106  37.389386 -16.097779  16.397680 -12.670831  100483.828385   \n",
       "1488  0.457523  38.886663 -16.456182  17.030758 -13.177868  100543.263542   \n",
       "\n",
       "      Year  Month  Day         T0  \n",
       "0     2000      1   16 -18.331431  \n",
       "1     2000      1   17 -18.283194  \n",
       "2     2000      1   18 -18.231236  \n",
       "3     2000      1   19 -18.061153  \n",
       "4     2000      1   20 -17.552069  \n",
       "...    ...    ...  ...        ...  \n",
       "1484  2016      2   25 -12.941386  \n",
       "1485  2016      2   26 -13.087914  \n",
       "1486  2016      2   27 -13.002636  \n",
       "1487  2016      2   28 -13.406665  \n",
       "1488  2016      2   29 -14.047164  \n",
       "\n",
       "[1489 rows x 10 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "91e86702",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2796660/2142732507.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SNODP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SWGDN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T2M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SWLAND'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TLML'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SLP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'T0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tsai/data/preparation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df, sample_col, feat_col, data_cols, target_col, steps_in_rows, to3d, splits, sort_by, ascending, y_func, return_names)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mdata_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpassed_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Are you sure you want to include {target_col} in X?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m-> 6740\u001b[0;31m     def sort_values(\n\u001b[0m\u001b[1;32m   6741\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6742\u001b[0m         \u001b[0mby\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexLabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6743\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "_, _, df = prepare_sequences(loc, sequence_length, target, features, season, res)\n",
    "\n",
    "X, y = df2xy(df, sample_col=df.index, feat_col=['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'], target_col='T0', data_cols=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1c336ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#1191) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#298) [1191,1192,1193,1194,1195,1196,1197,1198,1199,1200...])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.all import *\n",
    "\n",
    "loc = 'Toolik'\n",
    "sequence_length = 1\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "target= 'T0'\n",
    "res = 30\n",
    "season = 'Winter'\n",
    "X, y, df = prepare_sequences(loc, sequence_length, target, features, season, res)\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "X, y, splits = combine_split_data([X_train, X_test], [y_train, y_test])\n",
    "tfms  = [None, [TSRegression()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=1048)\n",
    "set_seed(2)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7aef74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# find optimal learning rate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuner\n\u001b[0;32m----> 4\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39msuggestion()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m fig \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mplot(show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, suggest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/tuner/tuning.py:177\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m lr_finder_callback\u001b[38;5;241m.\u001b[39m_early_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lr_finder_callback\u001b[38;5;241m.\u001b[39moptimal_lr\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:969\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 969\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_fit_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    972\u001b[0m _log_hyperparams(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/callbacks/lr_finder.py:126\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/callbacks/lr_finder.py:110\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimal_lr \u001b[38;5;241m=\u001b[39m \u001b[43m_lr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_min_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_training_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stop_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_early_stop_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attr_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/tuner/lr_finder.py:275\u001b[0m, in \u001b[0;36m_lr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    272\u001b[0m lr_finder\u001b[38;5;241m.\u001b[39m_exchange_scheduler(trainer)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[43m_try_loop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m!=\u001b[39m num_training \u001b[38;5;241m+\u001b[39m start_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/tuner/lr_finder.py:515\u001b[0m, in \u001b[0;36m_try_loop_run\u001b[0;34m(trainer, params)\u001b[0m\n\u001b[1;32m    513\u001b[0m loop\u001b[38;5;241m.\u001b[39mload_state_dict(deepcopy(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloop_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    514\u001b[0m loop\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:137\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py:285\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:127\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py:127\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py:56\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py:326\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py:132\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from fastai.callback.all import *\n",
    "\n",
    "seed = 42\n",
    "par = {}\n",
    "models = {'InceptionTimePlus','TSTPlus','XCMPlus','LSTMPlus','GRUPlus'}#,'MiniRocketPlus',\n",
    "output_inputs = 'T0_TimeTair'\n",
    "location = 'Deadhorse'\n",
    "mode = ['T0'] #'T0','T_02','T_07','T_12','T_22','T_32','T76','T_62','T_67','T97'\n",
    "\n",
    "freeze_epoch = 100\n",
    "\n",
    "season1 = location\n",
    "\n",
    "def get_dayofweek(x):\n",
    "    return x.dayofweek    \n",
    "def get_month(x):\n",
    "    return x.month    \n",
    "def get_year(x):\n",
    "    return x.year    \n",
    "\n",
    "iind = [3,6,9,12,18]#,18\n",
    "ind2 = ''\n",
    "\n",
    "tbl = pd.DataFrame(columns=['input', 'model', 'total params', 'train loss', 'valid loss', 'rmse_val', 'rmse_test', 'time'])\n",
    "\n",
    "tbl_ind = 0\n",
    "for II in range(len(iind)):\n",
    "    ind = str(iind[II]) + 'days_'\n",
    "    print(\"************************************\\n ind:\",ind,\":\")\n",
    "    for key in models:\n",
    "        exec('model_name = {}'.format(key)) \n",
    "        print(\"*\",model_name.__name__,\":\")\n",
    "        df = pd.read_csv(f'Air2UG_alltimeseries_Deadhorse_MERRA2data.csv')\n",
    "        # print('Processing Tair & output data..')\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        col = ['Signal']\n",
    "        row = [\"feature{}\".format(x) for x in np.array(range(4))] #Change this number to higher if necessary!\n",
    "        scalersIN = pd.DataFrame([], columns = col, index = row)\n",
    "        scalersOUT = pd.DataFrame([], columns = col, index = [0])\n",
    "        from sklearn.preprocessing import OrdinalEncoder\n",
    "        test_targetdata={}\n",
    "        test_predsdata={}\n",
    "        #read TS data as datetime\n",
    "        cols = TS.columns[:]\n",
    "        TS[cols] = TS[cols].apply(pd.to_datetime, errors='coerce')\n",
    "        TS_data = TS.iloc[:,:] # Train set \n",
    "        TS_D = TS_data.applymap(get_dayofweek)\n",
    "        TS_M = TS_data.applymap(get_month)\n",
    "        TS_Y = TS_data.applymap(get_year)\n",
    "\n",
    "        # define ordinal encoding\n",
    "        encoder_D = OrdinalEncoder()\n",
    "        encoder_M = OrdinalEncoder()\n",
    "        encoder_Y = OrdinalEncoder()\n",
    "        # transform data\n",
    "        TS_D = to3d(encoder_D.fit_transform(TS_D))\n",
    "        TS_M = to3d(encoder_M.fit_transform(TS_M))\n",
    "        TS_Y = to3d(encoder_Y.fit_transform(TS_Y))\n",
    "\n",
    "        # print('splitting data..')\n",
    "        Tair = to3d(Tair.iloc[:,:].values)\n",
    "        T0_data = to3d(T0.values)\n",
    "        T0_data = T0_data[:,:,-1]\n",
    "        data_X = np.concatenate([Tair[:,:,:], TS_D[:,:,:], TS_M[:,:,:], TS_Y[:,:,:]],axis=1)   #[TS_D[:,:,:], TS_M[:,:,:], TS_Y[:,:,:]],axis=1)    \n",
    "        data_y = T0_data[:,:]#T0_data[:,0,:]\n",
    "        X_train, X_test = data_X[:int((Tair.shape[0])-(Tair.shape[0])*20/100)], data_X[-int((Tair.shape[0])*20/100):]\n",
    "        y_train, y_test = data_y[:int((Tair.shape[0])-(Tair.shape[0])*20/100)], data_y[-int((Tair.shape[0])*20/100):]\n",
    "        \n",
    "        # print('Standardizing data..')\n",
    "        for i in range(X_train.shape[1]): #normalize IN data\n",
    "            scalersIN['Signal'][\"feature{}\".format(i)] = StandardScaler()\n",
    "            X_train[:, i, :] = scalersIN['Signal'][\"feature{}\".format(i)].fit_transform(X_train[:, i, :])    \n",
    "        scalersOUT['Signal'] = StandardScaler() #normalize OUT data\n",
    "        y_train = scalersOUT['Signal'][0].fit_transform(y_train.reshape(-1, 1)) #remove .reshape(-1, 1) in case! \n",
    "        print('training set: ',X_train.shape, y_train.shape, ', testing set: ',X_test.shape, y_test.shape)\n",
    "        splits = get_splits(y_train, valid_size=.2, stratify=False, random_state=seed, shuffle=True)\n",
    "        tfms  = [None, [TSRegression()]]\n",
    "        dsets = TSDatasets(X_train, y_train, tfms=tfms, splits=splits, inplace=True)\n",
    "        dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=1048)\n",
    "        set_seed(seed)\n",
    "        \n",
    "        #**************#**************#**************#**************#**************#**************#**************#**************\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "\n",
    "        tuning(model_name)\n",
    "\n",
    "        print(\"**Study statistics: \")\n",
    "        print(\"  Number of finished trials: \", len(study.trials))\n",
    "\n",
    "        print(\"Best trial:\")\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print(\"  Value: \", trial.value)\n",
    "\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\"{}\".format(value))\n",
    "\n",
    "        ### Model learning\n",
    "        # Get the best nf and dropout rate from the best trial object\n",
    "        trial = study.best_trial\n",
    "        for key, value in trial.params.items():\n",
    "            if  key != 'act' and key != 'wavelet':\n",
    "                exec('{} = {}'.format(key,value))\n",
    "            if key == 'act':\n",
    "                exec(\"act = '%s'\" % (value))\n",
    "            if key == 'wavelet':\n",
    "                exec(\"wavelet = '%s'\" % (value))\n",
    "\n",
    "        par = tuned_hyperparameters(model_name)\n",
    "        \n",
    "        #Model learning\n",
    "        learn = TSForecaster(X_train, y_train, splits=splits, tfms=tfms,\n",
    "                             bs=1042, arch=model_name,\n",
    "                             arch_config=par, metrics=[mae, rmse],\n",
    "                             cbs=[],#ShowGraph(),SaveModel(),EarlyStoppingCallback(monitor='_rmse', comp=np.less,min_delta=0.0001, patience=6)\n",
    "                             seed=seed)\n",
    "        lr = learn.lr_find()\n",
    "        learning_rate = lr.valley\n",
    "        plt.show()\n",
    "        \n",
    "        start = time.time()\n",
    "        learn.fit_one_cycle(freeze_epoch, learning_rate)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        vals = learn.recorder.values[-1]\n",
    "        \n",
    "        valid_dl = dls.valid\n",
    "\n",
    "        # Labeled test data\n",
    "        for i in range(X_test.shape[1]): #normalize IN data\n",
    "            X_test[:, i, :] = scalersIN['Signal'][\"feature{}\".format(i)].transform(X_test[:, i, :])    \n",
    "        y_test = scalersOUT['Signal'][0].transform(y_test)  \n",
    "        test_ds = valid_dl.dataset.add_test(X_test, y_test)\n",
    "        test_dl = valid_dl.new(test_ds)\n",
    "        _, temp_targets, temp_preds = learn.get_preds(dl=test_dl, with_decoded=True, save_preds=None, save_targs=None)\n",
    "        #denormalize data\n",
    "        test_predsdata = scalersOUT['Signal'][0].inverse_transform(temp_preds)  \n",
    "        test_targetdata = scalersOUT['Signal'][0].inverse_transform(temp_targets)    \n",
    "        del temp_targets, temp_preds\n",
    "        d = pd.DataFrame(test_targetdata)\n",
    "        dp = pd.DataFrame(test_predsdata)\n",
    "\n",
    "        RMSE = np.sqrt(np.square(np.subtract(d,dp)).mean(axis=0))\n",
    "        print(\"Root Mean Square Error:\", RMSE.mean())\n",
    "\n",
    "        results = pd.concat([d, dp],axis = 1)\n",
    "\n",
    "        # Write the results to a CSV file\n",
    "        results.to_csv('export3/SinglePrediction' + output_inputs + '_' + str(model_name.__name__.replace('()', '')) + ind + 'results.csv', index=False)  \n",
    "\n",
    "        tbl.loc[tbl_ind] = [int(iind[II]), model_name.__name__, count_parameters(learn, model_name), vals[0], vals[1], vals[3], RMSE[0], float(elapsed)]\n",
    "        tbl.sort_values(by='input', ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
    "        clear_output()\n",
    "        tbl_ind += 1\n",
    "\n",
    "display(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "37dec06f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datamodule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdatamodule\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datamodule' is not defined"
     ]
    }
   ],
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "501847dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tair</th>\n",
       "      <th>T0</th>\n",
       "      <th>T_02</th>\n",
       "      <th>T_07</th>\n",
       "      <th>T_12</th>\n",
       "      <th>T_22</th>\n",
       "      <th>T_32</th>\n",
       "      <th>T_42</th>\n",
       "      <th>T_62</th>\n",
       "      <th>...</th>\n",
       "      <th>HFLUX</th>\n",
       "      <th>SPEED</th>\n",
       "      <th>TLML</th>\n",
       "      <th>TSH</th>\n",
       "      <th>EVPSOIL</th>\n",
       "      <th>LWLAND</th>\n",
       "      <th>TS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>SLP</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4987</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1993-08-02 00:00:00</td>\n",
       "      <td>-12.290935</td>\n",
       "      <td>-5.889772</td>\n",
       "      <td>-5.830465</td>\n",
       "      <td>-5.036799</td>\n",
       "      <td>-6.158672</td>\n",
       "      <td>-5.915271</td>\n",
       "      <td>-5.984969</td>\n",
       "      <td>-6.117933</td>\n",
       "      <td>-6.154309</td>\n",
       "      <td>...</td>\n",
       "      <td>15.354276</td>\n",
       "      <td>7.036751</td>\n",
       "      <td>-9.803949</td>\n",
       "      <td>-7.971310</td>\n",
       "      <td>13.821814</td>\n",
       "      <td>-32.952282</td>\n",
       "      <td>-9.097297</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>101527.826355</td>\n",
       "      <td>2493.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1986-10-05 00:00:00</td>\n",
       "      <td>-49.000000</td>\n",
       "      <td>-38.800000</td>\n",
       "      <td>-23.950000</td>\n",
       "      <td>-23.250000</td>\n",
       "      <td>-22.640000</td>\n",
       "      <td>-21.320000</td>\n",
       "      <td>-20.740000</td>\n",
       "      <td>-20.300000</td>\n",
       "      <td>-19.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.992355</td>\n",
       "      <td>0.709324</td>\n",
       "      <td>-43.248114</td>\n",
       "      <td>-36.741125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-116.264725</td>\n",
       "      <td>-38.630301</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>97474.546875</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1990-03-04 12:00:00</td>\n",
       "      <td>-24.860000</td>\n",
       "      <td>-14.770000</td>\n",
       "      <td>-14.070000</td>\n",
       "      <td>-13.010000</td>\n",
       "      <td>-13.835000</td>\n",
       "      <td>-13.050000</td>\n",
       "      <td>-12.810000</td>\n",
       "      <td>-12.680000</td>\n",
       "      <td>-12.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.032649</td>\n",
       "      <td>4.473168</td>\n",
       "      <td>-19.481779</td>\n",
       "      <td>-16.667441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-44.339716</td>\n",
       "      <td>-18.117056</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>100869.179688</td>\n",
       "      <td>1246.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1993-08-02 00:00:00</td>\n",
       "      <td>-11.980000</td>\n",
       "      <td>-4.970000</td>\n",
       "      <td>-3.330000</td>\n",
       "      <td>-2.290000</td>\n",
       "      <td>-3.210000</td>\n",
       "      <td>-2.840000</td>\n",
       "      <td>-2.960000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>-3.680000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.835920</td>\n",
       "      <td>6.441012</td>\n",
       "      <td>-9.049780</td>\n",
       "      <td>-7.470221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-28.136324</td>\n",
       "      <td>-8.713141</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>101492.265625</td>\n",
       "      <td>2493.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1996-12-30 12:00:00</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>2.045000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>1.880000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>-0.480000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.152713</td>\n",
       "      <td>8.994404</td>\n",
       "      <td>0.227106</td>\n",
       "      <td>1.517847</td>\n",
       "      <td>21.689743</td>\n",
       "      <td>-17.075304</td>\n",
       "      <td>1.155923</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>102140.941406</td>\n",
       "      <td>3739.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2000-05-30 00:00:00</td>\n",
       "      <td>19.240000</td>\n",
       "      <td>16.070000</td>\n",
       "      <td>12.990000</td>\n",
       "      <td>14.060000</td>\n",
       "      <td>11.130000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>5.470000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>115.511772</td>\n",
       "      <td>22.758757</td>\n",
       "      <td>16.616571</td>\n",
       "      <td>9.980096</td>\n",
       "      <td>113.666199</td>\n",
       "      <td>7.704474</td>\n",
       "      <td>8.031946</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>105779.273438</td>\n",
       "      <td>4986.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.290626</td>\n",
       "      <td>10.231290</td>\n",
       "      <td>8.607557</td>\n",
       "      <td>8.465091</td>\n",
       "      <td>7.788902</td>\n",
       "      <td>7.231272</td>\n",
       "      <td>6.856190</td>\n",
       "      <td>6.537273</td>\n",
       "      <td>5.994305</td>\n",
       "      <td>...</td>\n",
       "      <td>15.104453</td>\n",
       "      <td>3.354161</td>\n",
       "      <td>11.416093</td>\n",
       "      <td>10.034733</td>\n",
       "      <td>22.822316</td>\n",
       "      <td>20.239449</td>\n",
       "      <td>10.402602</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>1034.631777</td>\n",
       "      <td>1439.767227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date         Tair           T0         T_02   \n",
       "count                 4987  4987.000000  4987.000000  4987.000000  \\\n",
       "mean   1993-08-02 00:00:00   -12.290935    -5.889772    -5.830465   \n",
       "min    1986-10-05 00:00:00   -49.000000   -38.800000   -23.950000   \n",
       "25%    1990-03-04 12:00:00   -24.860000   -14.770000   -14.070000   \n",
       "50%    1993-08-02 00:00:00   -11.980000    -4.970000    -3.330000   \n",
       "75%    1996-12-30 12:00:00     1.025000     2.045000     0.985000   \n",
       "max    2000-05-30 00:00:00    19.240000    16.070000    12.990000   \n",
       "std                    NaN    15.290626    10.231290     8.607557   \n",
       "\n",
       "              T_07         T_12         T_22         T_32         T_42   \n",
       "count  4987.000000  4987.000000  4987.000000  4987.000000  4987.000000  \\\n",
       "mean     -5.036799    -6.158672    -5.915271    -5.984969    -6.117933   \n",
       "min     -23.250000   -22.640000   -21.320000   -20.740000   -20.300000   \n",
       "25%     -13.010000   -13.835000   -13.050000   -12.810000   -12.680000   \n",
       "50%      -2.290000    -3.210000    -2.840000    -2.960000    -3.220000   \n",
       "75%       1.880000     0.010000     0.100000    -0.010000    -0.120000   \n",
       "max      14.060000    11.130000     7.400000     5.470000     5.000000   \n",
       "std       8.465091     7.788902     7.231272     6.856190     6.537273   \n",
       "\n",
       "              T_62  ...        HFLUX        SPEED         TLML          TSH   \n",
       "count  4987.000000  ...  4987.000000  4987.000000  4987.000000  4987.000000  \\\n",
       "mean     -6.154309  ...    15.354276     7.036751    -9.803949    -7.971310   \n",
       "min     -19.450000  ...   -54.992355     0.709324   -43.248114   -36.741125   \n",
       "25%     -12.100000  ...     6.032649     4.473168   -19.481779   -16.667441   \n",
       "50%      -3.680000  ...    13.835920     6.441012    -9.049780    -7.470221   \n",
       "75%      -0.480000  ...    23.152713     8.994404     0.227106     1.517847   \n",
       "max       2.890000  ...   115.511772    22.758757    16.616571     9.980096   \n",
       "std       5.994305  ...    15.104453     3.354161    11.416093    10.034733   \n",
       "\n",
       "           EVPSOIL       LWLAND           TS         QV2M            SLP   \n",
       "count  4987.000000  4987.000000  4987.000000  4987.000000    4987.000000  \\\n",
       "mean     13.821814   -32.952282    -9.097297     0.002199  101527.826355   \n",
       "min       0.000000  -116.264725   -38.630301     0.000090   97474.546875   \n",
       "25%       0.000000   -44.339716   -18.117056     0.000682  100869.179688   \n",
       "50%       0.000000   -28.136324    -8.713141     0.001653  101492.265625   \n",
       "75%      21.689743   -17.075304     1.155923     0.003621  102140.941406   \n",
       "max     113.666199     7.704474     8.031946     0.008496  105779.273438   \n",
       "std      22.822316    20.239449    10.402602     0.001697    1034.631777   \n",
       "\n",
       "          time_idx  \n",
       "count  4987.000000  \n",
       "mean   2493.000000  \n",
       "min       0.000000  \n",
       "25%    1246.500000  \n",
       "50%    2493.000000  \n",
       "75%    3739.500000  \n",
       "max    4986.000000  \n",
       "std    1439.767227  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "741fbece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of timestamps: 4987\n"
     ]
    }
   ],
   "source": [
    "start_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "# Calculate the total number of days\n",
    "total_days = (end_date - start_date).days + 1  # +1 to include both start and end dates\n",
    "\n",
    "print(f\"Expected number of timestamps: {total_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b3093",
   "metadata": {},
   "source": [
    "## MODEL LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1e656",
   "metadata": {},
   "source": [
    "### START LEARNING: T76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632b8f0",
   "metadata": {},
   "source": [
    "#### START LEARNING: T76 using the matlab processed dataset (already made sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970a7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare sequences\n",
    "def prepare_sequences(loc, sequence_length, target, features, season, res):\n",
    "    # Reading the csv file\n",
    "    if res > 1:\n",
    "        resolution = 'res'+str(res)+'D_'\n",
    "    else:\n",
    "        resolution = ''    \n",
    "\n",
    "    df = pd.read_csv(f'Air2UG_alltimeseries_{loc}_MERRA2data_{target}_{resolution}SeqLen{str(sequence_length)}_{season}.csv')\n",
    "\n",
    "    # Initialize an empty list to hold feature columns\n",
    "    feature_columns = []\n",
    "    \n",
    "    if len(features) > 0 and sequence_length > 1:\n",
    "        # Loop through each feature and sequence length to get the corresponding columns\n",
    "        for feature in features:\n",
    "            for i in range(sequence_length):\n",
    "                column_name = f\"{feature}_{i+1}\"\n",
    "                if column_name in df.columns:\n",
    "                    feature_columns.append(column_name)         \n",
    "    elif len(features) > 0 and sequence_length == 1:\n",
    "        # Loop through each feature and sequence length to get the corresponding columns\n",
    "        for feature in features:\n",
    "            for i in range(sequence_length):\n",
    "                column_name = f\"{feature}\"\n",
    "                if column_name in df.columns:\n",
    "                    feature_columns.append(column_name)        \n",
    "    else:\n",
    "        feature_columns = []\n",
    "        \n",
    "    # Extract time info\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    feature_columns.append('Year')\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    feature_columns.append('Month')\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    feature_columns.append('Day')\n",
    "    \n",
    "    # Extract the feature columns for X\n",
    "    X = df[feature_columns].values\n",
    "    \n",
    "    # Extract the target column for y\n",
    "    y = df[target].values\n",
    "    \n",
    "    feature_columns.append(target)\n",
    "    \n",
    "    return X, y, df[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11a9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# !pip install sktime\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, ElasticNet\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# !pip install PrettyTable\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "models = [#{'model': LinearRegression(), 'params': {}},\n",
    "#           {'model': Lasso(random_state=42, max_iter=20000), 'params': {'alpha': np.arange(0.01, 1.0, 0.005)}},\n",
    "#           {'model': Ridge(random_state=42, max_iter=20000), 'params': {'alpha': np.arange(0.01, 1.0, 0.005)}},\n",
    "          {'model': ElasticNet(random_state=42, max_iter=20000), 'params': {'alpha': np.arange(0.01, 1.0, 0.005),\n",
    "                                                                            'l1_ratio': np.arange(0.01, 1.0, 0.005)}}    \n",
    "#           {'model': KNeighborsRegressor(n_jobs=-1), 'params': {'n_neighbors': range(3,23,3),\n",
    "#                                                       'weights': ['uniform', 'distance'],\n",
    "#                                                       'p': [1, 2]}},\n",
    "#           {'model': DecisionTreeRegressor(), 'params': {'max_depth': range(3,23,3),\n",
    "#                                                         'min_samples_split': [2, 4, 6, 8],\n",
    "#                                                         'min_samples_leaf': [1, 2, 3, 5]}},\n",
    "#           {'model': RandomForestRegressor(verbose=0, n_jobs=-1), 'params': {'n_estimators': [10, 20, 50, 70],\n",
    "#                                                         'max_depth': range(3,23,3),\n",
    "#                                                         'min_samples_split': [2, 4, 6],\n",
    "#                                                         'min_samples_leaf': [2, 4, 6]}},\n",
    "#           {'model':LGBMRegressor(n_jobs=-1, verbose=-1,force_col_wise=True), 'params': {'learning_rate': [0.1, 0.01, 0.001],\n",
    "#                                                            'n_estimators': [100,150,200,250,350,500,1000],\n",
    "#                                                            'num_leaves': [20, 31, 40, 60, 80],\n",
    "#                                                            'min_data_in_leaf': range(50,300, 100)}},\n",
    "#           {'model':SVR(verbose=0), 'params':  {'C': [0.1, 1], #10, 100\n",
    "#                                       'gamma': [0.1, 0.01],#1, , 0.001\n",
    "#                                       'kernel': ['linear', 'rbf', 'sigmoid']}},# 'poly',\n",
    "#           {'model':MLPRegressor(verbose=False,max_iter=20000), 'params':  {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,50), (75,35)],\n",
    "#                                                'activation': ['relu','tanh'],#,'logistic'\n",
    "#                                                'alpha': [0.0001, 0.05]}}\n",
    "          ]\n",
    "\n",
    "# Function to scale features\n",
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fcfa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res.;Season;sequence length;Target;Dataset size;# years;Training set size;# years\n",
      "30;Winter;1;T0;1489;17;1191;14\n",
      "30;Spring;1;T0;1564;17;1251;14\n",
      "30;Summer;1;T0;1544;17;1235;14\n",
      "30;Autumn;1;T0;1456;16;1164;13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loc = 'Toolik'\n",
    "sequence_length = 1\n",
    "features = ''\n",
    "trgt = 'T0'\n",
    "res = 30\n",
    "print(f'Res.;Season;sequence length;Target;Dataset size;# years;Training set size;# years')\n",
    "for season in ['Winter','Spring','Summer','Autumn']:\n",
    "        X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        # Split into train and test sets\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        print(f'{res};{season};{sequence_length};{trgt};{X.shape[0]};{np.unique(X[:,-3]).shape[0]};{X_train.shape[0]};{np.unique(X_train[:,-3]).shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2086d5",
   "metadata": {},
   "source": [
    "### direct process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ef7afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res;seqLen;Season;trgt;model;Feature;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2\n",
      "30;1;Winter;T0;ElasticNet;['Tair'];{'alpha': 0.20999999999999996, 'l1_ratio': 0.45999999999999996};84.85188150405884;3.2214573801208983;2.617722120355103;0.5629015107021373;0.5629015107021372;0.00023174285888671875;2.3407272625916735;2.1278625891909693;0.5366634947975679;0.52846834320875\n",
      "30;1;Winter;T16;ElasticNet;['Tair'];{'alpha': 0.07999999999999999, 'l1_ratio': 0.01};82.42842245101929;3.3129634645409944;2.691593806200449;0.43582635101442;0.43582635101442013;0.0004723072052001953;2.853035661960714;2.3306352402583834;0.25892643972481544;0.03515139798985034\n",
      "30;1;Winter;T31;ElasticNet;['Tair'];{'alpha': 0.08999999999999998, 'l1_ratio': 0.01};83.07809090614319;2.3399860592142625;1.89368366607051;0.5335866808620655;0.5335866808620655;0.0004894733428955078;3.5794916570713338;2.8196027019900693;0.24697427493121626;-0.8669116889552859\n",
      "30;1;Winter;T46;ElasticNet;['Tair'];{'alpha': 0.08999999999999998, 'l1_ratio': 0.01};84.576589345932;2.3467707493764567;1.9056605238549056;0.5583925371923142;0.5583925371923142;0.00020003318786621094;2.236343372767598;1.8000653150054298;0.44344846655060144;0.34725477668175064\n",
      "30;1;Winter;T76;ElasticNet;['Tair'];{'alpha': 0.15, 'l1_ratio': 0.01};83.77213096618652;2.261992881796292;1.8437425732662187;0.5669811243937342;0.5669811243937344;0.0009872913360595703;2.033231502257077;1.6778229773163136;0.490997922428012;0.4740656803176887\n",
      "30;1;Winter;T97;ElasticNet;['Tair'];{'alpha': 0.41999999999999993, 'l1_ratio': 0.24499999999999997};84.09474110603333;2.217883757830004;1.8245869637252996;0.5189688319794329;0.5189688319794328;0.00021910667419433594;1.8926887562566546;1.6015965372507626;0.47675214970717994;0.4734815147343252\n",
      "\n",
      "30;1;Spring;T0;ElasticNet;['Tair'];{'alpha': 0.01, 'l1_ratio': 0.26999999999999996};84.38963961601257;1.7965516350302202;1.3589457352329462;0.9623590862147153;0.9623590862147153;0.0001888275146484375;0.9099491074201251;0.7607683368276864;0.9867294947168819;0.9864428556872646\n",
      "30;1;Spring;T16;ElasticNet;['Tair'];{'alpha': 0.24499999999999997, 'l1_ratio': 0.9949999999999999};84.31370258331299;2.265020557563254;1.7879689475666143;0.8958981016606593;0.8958981016606593;0.00018143653869628906;2.3183573361511236;1.9607007887452137;0.9157311347766197;0.8877088068760317\n",
      "30;1;Spring;T31;ElasticNet;['Tair'];{'alpha': 0.21999999999999997, 'l1_ratio': 0.9949999999999999};83.38062047958374;1.702539196962157;1.2646389467506551;0.8593768746453883;0.8593768746453883;0.0001804828643798828;2.870717128861176;2.1265103054392576;0.7640602914345733;0.7602486507586825\n",
      "30;1;Spring;T46;ElasticNet;['Tair'];{'alpha': 0.07499999999999998, 'l1_ratio': 0.9949999999999999};85.73222184181213;1.5460433371982114;1.2104075123982985;0.8166115171104199;0.8166115171104199;0.0001647472381591797;1.3925511467210936;1.124663307167453;0.8392059009389405;0.8369546228208795\n",
      "30;1;Spring;T76;ElasticNet;['Tair'];{'alpha': 0.11499999999999998, 'l1_ratio': 0.9949999999999999};92.18579602241516;1.5372663712049466;1.2147398334111688;0.71402521401364;0.71402521401364;0.0005452632904052734;1.3789556487877164;1.1404598471312453;0.7505871696812028;0.7147015409344271\n",
      "30;1;Spring;T97;ElasticNet;['Tair'];{'alpha': 0.16999999999999998, 'l1_ratio': 0.9849999999999999};83.77903532981873;1.5151898513821693;1.2027651698323238;0.6126426872681222;0.6126426872681223;0.00021004676818847656;1.4760132053580517;1.2078305177763746;0.6695596763092124;0.5455493691478223\n",
      "\n",
      "30;1;Summer;T0;ElasticNet;['Tair'];{'alpha': 0.049999999999999996, 'l1_ratio': 0.01};84.18396186828613;1.011917940137964;0.7851808613718401;0.8770370186204605;0.8770370186204605;0.0001590251922607422;1.1463566144224484;1.0388406759552469;0.9727966156967626;0.8515067809590133\n",
      "30;1;Summer;T16;ElasticNet;['Tair'];{'alpha': 0.27499999999999997, 'l1_ratio': 0.42499999999999993};84.22394442558289;2.401977223878099;1.843112137330183;0.5962382570759777;0.5962382570759777;0.0001804828643798828;1.3663529990695387;1.189999108556808;0.9488083700853248;0.7909281351150111\n",
      "30;1;Summer;T31;ElasticNet;['Tair'];{'alpha': 0.12999999999999998, 'l1_ratio': 0.01};84.58100748062134;2.295953601249903;1.8508873353784163;0.6157363575592383;0.6157363575592383;0.0001761913299560547;2.348104433080092;2.0274438597051625;0.532953695825325;0.36226510306718007\n",
      "30;1;Summer;T46;ElasticNet;['Tair'];{'alpha': 0.01, 'l1_ratio': 0.01};84.47576451301575;0.5631927968907029;0.45799433029261116;0.8162697403495988;0.8162697403495988;0.0001533031463623047;2.02092525754856;1.7621133924946286;0.5469005655019229;-0.6363885279736705\n",
      "30;1;Summer;T76;ElasticNet;['Tair'];{'alpha': 0.01, 'l1_ratio': 0.01};85.34730124473572;0.3139743789802384;0.2302395935979991;0.8656952618346565;0.8656952618346565;0.0002009868621826172;0.34296835168278933;0.26876342579004686;0.6144295776095215;0.5920996174628316\n",
      "30;1;Summer;T97;ElasticNet;['Tair'];{'alpha': 0.05499999999999999, 'l1_ratio': 0.19499999999999998};83.81482744216919;0.3314907110629169;0.22559481946722085;0.8455890363604622;0.8455890363604622;0.0006814002990722656;0.42314761155116953;0.3167816477884802;0.5234068696532964;0.08668879920887518\n",
      "\n",
      "30;1;Autumn;T0;ElasticNet;['Tair'];{'alpha': 0.205, 'l1_ratio': 0.6599999999999999};83.80135083198547;2.550785402152063;2.015091743501259;0.8681220146761924;0.8681220146761924;0.0008196830749511719;1.996514755424437;1.5817075797944407;0.917555564020803;0.9115142731854973\n",
      "30;1;Autumn;T16;ElasticNet;['Tair'];{'alpha': 0.11499999999999998, 'l1_ratio': 0.01};83.94267392158508;2.7236784959180143;2.115312233046651;0.6707509600267099;0.6707509600267099;0.0001900196075439453;3.0092909744099794;2.3552279977297434;0.7627136169036988;0.6584433398582535\n",
      "30;1;Autumn;T31;ElasticNet;['Tair'];{'alpha': 0.11999999999999998, 'l1_ratio': 0.58};85.15840816497803;1.1678330725798143;0.8112943753331199;0.6481403033198526;0.6481403033198526;0.0005550384521484375;2.714772756483651;2.175252753297677;0.595415880112165;0.1622075284858746\n",
      "30;1;Autumn;T46;ElasticNet;['Tair'];{'alpha': 0.42999999999999994, 'l1_ratio': 0.015};84.43069911003113;0.5921416012902857;0.39015757457379746;0.5935877279509562;0.5935877279509563;0.0008654594421386719;0.5414676857836741;0.3906528179424825;0.6959084092987026;0.6120955657307671\n",
      "30;1;Autumn;T76;ElasticNet;['Tair'];{'alpha': 0.24499999999999997, 'l1_ratio': 0.01};84.57881712913513;0.2711799972395421;0.1546265995704588;0.3934770982936514;0.3934770982936513;0.0009722709655761719;0.13135877511193209;0.10244992142977705;0.2963206064978643;0.18272686366852964\n",
      "30;1;Autumn;T97;ElasticNet;['Tair'];{'alpha': 0.175, 'l1_ratio': 0.024999999999999998};83.66013169288635;0.17807288977268082;0.09685197551544579;0.25769425201623786;0.25769425201623775;0.0005266666412353516;0.058693507694241795;0.045258697413701886;-0.957398006890287;-1.644157313876471\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "30;1;Winter;T0;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.03, 'l1_ratio': 0.9949999999999999};83.95559620857239;2.704011986205499;2.1180998799471022;0.6920418157979382;0.6920418157979382;0.0005900859832763672;3.034408704591366;2.599701844121467;0.3166212088807292;0.20757644039460077\n",
      "30;1;Winter;T16;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.44999999999999996, 'l1_ratio': 0.01};85.43419456481934;3.166061315307918;2.5308631715204526;0.48474982581014736;0.48474982581014736;0.0009667873382568359;3.0178397864613586;2.4558494458660673;0.32469590123359515;-0.07953600399582172\n",
      "30;1;Winter;T31;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.07499999999999998, 'l1_ratio': 0.01};86.30902242660522;1.9391523457490942;1.608701013644815;0.6796916475368228;0.6796916475368227;0.0005543231964111328;3.698499752707518;3.1182344532112447;0.3545888877539648;-0.9931145237341421\n",
      "30;1;Winter;T46;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.15, 'l1_ratio': 0.6049999999999999};86.56986999511719;1.9289803138991322;1.6079071822761895;0.7016330862090071;0.7016330862090071;0.0005316734313964844;1.9591002318837216;1.5803597508399287;0.5949723075097981;0.49906654072938583\n",
      "30;1;Winter;T76;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.33499999999999996, 'l1_ratio': 0.019999999999999997};85.53243088722229;1.9359172569057659;1.649042097629139;0.6828257347424884;0.6828257347424884;0.00048351287841796875;1.663230916930255;1.4079033055038874;0.6504311278946726;0.648064602459206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Winter;T97;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.06499999999999999, 'l1_ratio': 0.16499999999999998};85.41521763801575;1.670975739015088;1.3643419507274333;0.7269539206652567;0.7269539206652567;0.0005092620849609375;1.666945474120534;1.3337936020040309;0.6538902136825391;0.5915884576536392\n",
      "\n",
      "30;1;Spring;T0;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.08999999999999998, 'l1_ratio': 0.9549999999999998};91.49963569641113;1.914800612478319;1.4565518395418313;0.9572409679174336;0.9572409679174336;0.0007557868957519531;1.2622551030500084;0.9699641120910332;0.9739167435495926;0.9739127510503659\n",
      "30;1;Spring;T16;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.024999999999999998, 'l1_ratio': 0.9949999999999999};85.8959698677063;1.9534664779958189;1.532383116728135;0.9225669655043798;0.9225669655043798;0.0004680156707763672;1.776251623495886;1.3993458051193715;0.9529763260035045;0.9340835332897417\n",
      "30;1;Spring;T31;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.8799999999999999};86.26117396354675;1.4877984938220443;1.1044668847169796;0.8926132406460892;0.8926132406460892;0.0005245208740234375;2.1076194128360637;1.6851414551173622;0.873773246841644;0.8707696147914017\n",
      "30;1;Spring;T46;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.9949999999999999};85.43254113197327;1.3212437357762494;1.0182417256458245;0.8660648361479155;0.8660648361479155;0.0005738735198974609;1.0840984971553183;0.8728036759811956;0.9024058079576324;0.9011848204677632\n",
      "30;1;Spring;T76;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.9949999999999999};85.90207028388977;1.2845985398737068;1.0094565916676292;0.8003063117982258;0.8003063117982258;0.0005524158477783203;1.0697284323794967;0.8738497427065858;0.8283996651505442;0.8283096413994915\n",
      "30;1;Spring;T97;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.9949999999999999};86.19012188911438;1.2555564896793414;1.000441035354801;0.7340192734933766;0.7340192734933766;0.0005345344543457031;1.0042927897016565;0.8305761046682206;0.7924042421504465;0.7896091181007645\n",
      "\n",
      "30;1;Summer;T0;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.11499999999999998, 'l1_ratio': 0.39499999999999996};86.88000583648682;1.0759604553304718;0.8392691643456118;0.8609802786628433;0.8609802786628433;0.001035451889038086;1.261631957909631;1.1403545515210554;0.9628750774531663;0.8201408641570256\n",
      "30;1;Summer;T16;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.42499999999999993, 'l1_ratio': 0.7599999999999999};85.2101662158966;2.3263931955446386;1.8062090933043768;0.621249134200262;0.621249134200262;0.00048470497131347656;1.0237783945257573;0.8020316030686008;0.9403762802361499;0.8826233569719231\n",
      "30;1;Summer;T31;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.12999999999999998, 'l1_ratio': 0.01};85.61327719688416;2.1103736942586977;1.6782629643192357;0.6753452134508318;0.6753452134508318;0.001611471176147461;2.4758173547484392;2.1683384692709162;0.5255250568871028;0.29100598164948643\n",
      "30;1;Summer;T46;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.019999999999999997, 'l1_ratio': 0.9949999999999999};84.37041926383972;0.4968512761620218;0.3912853863698838;0.8570055028706485;0.8570055028706485;0.0005168914794921875;2.0817702616597256;1.7879010019643635;0.49700385259230095;-0.7364069813384415\n",
      "30;1;Summer;T76;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.01};86.08967566490173;0.26602186880698714;0.209398446080438;0.9035865564950638;0.9035865564950638;0.0010521411895751953;0.37502342075394096;0.2920467699101145;0.5566284178639804;0.5122887180853545\n",
      "30;1;Summer;T97;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.01, 'l1_ratio': 0.01};87.05041146278381;0.2660963516912641;0.2019472778344089;0.9005022529018148;0.9005022529018148;0.0006797313690185547;0.47789839208727797;0.3364708383853214;0.3256243311278182;-0.1649469127301395\n",
      "\n",
      "30;1;Autumn;T0;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.10499999999999998, 'l1_ratio': 0.01};85.8739812374115;2.4005625267181037;1.912315271641612;0.883197939867219;0.883197939867219;0.00022649765014648438;2.311466821813775;1.8185124893436508;0.8839907637021185;0.8813948622723117\n",
      "30;1;Autumn;T16;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.585, 'l1_ratio': 0.01};85.35690498352051;2.7557837542748653;2.1576312185738207;0.6629431912799557;0.6629431912799559;0.0005142688751220703;3.3614566038323295;2.577993852487256;0.7295893289392235;0.5738235858673579\n",
      "30;1;Autumn;T31;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.32499999999999996, 'l1_ratio': 0.345};86.20153427124023;1.1920677207293422;0.8355305282650842;0.6333853293128333;0.6333853293128333;0.00018072128295898438;2.760537028799633;2.1941786046667704;0.5811835187322107;0.13372328129278188\n",
      "30;1;Autumn;T46;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.27499999999999997, 'l1_ratio': 0.15999999999999998};85.79605031013489;0.5892077171921553;0.3946132071424729;0.5976050528286432;0.5976050528286432;0.0005462169647216797;0.48256441487392066;0.32259034953713983;0.7287385719954056;0.6919010321084006\n",
      "30;1;Autumn;T76;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.40499999999999997, 'l1_ratio': 0.01};85.94414114952087;0.2737759012884562;0.15526607085479982;0.3818094885737431;0.3818094885737431;0.0009427070617675781;0.11686561587514374;0.08833281851267465;0.38465761627538086;0.35312175167288384\n",
      "30;1;Autumn;T97;ElasticNet;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];{'alpha': 0.18999999999999997, 'l1_ratio': 0.01};84.36564254760742;0.17482691717750232;0.09506537203495026;0.28450960265247016;0.28450960265246994;0.0010693073272705078;0.057485417048000455;0.04032671194736574;-1.1859049682146248;-1.5364279651433463\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Time+Tair | TIME+Remote sensing\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# sequence length\n",
    "sequence_length = 1\n",
    "# resolution\n",
    "loc = 'Toolik'\n",
    "print(f'Res;seqLen;Season;trgt;model;Feature;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2')\n",
    "# Prepare sequences\n",
    "for features in [['Tair'], ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']]:\n",
    "    for res in [30]:\n",
    "        for season in ['Winter','Spring','Summer','Autumn']:\n",
    "            for trgt in ['T0','T16','T31','T46','T76','T97']:  \n",
    "                for model in models:\n",
    "                    print(f'{res};{sequence_length};{season};{trgt}',end=';')\n",
    "                    print(str(model['model']).split('(')[0],end=';')\n",
    "                    print(f'{features}',end=';')\n",
    "                    X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "                #     print(X.shape)\n",
    "\n",
    "                    # Split into train and test sets\n",
    "                    train_size = int(len(X) * 0.8)\n",
    "                    X_train, X_test = X[:train_size], X[train_size:]\n",
    "                    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "                    # Scale features\n",
    "                    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)   \n",
    "\n",
    "                    # hyperparameter tuning\n",
    "                    if str(model['model']).split('(')[0] != 'LinearRegression' and str(model['model']).split('(')[0] != 'LGBMRegressor':\n",
    "                        model_final = HalvingGridSearchCV(model['model'], model['params'], cv=10, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)#n_iter=15,\n",
    "                    elif str(model['model']).split('(')[0] == 'LGBMRegressor':\n",
    "                        model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "                    else:\n",
    "                        model_final = model['model']\n",
    "\n",
    "                    # Train model\n",
    "                    start_train = time.time()\n",
    "                    model_final.fit(X_train_scaled, y_train)\n",
    "                    end_train = time.time()\n",
    "\n",
    "                    # Evaluate training performance\n",
    "                    y_pred_train = model_final.predict(X_train_scaled)\n",
    "                    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "                    r2_train = r2_score(y_train, y_pred_train)\n",
    "                    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "                    explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "                    train_duration = end_train - start_train\n",
    "\n",
    "                    if str(model['model']).split('(')[0] != 'LinearRegression' and str(model['model']).split('(')[0] != 'LGBMRegressor':\n",
    "                        print(f'{model_final.best_params_};{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "                    elif str(model['model']).split('(')[0] == 'LGBMRegressor':\n",
    "                        print(f'learning_rate: 0.01, min_data_in_leaf: 50, n_estimators: 1000, num_leaves: 60;{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "                    else:\n",
    "                        print(f'-;{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "\n",
    "                    # Test model\n",
    "                    start_test = time.time()\n",
    "                    y_pred_test = model_final.predict(X_test_scaled)\n",
    "                    end_test = time.time()\n",
    "\n",
    "                    # Evaluate testing performance\n",
    "                    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "                    r2_test = r2_score(y_test, y_pred_test)\n",
    "                    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "                    explained_variance_test = explained_variance_score(y_test, y_pred_test)\n",
    "                    test_duration = end_test - start_test\n",
    "\n",
    "                    print(f'{test_duration};{rmse_test};{mae_test};{explained_variance_test};{r2_test}')\n",
    "\n",
    "                    # Write results to a CSV file\n",
    "            #         formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test])\n",
    "            #         formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "            #         y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "            #         y_preds = pd.DataFrame(y_pred, columns=['preds'])\n",
    "            #         results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "            #         results.to_csv(f'Results2/Preliminary{loc}_{trgt}_res{res}D_features{sequence_length}_LGBMRegressor_{season}_results.csv', index=False)\n",
    "            print(f'')\n",
    "        print(f'')\n",
    "    print(f'')\n",
    "    print(f'')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb97ebe",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401ab432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2\n",
      "1;T0;1;Winter;['Tair'];LinearRegression;;0.0009984970092773438;4.698078245392575;3.543870757582556;0.6640701966105087;0.6640701966105087;9.083747863769531e-05;4.383384466534853;3.6055422098026195;0.7940217159973029;0.7911215796226171\n",
      "\n",
      "1;T0;1;Spring;['Tair'];LinearRegression;;0.0008130073547363281;3.1321492142827685;2.3849425545586884;0.9110317843667037;0.9110317843667037;0.00011467933654785156;2.510465597789713;1.9171644289424923;0.9424494404542484;0.9366544060214238\n",
      "\n",
      "1;T0;1;Summer;['Tair'];LinearRegression;;0.0007884502410888672;1.6519572257156934;1.1981771498039637;0.888364225127729;0.888364225127729;9.1552734375e-05;1.5937139929886348;1.376976416195647;0.9718143542434086;0.9077607413786237\n",
      "\n",
      "1;T0;1;Autumn;['Tair'];LinearRegression;;0.0007767677307128906;3.429277516498767;2.600986645173055;0.8287199372014691;0.8287199372014691;0.000225067138671875;2.7832655671689324;2.0806200817560154;0.8948995049769289;0.8935870757942656\n",
      "\n",
      "1;T16;1;Winter;['Tair'];LinearRegression;;0.0009937286376953125;4.139057081370422;3.2179660901620806;0.4378724185453047;0.4378724185453047;8.678436279296875e-05;4.221205206550826;3.3529670422901314;0.5349959098373269;0.5309426232091725\n",
      "\n",
      "1;T16;1;Spring;['Tair'];LinearRegression;;0.0008006095886230469;3.4712764284452566;2.640583610975508;0.8003048096717091;0.8003048096717091;8.440017700195312e-05;3.2409458753583;2.486012982559968;0.8511070850206203;0.8509680881520962\n",
      "\n",
      "1;T16;1;Summer;['Tair'];LinearRegression;;0.0007741451263427734;3.010587759843075;2.248548242598699;0.6503697992561741;0.6503697992561741;8.535385131835938e-05;2.1632358554937663;1.8733270750077196;0.9471378602455996;0.829416326341899\n",
      "\n",
      "1;T16;1;Autumn;['Tair'];LinearRegression;;0.0007715225219726562;3.5619482508425877;2.6936868759322126;0.5888865783162917;0.5888865783162915;8.487701416015625e-05;4.089536368135005;3.0697027312259033;0.6789237008525135;0.6257808584364594\n",
      "\n",
      "1;T31;1;Winter;['Tair'];LinearRegression;;0.0007531642913818359;2.7125822802642467;2.1934847792007446;0.4645497426820907;0.4645497426820907;8.392333984375e-05;3.865931525173478;2.8703262380855;0.34733309353975705;0.03957359479812883\n",
      "\n",
      "1;T31;1;Spring;['Tair'];LinearRegression;;0.0007548332214355469;2.512468274114862;1.9674708219836994;0.7423545721315363;0.7423545721315363;8.416175842285156e-05;3.937712012116215;2.720237947127959;0.6906281268425039;0.6801823330190795\n",
      "\n",
      "1;T31;1;Summer;['Tair'];LinearRegression;;0.00075531005859375;2.7493378939039337;2.19462650067962;0.5746625438877598;0.5746625438877597;8.463859558105469e-05;3.495997578341936;2.84509568254455;0.5122438541819896;0.5111361851368108\n",
      "\n",
      "1;T31;1;Autumn;['Tair'];LinearRegression;;0.0007519721984863281;1.4817546305489977;1.026985756199407;0.5306120089483056;0.5306120089483056;8.630752563476562e-05;3.2092218743798964;2.4474696550265995;0.46958093424438674;0.24416313209338802\n",
      "\n",
      "1;T46;1;Winter;['Tair'];LinearRegression;;0.0007684230804443359;2.6033651422345243;2.1233397688769813;0.500827916244636;0.500827916244636;8.344650268554688e-05;2.2981531358291534;1.8461709271535511;0.46515413985838316;0.4435737386180393\n",
      "\n",
      "1;T46;1;Spring;['Tair'];LinearRegression;;0.0007693767547607422;2.1842775292556285;1.8141627902036057;0.6931277569134213;0.6931277569134213;8.678436279296875e-05;2.1814376859657796;1.7876302065528886;0.7057298374159593;0.6735684360180261\n",
      "\n",
      "1;T46;1;Summer;['Tair'];LinearRegression;;0.0007596015930175781;0.8992628334863497;0.6987833872645189;0.6188363658594009;0.618836365859401;8.511543273925781e-05;2.148130872225603;1.6951940181724792;0.4075505266344066;-0.0037918876240135013\n",
      "\n",
      "1;T46;1;Autumn;['Tair'];LinearRegression;;0.0007457733154296875;0.6363293346732103;0.4819911229434055;0.5309661475999035;0.5309661475999036;8.249282836914062e-05;0.6814769864475463;0.49288463623869605;0.5284107052349891;0.44418062232857447\n",
      "\n",
      "1;T76;1;Winter;['Tair'];LinearRegression;;0.0009932518005371094;2.418024602389648;1.974316972695639;0.5357031267457307;0.5357031267457308;9.083747863769531e-05;2.10720411389659;1.6877428777590535;0.5076498721702905;0.5046721305042478\n",
      "\n",
      "1;T76;1;Spring;['Tair'];LinearRegression;;0.0008039474487304688;1.9983536004480111;1.6603191920305964;0.5877969350732826;0.5877969350732826;8.535385131835938e-05;1.9725587633232582;1.6130743999198338;0.5807769828049105;0.5093645396546659\n",
      "\n",
      "1;T76;1;Summer;['Tair'];LinearRegression;;0.0007736682891845703;0.3207434046888545;0.24555956554411013;0.8336183686656375;0.8336183686656375;8.702278137207031e-05;0.3152022537280317;0.25442727942043586;0.7343814914421695;0.7343362031926202\n",
      "\n",
      "1;T76;1;Autumn;['Tair'];LinearRegression;;0.0007681846618652344;0.25018453499440313;0.15010427174937574;0.35826597065456656;0.35826597065456656;8.678436279296875e-05;0.19114350750011222;0.15469880398770686;-0.5469954036992584;-0.9216454867170956\n",
      "\n",
      "1;T97;1;Winter;['Tair'];LinearRegression;;0.0007550716400146484;2.2260684198030964;1.787163691372735;0.542278198508721;0.5422781985087208;8.320808410644531e-05;1.9774784660287805;1.5863234941320854;0.4978485322169224;0.49047812509772803\n",
      "\n",
      "1;T97;1;Spring;['Tair'];LinearRegression;;0.0007674694061279297;1.8511096958266762;1.5299188453822996;0.49868773560488167;0.49868773560488167;8.702278137207031e-05;1.827723796261771;1.4936887013079472;0.517241082664239;0.40734815202307606\n",
      "\n",
      "1;T97;1;Summer;['Tair'];LinearRegression;;0.0007586479187011719;0.2980908636588889;0.21934541599747273;0.8455018996022051;0.8455018996022051;0.00010251998901367188;0.3358413187380515;0.2591899291841373;0.5450960659702742;0.42340952923868136\n",
      "\n",
      "1;T97;1;Autumn;['Tair'];LinearRegression;;0.0007586479187011719;0.13886058847387228;0.07529843696737326;0.3230302471947937;0.3230302471947937;0.00010132789611816406;0.04609921054519616;0.038721268109517225;-3.2506354662469947;-3.4629218841397513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Baseline1: insitu + daily res\n",
    "\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "loc = 'Toolik'\n",
    "features = ['Tair'] \n",
    "sequence_length = 1\n",
    "print(f'\\n\\n\\n\\n\\nres;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2')\n",
    "for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "    for season in ['Winter','Spring','Summer','Autumn']:\n",
    "        for res in [1]:\n",
    "            model_final = LinearRegression()\n",
    "            print(f'{res};{trgt};{sequence_length};{season};{features}',end=';')\n",
    "            print(str(model_final).split('(')[0],end=';')\n",
    "            # Prepare sequences\n",
    "            X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "            # Scale features\n",
    "            X_train_scaled, X_test_scaled = scale_features(X_train, X_test)   \n",
    "\n",
    "            # Train model\n",
    "            start_train = time.time()\n",
    "            model_final.fit(X_train_scaled, y_train)\n",
    "            end_train = time.time()\n",
    "\n",
    "            # Evaluate training performance\n",
    "            y_pred_train = model_final.predict(X_train_scaled)\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "            explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "            train_duration = end_train - start_train\n",
    "            hyperparams = str(model_final).split('(')[1].replace('\\n', '').replace('              ',' ').replace('force_col_wise=True, ','').replace(', verbose=-1','').replace(')','').replace(', verbose=0','').replace(', n_jobs=-1','')\n",
    "            print(f'{hyperparams};{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "            \n",
    "            # Test model\n",
    "            start_test = time.time()\n",
    "            y_pred_test = model_final.predict(X_test_scaled)\n",
    "            end_test = time.time()\n",
    "\n",
    "            # Evaluate testing performance\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "            explained_variance_test = explained_variance_score(y_test, y_pred_test)\n",
    "            test_duration = end_test - start_test\n",
    "\n",
    "            print(f'{test_duration};{rmse_test};{mae_test};{explained_variance_test};{r2_test}')\n",
    "\n",
    "            # Write results to a CSV file\n",
    "            formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test[:,-3:]])\n",
    "            formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "            y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "            y_preds = pd.DataFrame(y_pred_test, columns=['preds'])\n",
    "            results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "            if res == 1:\n",
    "                RES = ''\n",
    "            else:\n",
    "                RES = '_res'+ str(res) + 'D'\n",
    "            model_name = str(model_final).split('(')[0]\n",
    "            results.to_csv(f'Results3/Baseline1_{loc}_{RES}_{trgt}_{sequence_length}_{season}_results.csv', index=False)\n",
    "        print(f'')\n",
    "    # #PLOT\n",
    "    # # Assuming model_final is your trained LightGBM model\n",
    "    # get_lgbm_varimp(model_final, sequence_length, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f13a28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baseline2Input_switch(trgt, season):\n",
    "    feature = ['Tair']\n",
    "    if trgt == 'T0':\n",
    "        if season == 'Winter':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Spring':\n",
    "            model_final = SVR(C= 1, gamma = 0.1, kernel = 'linear')\n",
    "        if season == 'Summer':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LinearRegression()\n",
    "\n",
    "    elif trgt == \"T16\":\n",
    "        if season == 'Winter':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "        if season == 'Autumn':\n",
    "            model_final = SVR(C= 1, gamma = 0.1, kernel = 'rbf')    \n",
    "\n",
    "    elif trgt == \"T31\":\n",
    "        if season == 'Winter':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 1, min_samples_split = 2)\n",
    "        if season == 'Spring':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 1, min_samples_split = 4)\n",
    "        if season == 'Summer':\n",
    "            model_final = RandomForestRegressor(max_depth = 12, min_samples_leaf = 2, min_samples_split = 4, n_estimators = 20, verbose=0, n_jobs=-1)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "\n",
    "    elif trgt == \"T46\":\n",
    "        if season == 'Winter':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Spring':\n",
    "            model_final = SVR(C = 0.1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "        if season == 'Summer':\n",
    "            model_final = MLPRegressor(activation = 'tanh', alpha = 0.005, hidden_layer_sizes = (75, 35), verbose=False, max_iter=20000)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LinearRegression()   \n",
    "\n",
    "    elif trgt == \"T76\":\n",
    "        if season == 'Winter':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = SVR(C = 1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "        if season == 'Autumn':\n",
    "            model_final = KNeighborsRegressor(n_neighbors = 18, p = 1, weights = 'distance', n_jobs=-1)   \n",
    "\n",
    "    elif trgt == \"T97\":\n",
    "        if season == 'Winter':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 2, min_samples_split = 8) \n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = SVR(C = 1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "            \n",
    "    return feature, model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39178a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2\n",
      "30;T0;1;Winter;['Tair'];LinearRegression;;0.0010073184967041016;3.1910435713958294;2.621340570601002;0.5711158526607181;0.5711158526607181;0.00022840499877929688;2.4575913187884493;2.2450647184315846;0.4939935640263112;0.48020923820535355\n",
      "\n",
      "30;T0;1;Spring;['Tair'];SVR;C=1, gamma=0.1, kernel='linear';0.046190500259399414;1.847386355120869;1.3328617170793422;0.9603954668667296;0.9601987956705758;0.004945039749145508;1.0805471703580585;0.860593453923873;0.9843336760757643;0.9808829237155322\n",
      "\n",
      "30;T0;1;Summer;['Tair'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;1.2689969539642334;0.28660598597372716;0.17370712614113804;0.9901359717592061;0.9901359717592061;0.002457141876220703;0.36662640200158747;0.27679979248858844;0.9861635409422227;0.9848115095250283\n",
      "\n",
      "30;T0;1;Autumn;['Tair'];LinearRegression;;0.0013017654418945312;2.499989013851107;1.9684885886562982;0.8733221574108748;0.8733221574108748;8.654594421386719e-05;1.780873085012322;1.365963787833002;0.9304898848251133;0.9295965219339807\n",
      "\n",
      "30;T16;1;Winter;['Tair'];LinearRegression;;0.0008425712585449219;3.308296916376127;2.692055354975295;0.43741459026297846;0.4374145902629787;8.535385131835938e-05;2.9135553565469015;2.4029522322347545;0.23535823647716192;-0.006216232963841328\n",
      "\n",
      "30;T16;1;Spring;['Tair'];LinearRegression;;0.0020530223846435547;2.2399627626491774;1.8212497348393453;0.8981887076435928;0.8981887076435928;8.654594421386719e-05;2.149515561696663;1.793508502708035;0.9257706495908106;0.9034691497599492\n",
      "\n",
      "30;T16;1;Summer;['Tair'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;1.2560949325561523;0.4932465367529267;0.34716213485050734;0.9829738880095008;0.9829738880095008;0.001352548599243164;0.8907567071637735;0.7705114653819972;0.9533760414437097;0.9111437528901163\n",
      "\n",
      "30;T16;1;Autumn;['Tair'];SVR;C=1, gamma=0.1;0.04281282424926758;2.61926327292739;1.7518185683478007;0.7117949989520531;0.6955113280010747;0.011100530624389648;3.004584659685125;2.2257882037022245;0.723078684751149;0.6595108445737812\n",
      "\n",
      "30;T31;1;Winter;['Tair'];DecisionTreeRegressor;max_depth=3;0.0009188652038574219;2.101103047414954;1.6222047141215923;0.6239556143145173;0.6239556143145173;0.00023293495178222656;3.0645687359767146;2.474040856721971;0.254737566374164;-0.3684212171647847\n",
      "\n",
      "30;T31;1;Spring;['Tair'];DecisionTreeRegressor;max_depth=3, min_samples_split=4;0.0008704662322998047;1.4995588002557727;1.1597385749188796;0.8909088532511265;0.8909088532511263;0.0002224445343017578;2.0909727555827198;1.6422704268611763;0.8777624738978199;0.8728029593960992\n",
      "\n",
      "30;T31;1;Summer;['Tair'];RandomForestRegressor;max_depth=12, min_samples_leaf=2, min_samples_split=4,         n_estimators=20;0.037593841552734375;0.16470130644045225;0.09017641287035869;0.998022598610653;0.9980225879838333;0.006127119064331055;0.9873647979409893;0.7782285003867826;0.8885675849802486;0.8872385519601395\n",
      "\n",
      "30;T31;1;Autumn;['Tair'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;3.39237642288208;0.3213484743329693;0.21426945812331974;0.9733583898825561;0.9733583898825561;0.002162456512451172;1.9033603935453165;1.424298610441318;0.704558865232545;0.5881761021491301\n",
      "\n",
      "30;T46;1;Winter;['Tair'];LinearRegression;;0.0013637542724609375;2.3357975867718395;1.883483560295931;0.5625126680527758;0.5625126680527758;8.869171142578125e-05;2.384744558003411;1.8548562633240495;0.4249486046936133;0.2577495636101339\n",
      "\n",
      "30;T46;1;Spring;['Tair'];SVR;C=0.1, gamma=0.1, kernel='linear';0.03477835655212402;1.5519528163585208;1.1889993797596947;0.8169445323790752;0.8152068972315875;0.005036592483520508;1.3794355207942237;1.1218331073736072;0.8467772904209354;0.8400114179874363\n",
      "\n",
      "30;T46;1;Summer;['Tair'];MLPRegressor;activation='tanh', alpha=0.005, hidden_layer_sizes=;1.4573495388031006;0.3233261837063159;0.2366963686719114;0.9395225526092303;0.9394452440817688;0.00037026405334472656;1.2288695969230663;1.0069666030664974;0.7177912610021797;0.3949412833136543\n",
      "\n",
      "30;T46;1;Autumn;['Tair'];LinearRegression;;0.0009450912475585938;0.5714413579376624;0.41544791467030917;0.6215059957807001;0.6215059957807001;8.702278137207031e-05;0.5148941298558846;0.36573181695723644;0.7377115988903082;0.6492355764510043\n",
      "\n",
      "30;T76;1;Winter;['Tair'];LinearRegression;;0.0008311271667480469;2.234587445559185;1.7999042825726104;0.5774101428433063;0.5774101428433063;8.463859558105469e-05;2.1575275584065507;1.7146979498016302;0.47832907168697447;0.40779706543541405\n",
      "\n",
      "30;T76;1;Spring;['Tair'];LinearRegression;;0.0007958412170410156;1.5208895432346503;1.1867647201310456;0.7200858597718365;0.7200858597718365;0.00010728836059570312;1.2671988463807424;1.0262469715025464;0.7602011229605966;0.7590713943073192\n",
      "\n",
      "30;T76;1;Summer;['Tair'];SVR;C=1, gamma=0.1, kernel='linear';0.06740736961364746;0.3223864043704163;0.22348693179541473;0.8587405856746099;0.8584022503591191;0.003618478775024414;0.307512320408765;0.2470796318350145;0.7019681599339964;0.6720776173079752\n",
      "\n",
      "30;T76;1;Autumn;['Tair'];KNeighborsRegressor;n_jobs=-1, n_neighbors=18, p=1, weights='distance';0.0007681846618652344;0.0;0.0;1.0;1.0;0.016749858856201172;0.1630715429468228;0.09544302552461698;-0.24533661789036576;-0.25952097588023193\n",
      "\n",
      "30;T97;1;Winter;['Tair'];DecisionTreeRegressor;max_depth=3, min_samples_leaf=2, min_samples_split=8;0.0009577274322509766;1.5864425260556074;1.2366114796787002;0.7538814532062466;0.7538814532062466;0.00046634674072265625;1.4420173397767064;1.1237459444790456;0.694553265609327;0.6943698572890139\n",
      "\n",
      "30;T97;1;Spring;['Tair'];LinearRegression;;0.0012042522430419922;1.4826351511953928;1.1650593077729585;0.6291090482167683;0.6291090482167683;8.392333984375e-05;1.226549398205854;1.0213102197289423;0.6941232782455127;0.6861831114496757\n",
      "\n",
      "30;T97;1;Summer;['Tair'];SVR;C=1, gamma=0.1, kernel='linear';0.07273650169372559;0.3439463634757782;0.2202258075676338;0.8353205689531783;0.8337671499435049;0.0036225318908691406;0.40165988474794867;0.30576150083499903;0.6055238637868633;0.17709080557556178\n",
      "\n",
      "30;T97;1;Autumn;['Tair'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;1.331000566482544;0.08584155866752985;0.037371504766785774;0.8275026609806557;0.8275026609806557;0.0016961097717285156;0.08087216079720166;0.0417081099239052;-3.699476587589033;-4.020017764448405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Baseline2: insitu + monthly res\n",
    "\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "loc = 'Toolik'\n",
    "features = ['Tair'] \n",
    "sequence_length = 1\n",
    "print(f'\\n\\n\\n\\n\\nres;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2')\n",
    "for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "    for season in ['Winter','Spring','Summer','Autumn']:\n",
    "        for res in [30]:\n",
    "            features, model_final = Baseline2Input_switch(trgt, season)\n",
    "            print(f'{res};{trgt};{sequence_length};{season};{features}',end=';')\n",
    "            print(str(model_final).split('(')[0],end=';')\n",
    "            # Prepare sequences\n",
    "            X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "            # Scale features\n",
    "            X_train_scaled, X_test_scaled = scale_features(X_train, X_test)   \n",
    "\n",
    "            # Train model\n",
    "            start_train = time.time()\n",
    "            model_final.fit(X_train_scaled, y_train)\n",
    "            end_train = time.time()\n",
    "\n",
    "            # Evaluate training performance\n",
    "            y_pred_train = model_final.predict(X_train_scaled)\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "            explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "            train_duration = end_train - start_train\n",
    "            hyperparams = str(model_final).split('(')[1].replace('\\n', '').replace('              ',' ').replace('force_col_wise=True, ','').replace(', verbose=-1','').replace(')','').replace(', verbose=0','').replace(', n_jobs=-1','')\n",
    "            print(f'{hyperparams};{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "            \n",
    "            # Test model\n",
    "            start_test = time.time()\n",
    "            y_pred_test = model_final.predict(X_test_scaled)\n",
    "            end_test = time.time()\n",
    "\n",
    "            # Evaluate testing performance\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "            explained_variance_test = explained_variance_score(y_test, y_pred_test)\n",
    "            test_duration = end_test - start_test\n",
    "\n",
    "            print(f'{test_duration};{rmse_test};{mae_test};{explained_variance_test};{r2_test}')\n",
    "\n",
    "            # Write results to a CSV file\n",
    "            formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test[:,-3:]])\n",
    "            formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "            y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "            y_preds = pd.DataFrame(y_pred_test, columns=['preds'])\n",
    "            results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "            if res == 1:\n",
    "                RES = ''\n",
    "            else:\n",
    "                RES = '_res'+ str(res) + 'D'\n",
    "            model_name = str(model_final).split('(')[0]\n",
    "            results.to_csv(f'Results3/Baseline2_{loc}_{RES}_{trgt}_{sequence_length}_{season}_results.csv', index=False)\n",
    "        print(f'')\n",
    "    # #PLOT\n",
    "    # # Assuming model_final is your trained LightGBM model\n",
    "    # get_lgbm_varimp(model_final, sequence_length, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7824603",
   "metadata": {},
   "source": [
    "## Impact of input length on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44e2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BestModelInput_switch(trgt, season):\n",
    "\n",
    "#     if trgt == 'T0':\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = LinearRegression()\n",
    "#         if season == 'Spring':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = LinearRegression()\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = RandomForestRegressor(max_depth = 15, min_samples_leaf = 4, min_samples_split = 4, n_estimators = 20, verbose=0, n_jobs=-1)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = LinearRegression()\n",
    "\n",
    "#     elif trgt == \"T16\":\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Spring':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = SVR(C = 1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = RandomForestRegressor(max_depth = 9, min_samples_leaf = 2, min_samples_split = 2, n_estimators = 70, verbose=0, n_jobs=-1)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "\n",
    "#     elif trgt == \"T31\":\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Spring':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = RandomForestRegressor(max_depth = 12, min_samples_leaf = 2, min_samples_split = 4, n_estimators = 20, verbose=0, n_jobs=-1)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "\n",
    "#     elif trgt == \"T46\":\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = SVR(C = 0.1, gamma = 0.1, kernel = 'rbf', verbose=0)\n",
    "#         if season == 'Spring':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = MLPRegressor(activation = 'relu', alpha = 0.0001, hidden_layer_sizes = (75, 35), verbose=False, max_iter=20000)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = RandomForestRegressor(max_depth = 18, min_samples_leaf = 2, min_samples_split = 6, n_estimators = 20, verbose=0, n_jobs=-1)   \n",
    "\n",
    "#     elif trgt == \"T76\":\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = RandomForestRegressor(max_depth = 9, min_samples_leaf = 4, min_samples_split = 6, n_estimators = 70, verbose=0, n_jobs=-1) \n",
    "#         if season == 'Spring':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = SVR(C = 1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = KNeighborsRegressor(n_neighbors = 18, p = 1, weights = 'distance', n_jobs=-1)   \n",
    "\n",
    "#     elif trgt == \"T97\":\n",
    "#         if season == 'Winter':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = RandomForestRegressor(max_depth = 21, min_samples_leaf = 4, min_samples_split = 2, n_estimators = 70, verbose=0, n_jobs=-1) \n",
    "#         if season == 'Spring':\n",
    "#             feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "#             model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)\n",
    "#         if season == 'Summer':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = SVR(C = 1, gamma = 0.1, kernel = 'linear', verbose=0)\n",
    "#         if season == 'Autumn':\n",
    "#             feature = ['Tair']\n",
    "#             model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 5, min_samples_split = 6)   \n",
    "            \n",
    "#     return feature, model_final\n",
    "\n",
    "def BestModelInput_switch(trgt, season):\n",
    "    feature = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "    if trgt == 'T0':\n",
    "        if season == 'Winter':\n",
    "            model_final = SVR(C= 0.1, gamma = 0.1, kernel = 'linear')\n",
    "        if season == 'Spring':\n",
    "            model_final = SVR(C= 0.1, gamma = 0.1, kernel = 'linear')\n",
    "        if season == 'Summer':\n",
    "            model_final = RandomForestRegressor(max_depth = 6, min_samples_leaf = 2, min_samples_split = 6, n_estimators = 50, verbose=0, n_jobs=-1)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LinearRegression()\n",
    "\n",
    "    elif trgt == \"T16\":\n",
    "        if season == 'Winter':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 6, min_samples_leaf = 2, min_samples_split = 8)\n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = RandomForestRegressor(max_depth = 15, min_samples_leaf = 2, min_samples_split = 6, n_estimators = 70, verbose=0, n_jobs=-1)\n",
    "        if season == 'Autumn':\n",
    "            model_final = SVR(C= 0.1, gamma = 0.1, kernel = 'rbf')    \n",
    "\n",
    "    elif trgt == \"T31\":\n",
    "        if season == 'Winter':\n",
    "            model_final = RandomForestRegressor(max_depth = 15, min_samples_leaf = 2, min_samples_split = 2, n_estimators = 50, verbose=0, n_jobs=-1)\n",
    "        if season == 'Spring':\n",
    "            model_final = RandomForestRegressor(max_depth = 6, min_samples_leaf = 2, min_samples_split = 2, n_estimators = 70, verbose=0, n_jobs=-1)\n",
    "        if season == 'Summer':\n",
    "            model_final = RandomForestRegressor(max_depth = 9, min_samples_leaf = 2, min_samples_split = 4, n_estimators = 70, verbose=0, n_jobs=-1)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "\n",
    "    elif trgt == \"T46\":\n",
    "        if season == 'Winter':\n",
    "            model_final = SVR(C = 1, gamma = 0.1, kernel = 'rbf', verbose=0)\n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = MLPRegressor(activation = 'relu', alpha = 0.0001, hidden_layer_sizes = (75, 35), verbose=False, max_iter=20000)\n",
    "        if season == 'Autumn':\n",
    "            model_final = LinearRegression()   \n",
    "\n",
    "    elif trgt == \"T76\":\n",
    "        if season == 'Winter':\n",
    "            model_final = SVR(C = 1, gamma = 0.1, kernel = 'rbf', verbose=0)\n",
    "        if season == 'Spring':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 3, min_samples_split = 4)\n",
    "        if season == 'Summer':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Autumn':\n",
    "            model_final = KNeighborsRegressor(n_neighbors = 18, p = 1, weights = 'distance', n_jobs=-1)   \n",
    "\n",
    "    elif trgt == \"T97\":\n",
    "        if season == 'Winter':\n",
    "            model_final = DecisionTreeRegressor(max_depth = 3, min_samples_leaf = 3, min_samples_split = 4) \n",
    "        if season == 'Spring':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Summer':\n",
    "            model_final = LinearRegression()\n",
    "        if season == 'Autumn':\n",
    "            model_final = LGBMRegressor(learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60, n_jobs=-1, verbose=-1,force_col_wise=True)    \n",
    "            \n",
    "    return feature, model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ecbb974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2\n",
      "30;T0;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;C=0.1, gamma=0.1, kernel='linear';0.5119249820709229;2.4692590938252206;1.5290472261018166;0.7400972958881955;0.7354752030578858;0.051882028579711914;4.182660932868903;3.518294829099853;-0.5079621256167655;-0.5786010979959557\n",
      "\n",
      "30;T0;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;C=0.1, gamma=0.1, kernel='linear';0.5040614604949951;1.2372369831041354;0.7593421408567249;0.9832970900485756;0.9828339456496967;0.05270242691040039;4.071820136305824;2.861889836562539;0.8466140700148594;0.741117943430604\n",
      "\n",
      "30;T0;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;max_depth=6, min_samples_leaf=2, min_samples_split=6,         n_estimators=50;0.38544774055480957;0.3450209104380672;0.25410414327091124;0.9848918547740241;0.9848917102513711;0.01109766960144043;1.5848751912173749;1.0214650159895335;0.792228558001896;0.7134657008249292\n",
      "\n",
      "30;T0;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.08977413177490234;1.6266376261717843;1.239462458901098;0.9494817936245643;0.9494817936245643;0.020860671997070312;3.0761629790852028;2.3604720745941994;0.7937940954267912;0.7700101658687707\n",
      "\n",
      "30;T16;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;max_depth=6, min_samples_leaf=2, min_samples_split=8;0.19079113006591797;1.1684794739530264;0.7754082774500632;0.9285974941579517;0.9285974941579517;0.000385284423828125;3.8893027275884298;3.1205145165959265;-0.7726580365850984;-0.7991807925919863\n",
      "\n",
      "30;T16;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.08947348594665527;1.4231336468193403;1.113844851550044;0.960714164547521;0.960714164547521;0.024854660034179688;4.6332728188728804;3.587661563235959;0.816240795988434;0.5719101711396548\n",
      "\n",
      "30;T16;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;max_depth=15, min_samples_leaf=2, min_samples_split=6,         n_estimators=70;0.708749532699585;0.23816563282921738;0.1436464605426605;0.9959688474839561;0.9959676444178572;0.014180898666381836;1.3136284399685545;1.0839633443095744;0.9018491867083481;0.8048702385684963\n",
      "\n",
      "30;T16;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;C=0.1, gamma=0.1;0.2662687301635742;4.836758155732053;3.756709579361671;0.04071238423774659;0.039181910660344044;0.09922099113464355;6.179368100989272;4.842611733193902;0.0003486531377729918;-0.4160941402543499\n",
      "\n",
      "30;T31;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;max_depth=15, min_samples_leaf=2, n_estimators=50,         n_jobs=-1;0.4727296829223633;0.19935612133279892;0.12538376514093721;0.9966048889750367;0.9966031707198834;0.011624813079833984;2.8609984951646776;2.449555834315493;0.13862152629561852;-0.16081152169166169\n",
      "\n",
      "30;T31;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;max_depth=6, min_samples_leaf=2, n_estimators=70,         n_jobs=-1;0.49924254417419434;0.4422240331857154;0.331096634277486;0.9908561199151236;0.990854672841699;0.013686418533325195;1.7898333222659826;1.4652833000620238;0.9112982690768032;0.9106445364420483\n",
      "\n",
      "30;T31;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;max_depth=9, min_samples_leaf=2, min_samples_split=4,         n_estimators=70;0.6285004615783691;0.12056035483131519;0.07737096804076578;0.9990088894664679;0.9990088290433196;0.014074563980102539;1.2665054248294314;1.0018738518032824;0.81494563991743;0.8140735191359606\n",
      "\n",
      "30;T31;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;37.17142367362976;0.12447496200776821;0.053615262687774064;0.9963325067403432;0.9963325067403432;0.04790329933166504;2.1591141728186747;1.5375745827787408;0.6518732661387016;0.49657225661921633\n",
      "\n",
      "30;T46;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;C=1, gamma=0.1;0.32853007316589355;2.6738101316270746;1.9182582654183027;0.43180992052554523;0.41927484440486806;0.10541963577270508;3.2721408199157485;2.668018533122759;6.251260989864615e-05;-0.37658316462420016\n",
      "\n",
      "30;T46;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.10585498809814453;0.7296793743718902;0.5753317414157005;0.9605686893642774;0.9605686893642774;0.010680198669433594;2.7969560430977705;2.1199380030870274;0.6662167927374517;0.36570961254799816\n",
      "\n",
      "30;T46;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;hidden_layer_sizes=;1.0319287776947021;0.03329830975251088;0.02549088688631397;0.9993927633409972;0.9993732472704958;0.0008945465087890625;2.5586300131434117;2.411467479735176;0.7198868729285146;-1.5071844074238494\n",
      "\n",
      "30;T46;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.08481550216674805;0.37665555020387925;0.27506435286315395;0.8452279509900349;0.8452279509900349;0.0022156238555908203;1.0266045099599226;0.8484132013113023;0.315602707930295;-0.33134628763156604\n",
      "\n",
      "30;T76;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;C=1, gamma=0.1;0.33196187019348145;2.553942383197837;1.884300913940059;0.4566485006418234;0.43976205424696413;0.1014711856842041;3.435288404157983;2.821967469406276;6.462399241369265e-05;-0.4800236112974481\n",
      "\n",
      "30;T76;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;max_depth=3, min_samples_leaf=3, min_samples_split=4;0.11617922782897949;1.1869076873667896;0.9027546299937084;0.8359275007382412;0.8359275007382412;0.0007123947143554688;1.2078987889388206;1.0232210048961268;0.7996830122848881;0.7879772103125429\n",
      "\n",
      "30;T76;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.10564565658569336;0.16832033464458818;0.13234087933961255;0.9569006264249239;0.9569006264249239;0.0021986961364746094;0.7264388211327022;0.5952080652298485;0.011778677838377605;-0.7485773131350439\n",
      "\n",
      "30;T76;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;n_jobs=-1, n_neighbors=18, p=1, weights='distance';0.002156972885131836;0.0;0.0;1.0;1.0;0.21038222312927246;0.1159456645890584;0.07238750503008633;0.5218759261241126;0.3977086986113071\n",
      "\n",
      "30;T97;84;Winter;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;max_depth=3, min_samples_leaf=3, min_samples_split=4;0.10550498962402344;1.4815772771113604;1.1482268707694812;0.7817903382307301;0.7817903382307301;0.0006248950958251953;2.0508564364881217;1.4314410618928357;0.4416223112228528;0.3925549102071382\n",
      "\n",
      "30;T97;84;Spring;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.1016230583190918;0.6988736197329108;0.5598696206234389;0.9208523291616677;0.9208523291616677;0.00035572052001953125;2.51833045530676;1.9491937648597133;0.2896140826011192;-0.2837547885212426\n",
      "\n",
      "30;T97;84;Summer;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;;0.38364315032958984;0.1413606232431145;0.09999792034975406;0.9678925490531519;0.9678925490531519;0.016898393630981445;0.6372051457972148;0.4922634102853035;-0.24230428293600914;-0.9822445387531882\n",
      "\n",
      "30;T97;84;Autumn;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;learning_rate=0.01, min_data_in_leaf=50, n_estimators=1000, num_leaves=60;1985.7973091602325;0.050851134153348965;0.015014339282148795;0.9427593594096849;0.9427593594096849;0.05614447593688965;0.05243113597376602;0.03308944910512278;-0.9264384814527691;-0.9893143623292198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Baseline: features for different lengths...\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "loc = 'Toolik'\n",
    "# resolution\n",
    "res = 30\n",
    "# sequence length\n",
    "print(f'\\n\\n\\n\\n\\nres;Target;Len;Season;Feature;model;bestparam;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2')\n",
    "for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "    for season in ['Winter','Spring','Summer','Autumn']:\n",
    "        for sequence_length in [84]:#[1,4,7,10,14,21,28,35]:\n",
    "            features, model_final = BestModelInput_switch(trgt, season)\n",
    "            print(f'{res};{trgt};{sequence_length};{season};{features}',end=';')\n",
    "            print(str(model_final).split('(')[0],end=';')\n",
    "            # Prepare sequences\n",
    "            X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "            # Scale features\n",
    "            X_train_scaled, X_test_scaled = scale_features(X_train, X_test)   \n",
    "\n",
    "            # Train model\n",
    "            start_train = time.time()\n",
    "            model_final.fit(X_train_scaled, y_train)\n",
    "            end_train = time.time()\n",
    "\n",
    "            # Evaluate training performance\n",
    "            y_pred_train = model_final.predict(X_train_scaled)\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "            explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "            train_duration = end_train - start_train\n",
    "            hyperparams = str(model_final).split('(')[1].replace('\\n', '').replace('              ',' ').replace('force_col_wise=True, ','').replace(', verbose=-1','').replace(')','').replace(', verbose=0','').replace(', n_jobs=-1','')\n",
    "            print(f'{hyperparams};{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "            \n",
    "            # Test model\n",
    "            start_test = time.time()\n",
    "            y_pred_test = model_final.predict(X_test_scaled)\n",
    "            end_test = time.time()\n",
    "\n",
    "            # Evaluate testing performance\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "            explained_variance_test = explained_variance_score(y_test, y_pred_test)\n",
    "            test_duration = end_test - start_test\n",
    "\n",
    "            print(f'{test_duration};{rmse_test};{mae_test};{explained_variance_test};{r2_test}')\n",
    "\n",
    "            # Write results to a CSV file\n",
    "            formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test[:,-3:]])\n",
    "            formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "            y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "            y_preds = pd.DataFrame(y_pred_test, columns=['preds'])\n",
    "            results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "            if res == 1:\n",
    "                RES = ''\n",
    "            else:\n",
    "                RES = '_res'+ str(res) + 'D'\n",
    "            model_name = str(model_final).split('(')[0]\n",
    "            results.to_csv(f'Results3/InputLength_{loc}_{RES}_{trgt}_{sequence_length}_{season}_{model_name}_results.csv', index=False)\n",
    "        print(f'')\n",
    "    # #PLOT\n",
    "    # # Assuming model_final is your trained LightGBM model\n",
    "    # get_lgbm_varimp(model_final, sequence_length, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655a1b4",
   "metadata": {},
   "source": [
    "### Impact of train size on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a326944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_indices(df, num_years):\n",
    "    \"\"\"Get indices for the last `num_years` years.\"\"\"\n",
    "    max_year = df['Year'].max()\n",
    "    start_year = max_year - num_years + 1\n",
    "    return df[df['Year'].between(start_year, max_year)].index    \n",
    "\n",
    "def get_optimal_sequence_length(loc, season, trgt):\n",
    "    if trgt == 'T16' and season == 'Winter':\n",
    "        sequence_length = 34\n",
    "    else: \n",
    "        if trgt == 'T31' and season == 'Spring':\n",
    "            sequence_length = 6    \n",
    "        else: \n",
    "            if trgt == 'T76' and season == 'Spring':\n",
    "                sequence_length = 6\n",
    "            else:\n",
    "                sequence_length = 1\n",
    "    return sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67d440e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res;seqLen;season;target;trSize;feature;model;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2\n",
      "30;1;Winter;T0;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0006785392761230469;0.19469560415129658;0.15593619446729023;0.98624169171885;0.9861907286811257;0.0002167224884033203;5.301238803946587;4.21783125983912;-0.34572951823391507;-1.4185995322923493\n",
      "\n",
      "30;1;Winter;T0;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0010612010955810547;0.5264269994561808;0.3689332437564934;0.9837173948518233;0.9836135309785717;0.0005412101745605469;7.686822994182455;6.994329325303378;-0.38653081451286075;-4.085140880473887\n",
      "\n",
      "30;1;Winter;T0;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0023467540740966797;0.9304906932120071;0.7505452643172216;0.956246679948187;0.956194870189147;0.0010488033294677734;4.006392356876815;3.4326727085443145;0.07385187036814378;-0.38138961072886746\n",
      "\n",
      "30;1;Winter;T0;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.002969980239868164;2.0330463042987956;1.540028727076414;0.8173819251763869;0.8167691015397549;0.00159454345703125;5.4333831265765635;4.874707172142494;0.13838379049787175;-1.5406795238270141\n",
      "\n",
      "30;1;Winter;T0;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0064504146575927734;2.1036650026070385;1.4687980170412616;0.7948434087655012;0.7916236586244616;0.0017955303192138672;3.8211070473245634;3.310020684442427;0.2910419245735604;-0.2565727490648164\n",
      "\n",
      "30;1;Winter;T0;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.006845235824584961;2.5382668497676217;1.8256721325145404;0.7393432992532915;0.7304251735966149;0.002306222915649414;4.340192443017226;3.6433864916874588;-0.29690136112849763;-0.6211649325503363\n",
      "\n",
      "30;1;Winter;T0;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.00944662094116211;2.4903099608998396;1.890479950274441;0.7315504465543672;0.7300164933343092;0.0027055740356445312;4.235415762129885;3.362444948243899;-0.2598921826731162;-0.5438365732086794\n",
      "\n",
      "30;1;Winter;T0;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.013979434967041016;2.377546733190991;1.75447456957462;0.7358820255261007;0.735536232784372;0.0031554698944091797;4.5612848730742;3.6164091012323185;-0.18847409258131664;-0.79053832668734\n",
      "\n",
      "30;1;Winter;T0;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.017038822174072266;2.328228493590783;1.6945597041473446;0.7722114775184812;0.7718664038711683;0.0033931732177734375;4.426780430552623;3.5373997634878007;-0.14558820965485264;-0.6864955160015769\n",
      "\n",
      "30;1;Winter;T0;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.020897388458251953;2.9076990389895756;2.147419117866314;0.6756883343096995;0.6725918194844074;0.0038144588470458984;3.10183431410334;2.524547320023328;0.17862072339499957;0.17196933581390272\n",
      "\n",
      "30;1;Winter;T0;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.024715185165405273;2.920587840093752;2.264489922764029;0.6485789051342745;0.6443063460463989;0.0041599273681640625;3.1125144052393843;2.648218748316759;0.29614342453553966;0.16625744618673277\n",
      "\n",
      "30;1;Winter;T0;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.030195236206054688;2.80395360146556;2.131921342340524;0.6662249029198344;0.6632063543423625;0.004650115966796875;3.0446558772325134;2.574359575775885;0.32490655296161153;0.20221538816438356\n",
      "\n",
      "30;1;Winter;T0;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.032779693603515625;2.763671987532865;2.1244194025316783;0.6714745687013439;0.6698968751997072;0.005092620849609375;3.093501928567653;2.6411888263593095;0.34948936185882873;0.17641199979114175\n",
      "\n",
      "30;1;Winter;T0;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.036551713943481445;2.7358282988970815;2.08950373654479;0.6857225671024718;0.6847521012807094;0.005228757858276367;3.04928278387913;2.513455927515788;0.30752399287632637;0.19978878899995367\n",
      "\n",
      "30;1;Winter;T16;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.00058746337890625;0.21492905941213106;0.1720585474537077;0.9830475976524791;0.9830475976524791;7.724761962890625e-05;2.894283040685914;2.390400220606624;0.023225984067711702;0.007051392402276924\n",
      "\n",
      "30;1;Winter;T16;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0005440711975097656;0.25754008811790163;0.1812718621568855;0.9958055123577072;0.9958055123577072;8.058547973632812e-05;2.9321965304642106;2.3121830352704427;0.16085956226332676;-0.019133136263977946\n",
      "\n",
      "30;1;Winter;T16;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0009236335754394531;0.6360140932067065;0.35731320934424626;0.9735751602520966;0.9735751602520966;8.392333984375e-05;3.0078060651763168;2.2076401215356474;-0.05518019567588506;-0.07236944950028024\n",
      "\n",
      "30;1;Winter;T16;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0010173320770263672;0.6630245379058449;0.37311769539514744;0.9768002438088949;0.9768002438088949;8.153915405273438e-05;3.5937193483000898;2.810062865903349;-0.5308514422921142;-0.5308515814183026\n",
      "\n",
      "30;1;Winter;T16;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0012068748474121094;0.590427252968422;0.3251205871511115;0.9826449095486715;0.9826449095486715;7.82012939453125e-05;3.192176194821894;2.465382655703418;-0.19583286756755292;-0.20786523263131107\n",
      "\n",
      "30;1;Winter;T16;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0014307498931884766;0.924169479148641;0.5991619080954514;0.9633533248309125;0.9633533248309125;7.772445678710938e-05;4.036065768424287;3.290413731988391;-0.9302589698940222;-0.930906482343991\n",
      "\n",
      "30;1;Winter;T16;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0017364025115966797;1.0384591691592908;0.6885002068332755;0.9493173741056343;0.9493173741056343;7.891654968261719e-05;3.8214724816277243;3.133901785137956;-0.7165869821670945;-0.7310365769313927\n",
      "\n",
      "30;1;Winter;T16;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.001851797103881836;0.7653993991132555;0.5233164678185458;0.9719102706474351;0.9719102706474351;8.034706115722656e-05;4.0951633747119685;3.2649510610409562;-0.8343640907596304;-0.9878665979161094\n",
      "\n",
      "30;1;Winter;T16;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.002089262008666992;0.974020513421903;0.7185578951167322;0.952909615223691;0.952909615223691;8.034706115722656e-05;4.179840742622025;3.284856244090219;-0.6736303383310756;-1.070924381957763\n",
      "\n",
      "30;1;Winter;T16;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0024614334106445312;1.2022836401105346;0.8164526065210364;0.9353992459500482;0.9353992459500482;8.368492126464844e-05;3.317122296060714;2.637802796150499;-0.04855536903889868;-0.30427070072314666\n",
      "\n",
      "30;1;Winter;T16;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.002577543258666992;1.420906040722724;1.0291270642589334;0.9009340393716799;0.9009340393716799;8.106231689453125e-05;2.927216085691098;2.253932786817311;-0.014846994892335319;-0.015674005482288367\n",
      "\n",
      "30;1;Winter;T16;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0029218196868896484;1.238911981797336;0.8719482431235482;0.9226134965679986;0.9226134965679986;8.392333984375e-05;4.154937868718439;3.3971165640076673;-1.0109517238262602;-1.0463213681939214\n",
      "\n",
      "30;1;Winter;T16;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0031015872955322266;1.3547701151068807;0.9052723768097497;0.9040784893725937;0.9040784893725936;8.153915405273438e-05;2.9027072089448103;2.218850158661115;0.0019284462782857004;0.0012627812902520352\n",
      "\n",
      "30;1;Winter;T16;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0036988258361816406;1.5175702960901363;1.0409365653712774;0.8816203260873372;0.8816203260873373;0.00010848045349121094;3.667066380928895;3.0148305471688244;-0.2773676884130416;-0.5939779694497513\n",
      "\n",
      "30;1;Winter;T31;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07628321647644043;0.10137199489948474;0.056161057184197366;0.9956696318757876;0.9955067879832351;0.011627435684204102;3.026678948684247;2.4309940758529756;0.17071965124988353;-0.33479256258291956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Winter;T31;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07108616828918457;0.200572617541375;0.08805480842548648;0.9967251157917522;0.9966597627365966;0.01121377944946289;3.1255859020906454;2.4191178804096256;-0.07258982163621353;-0.4234556629825186\n",
      "\n",
      "30;1;Winter;T31;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07466864585876465;0.30048264978282463;0.10823548378811668;0.9916665388398235;0.9916172187282066;0.011062145233154297;3.0474301857164794;2.5588243192699793;-0.2666478176074192;-0.35315826942443573\n",
      "\n",
      "30;1;Winter;T31;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07135677337646484;0.1512789534435908;0.07616414110687751;0.9980227443055525;0.9980215834183176;0.011225700378417969;2.464822450563366;1.850692589584267;0.43476113699628194;0.11477769580401764\n",
      "\n",
      "30;1;Winter;T31;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0730745792388916;0.1271410071409757;0.06644860374700802;0.9985902927778414;0.9985899522814027;0.010734796524047852;2.5006165652471;1.9621776250760148;0.4466152928315492;0.08888064293241438\n",
      "\n",
      "30;1;Winter;T31;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07472705841064453;0.09557336556782352;0.059009069577291656;0.9993928311783029;0.9993920225192375;0.010618448257446289;2.7697097258721013;2.0983304396003115;0.4244668283126617;-0.11776260971964447\n",
      "\n",
      "30;1;Winter;T31;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07512235641479492;0.12747063649457524;0.07347415661583856;0.9988340166721861;0.9988339383816721;0.010903358459472656;2.7542443923197086;2.129719863440017;0.4047395853062773;-0.10531487226397873\n",
      "\n",
      "30;1;Winter;T31;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07469892501831055;0.10715823624284979;0.06901036011721745;0.9991554206928055;0.9991553992332296;0.01074361801147461;3.0618685043652447;2.334971931790973;0.3462201367904171;-0.36601081187367757\n",
      "\n",
      "30;1;Winter;T31;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07758808135986328;0.12411882544679739;0.07469158988465938;0.9987613635484014;0.9987611853107947;0.011065959930419922;2.532771615427238;1.826292151928451;0.37960256288434924;0.06529809735372616\n",
      "\n",
      "30;1;Winter;T31;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0756077766418457;0.13191794592945352;0.08078097654095394;0.9987180246348538;0.9987179982420359;0.010982990264892578;2.6885186891629242;2.0109221669827093;0.413095162696656;-0.05319111217686334\n",
      "\n",
      "30;1;Winter;T31;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07880830764770508;0.18063663088785373;0.09969966647905196;0.9974033896431024;0.9974022588819887;0.01104879379272461;2.3669636912032375;1.806959273713997;0.3272012846890149;0.18367281807535119\n",
      "\n",
      "30;1;Winter;T31;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0793600082397461;0.15463461572311338;0.09433552340351278;0.9981042208645674;0.9981042182122061;0.010697126388549805;2.543225928307112;1.9104044971851573;0.34423673334807203;0.05756598844113514\n",
      "\n",
      "30;1;Winter;T31;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.08136343955993652;0.18880415854246116;0.1056690085074196;0.9969551789448506;0.9969541886228032;0.010765552520751953;2.6966609097444016;2.149088975941591;0.036309658427920666;-0.05957998320887148\n",
      "\n",
      "30;1;Winter;T31;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07938909530639648;0.19433509712790176;0.1114305647394102;0.9967848557216794;0.9967830309159434;0.010754823684692383;2.4050619429200384;1.9029361971328707;0.258234974058007;0.15718239583997728\n",
      "\n",
      "30;1;Winter;T46;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0005114078521728516;0.16065784854784512;0.09774513725681051;0.9867773126731327;0.9859772171306264;0.00022220611572265625;2.82038593936515;2.458380815335959;-0.0029351595306559286;-0.03820666020814212\n",
      "\n",
      "30;1;Winter;T46;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.001041412353515625;0.21813218424974667;0.13424969090854272;0.9968057712931282;0.9968033500192802;0.0007863044738769531;2.2978660143819623;1.8266225207819;0.4146814802583244;0.3108462357366921\n",
      "\n",
      "30;1;Winter;T46;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0021524429321289062;0.47999044894418896;0.25176493553541635;0.9787337755370303;0.9781833383855494;0.001825094223022461;1.7887355963911264;1.4635420181956533;0.5836587522001686;0.5824014056524782\n",
      "\n",
      "30;1;Winter;T46;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0038042068481445312;0.6013218185534571;0.32432803721485753;0.966641657492644;0.9655841304529146;0.002717733383178711;1.784710840801492;1.4878024395363825;0.5902501671699645;0.5842785320530313\n",
      "\n",
      "30;1;Winter;T46;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.005465507507324219;0.6044021708049117;0.32035771964434523;0.9652327871753207;0.9645915329470137;0.003260374069213867;1.8325657884515578;1.5821277158964389;0.5715732019314224;0.561685459511631\n",
      "\n",
      "30;1;Winter;T46;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.008970975875854492;0.6393729846662374;0.33117116301593735;0.9730832465449933;0.9727745986964118;0.004262685775756836;1.654725952181383;1.423952072227473;0.6865993143244394;0.6426293472724234\n",
      "\n",
      "30;1;Winter;T46;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.012123584747314453;0.6228810558380651;0.34106808478088046;0.9731057325225608;0.9729384115317449;0.004976034164428711;1.5531397294707894;1.3323497830831674;0.7393103307793251;0.6851615344452497\n",
      "\n",
      "30;1;Winter;T46;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.01584148406982422;0.6463015720217528;0.3702450243287331;0.970602818114964;0.9705827597909444;0.0059223175048828125;1.499834784579763;1.2842785552187708;0.738594103941207;0.7064016759799896\n",
      "\n",
      "30;1;Winter;T46;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.021925687789916992;0.651620682613073;0.3729213810741655;0.9677203866846029;0.9676689114524967;0.006779193878173828;1.4185020992926842;1.2055431731715966;0.7481452447951423;0.7373806666689804\n",
      "\n",
      "30;1;Winter;T46;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.027311086654663086;0.6670054404901977;0.38339786865221115;0.9689336275148732;0.9688859450519609;0.007587432861328125;1.4251341789634462;1.223862554823871;0.7453153182215921;0.7349192195513099\n",
      "\n",
      "30;1;Winter;T46;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.03282904624938965;0.7183581426840802;0.4422806574350174;0.9612968666169521;0.9612527679955851;0.008579730987548828;1.3655943712032346;1.1463734890036308;0.7591226377862286;0.7566058336885633\n",
      "\n",
      "30;1;Winter;T46;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.03743457794189453;0.8493918248388402;0.5363581493705561;0.9461618487672806;0.9461278692922197;0.00962209701538086;1.4386953176798167;1.1661548675992033;0.7498809056066296;0.7298503624841081\n",
      "\n",
      "30;1;Winter;T46;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.04530501365661621;0.8588091448578535;0.5399013900022895;0.9405942993657691;0.9405932709713863;0.010560274124145508;1.557327211036463;1.2596315649177303;0.735606565228077;0.6834615489072213\n",
      "\n",
      "30;1;Winter;T46;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.05131220817565918;0.9371767159720806;0.5968692493275551;0.9297143008008427;0.9295731845023079;0.011632204055786133;1.6326007449016728;1.319003311449384;0.7267238366443964;0.652122204379447\n",
      "\n",
      "30;1;Winter;T76;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0003840923309326172;0.15673699513954045;0.10146552401731374;0.9866902810974166;0.9859511487754766;0.00022745132446289062;2.8387805445633156;2.5203238080268315;0.002180413997056041;-0.025230439269702254\n",
      "\n",
      "30;1;Winter;T76;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.001100301742553711;0.21260565562600592;0.13032962607024265;0.997051456471271;0.9970449872537112;0.0008821487426757812;2.3917409741189446;1.9154395591229663;0.4035796016231078;0.27224342284422387\n",
      "\n",
      "30;1;Winter;T76;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.002227306365966797;0.49532610315709186;0.2562623229333522;0.9777916986674012;0.9771812017299355;0.0017871856689453125;1.813414096198073;1.5137372551439596;0.5901083669596312;0.5816383893794836\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Winter;T76;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0037429332733154297;0.6047532862686249;0.3199480738215742;0.9657026193669923;0.9645909793151932;0.0028336048126220703;1.7462908778858144;1.4737967100907137;0.6120374473700033;0.6120363626135976\n",
      "\n",
      "30;1;Winter;T76;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.006215095520019531;0.6006943779648444;0.31116178611862383;0.9641419127042001;0.9634613706711016;0.003358125686645508;1.770006630386877;1.5429882097596292;0.6018139089510559;0.6014272178868205\n",
      "\n",
      "30;1;Winter;T76;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.008758068084716797;0.6250620505076183;0.31239604430195433;0.9721071389945384;0.9717339740000844;0.004049777984619141;1.6944423069485715;1.4797421990227395;0.7035967206091398;0.6347321700784673\n",
      "\n",
      "30;1;Winter;T76;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.011641979217529297;0.6063198831848736;0.3187740143817223;0.9722420174819806;0.9721080577810206;0.004968404769897461;1.588630160740598;1.3523632260126648;0.7569383285268987;0.6789272419099486\n",
      "\n",
      "30;1;Winter;T76;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.01686239242553711;0.6427765264269332;0.35899216888831753;0.9690589225445664;0.9689199571388507;0.00584721565246582;1.4940021489840263;1.276274034059125;0.7703201963936819;0.716037954434378\n",
      "\n",
      "30;1;Winter;T76;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.021450042724609375;0.6487800008642928;0.3589199291926467;0.9663780255922546;0.9662243801823058;0.006539106369018555;1.4086189905551392;1.2086166406360443;0.7896045019815239;0.7475676967510263\n",
      "\n",
      "30;1;Winter;T76;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.027341842651367188;0.6598859380588904;0.3655488529094161;0.9672608844964202;0.9670746524010244;0.007637977600097656;1.3500278928713283;1.1703798215494072;0.7945084965362189;0.7681306552459697\n",
      "\n",
      "30;1;Winter;T76;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.03310370445251465;0.7166174676987264;0.4284380253018716;0.9591388730870872;0.9589634052869422;0.008552789688110352;1.2204030516794175;1.0409212958776781;0.8107034839695484;0.8105195438472718\n",
      "\n",
      "30;1;Winter;T76;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.03857898712158203;0.827299793715766;0.5101081702324279;0.9459826133911486;0.9458231973195025;0.009616851806640625;1.2482104927173399;1.0255873475869202;0.8050133799034137;0.8017863724403508\n",
      "\n",
      "30;1;Winter;T76;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.04280352592468262;0.8343316507322351;0.5164284343656202;0.9407388816603022;0.9407036227957909;0.010230302810668945;1.3134810386736293;1.0668000196809933;0.7950071094040493;0.7805146856548904\n",
      "\n",
      "30;1;Winter;T76;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.05154013633728027;0.9102913880788801;0.572489655182844;0.9301423639852999;0.9298730051896633;0.011449575424194336;1.329548853149634;1.0678995994631337;0.7977163841884851;0.7751119124767597\n",
      "\n",
      "30;1;Winter;T97;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0004584789276123047;0.14065044377385297;0.12151250868055391;0.9865891108624324;0.9865891108624324;9.512901306152344e-05;2.9005986204308596;2.261261731450401;0.004138728392917623;-0.23660355607724637\n",
      "\n",
      "30;1;Winter;T97;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0004649162292480469;0.38407373343536233;0.3077910866615535;0.9884060797293697;0.9884060797293697;7.534027099609375e-05;3.650196312241607;2.922191098980806;-0.9283050879327179;-0.9583380530308241\n",
      "\n",
      "30;1;Winter;T97;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0005767345428466797;0.581336671095299;0.45966454928174605;0.9630401400834022;0.9630401400834021;0.0001010894775390625;2.0413866258082414;1.6885598247870226;0.5410267565842453;0.3875004849135175\n",
      "\n",
      "30;1;Winter;T97;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0007266998291015625;0.7690863891645359;0.6334451424038493;0.9315177928231958;0.9315177928231958;8.20159912109375e-05;2.0685203247309016;1.6125071901241028;0.5787955928500738;0.3711098336512695\n",
      "\n",
      "30;1;Winter;T97;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0008287429809570312;0.7535757832252127;0.5930990820298901;0.9303891441022045;0.9303891441022045;7.677078247070312e-05;2.3233230428694247;1.7560613286883067;0.22740759179868075;0.20663247676864827\n",
      "\n",
      "30;1;Winter;T97;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0009343624114990234;1.2863736876693936;0.9498057450750542;0.8568367490531874;0.8568367490531874;7.557868957519531e-05;1.7266662315419548;1.2858180254833855;0.5619350052689347;0.5618003664263749\n",
      "\n",
      "30;1;Winter;T97;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.001096963882446289;1.3733798108472877;1.0463531047749797;0.8296219176232711;0.8296219176232711;8.082389831542969e-05;1.7396038056027043;1.285722465960458;0.555829771019761;0.5552090758044399\n",
      "\n",
      "30;1;Winter;T97;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0012736320495605469;1.418932343868874;1.073836316042805;0.8212788738956687;0.8212788738956687;8.082389831542969e-05;1.7566919598272415;1.3054415910421955;0.5464291748346125;0.5464277813923172\n",
      "\n",
      "30;1;Winter;T97;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0014014244079589844;1.4598902027325358;1.1257031784601241;0.8004362803845504;0.8004362803845504;8.082389831542969e-05;1.7699259273453687;1.3052126327987619;0.5397332518960098;0.5395681040362909\n",
      "\n",
      "30;1;Winter;T97;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0014181137084960938;1.4744701675546543;1.1666932045966543;0.8088882424046013;0.8088882424046013;7.748603820800781e-05;1.5664876925737286;1.2081709708851576;0.6419007147122042;0.6393306865740115\n",
      "\n",
      "30;1;Winter;T97;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0015285015106201172;1.533377395611941;1.197635656962501;0.782311024661058;0.7823110246610578;7.653236389160156e-05;1.5664876925737288;1.2081709708851576;0.641900714712204;0.6393306865740114\n",
      "\n",
      "30;1;Winter;T97;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0017077922821044922;1.5314108135304028;1.2034729645823246;0.7847586719891105;0.7847586719891105;8.106231689453125e-05;1.5967154950875957;1.2253450554566356;0.6285975220117127;0.6252770446220131\n",
      "\n",
      "30;1;Winter;T97;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0018274784088134766;1.5571129585884942;1.2367707287949778;0.7610126457439108;0.7610126457439108;7.62939453125e-05;1.5967154950875948;1.225345055456635;0.628597522011713;0.6252770446220134\n",
      "\n",
      "30;1;Winter;T97;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0018889904022216797;1.5447899790368806;1.2304646796295686;0.7666356330363608;0.7666356330363608;7.772445678710938e-05;1.5967154950875952;1.225345055456635;0.628597522011713;0.6252770446220133\n",
      "\n",
      "\n",
      "30;1;Spring;T0;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0006527900695800781;0.3455838759788251;0.28800616028479203;0.9603340801981908;0.9603230704996318;0.0003330707550048828;4.468387136368751;3.425318143659618;0.7722255064137363;0.6730844352954239\n",
      "\n",
      "30;1;Spring;T0;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0012676715850830078;0.6148559069422971;0.48235766048297185;0.9965177257851222;0.9965176273476523;0.0007236003875732422;12.753259813274505;11.747023513236345;0.5963498673601059;-1.6630322574307805\n",
      "\n",
      "30;1;Spring;T0;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0020399093627929688;1.2083609403569595;0.8662817791896029;0.9856322654694493;0.9853992613696291;0.0011413097381591797;2.472275213633505;2.233507508348429;0.9496982563936207;0.8999245291968281\n",
      "\n",
      "30;1;Spring;T0;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0036127567291259766;2.354790260431118;1.4766577924294169;0.939379586876345;0.938688877077321;0.001783132553100586;2.5885085694610606;2.2675939907410374;0.9649350889144521;0.8902932810610317\n",
      "\n",
      "30;1;Spring;T0;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.005557060241699219;2.0468235085188398;1.3002749994577547;0.9562065126957061;0.9561949268558548;0.002028942108154297;1.8547814446488278;1.649252451805653;0.9613682483054177;0.9436726269469359\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Spring;T0;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.007477283477783203;1.9753737594461742;1.2054806089153711;0.960860224270839;0.959984222362233;0.0024666786193847656;2.3893625344474017;2.1952681996905596;0.9718902831649675;0.9065244322577386\n",
      "\n",
      "30;1;Spring;T0;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.009428262710571289;1.9430001290538716;1.2630957739366135;0.962452233790763;0.9622978375907738;0.0029211044311523438;1.8591930704811157;1.625367771436807;0.9593098883087658;0.9434043572597889\n",
      "\n",
      "30;1;Spring;T0;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.016074180603027344;1.8870996291424749;1.2600010225418001;0.9630918307507778;0.9630252567494377;0.003369569778442383;1.5970282365883068;1.1933307998837341;0.9586467101918519;0.9582401238416046\n",
      "\n",
      "30;1;Spring;T0;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.018800735473632812;1.859240694108745;1.3181867736915684;0.9628994747931067;0.9628710838572028;0.0039327144622802734;1.3645787961200755;1.0166229637565463;0.9699480238382843;0.9695118375672906\n",
      "\n",
      "30;1;Spring;T0;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.023334741592407227;1.8929510935880505;1.3172087506067227;0.9622675296702569;0.962248611411153;0.0040340423583984375;1.4376804305718998;1.136303741191117;0.9692978514620287;0.9661577885328555\n",
      "\n",
      "30;1;Spring;T0;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.02722311019897461;1.8750962008651213;1.3431396203109132;0.9611275718470357;0.9610465327614334;0.004777193069458008;1.3277067406557619;1.043602858645033;0.974153075358266;0.971137208463067\n",
      "\n",
      "30;1;Spring;T0;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.029092788696289062;1.9809373769228216;1.4391424111687747;0.9583238583461549;0.9578318120174528;0.004961967468261719;1.3477981727757524;1.0301769236815648;0.970496641140924;0.970257070690859\n",
      "\n",
      "30;1;Spring;T0;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.035788536071777344;1.8977505692770937;1.3715822508249573;0.9603361765851787;0.9599643131035076;0.005383729934692383;1.2963544318352942;1.0059514828353278;0.9725279121360952;0.9724842391694429\n",
      "\n",
      "30;1;Spring;T0;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.04044485092163086;1.8556396751713193;1.3608697546473378;0.9600026884697961;0.9598423722762023;0.0059587955474853516;1.1888825764315292;0.9224197773957479;0.977397460456439;0.9768574112943801\n",
      "\n",
      "30;1;Spring;T16;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0019276142120361328;0.1515312595663755;0.11841581535945649;0.9786836735141337;0.9786836735141337;0.00010323524475097656;25.195604575376862;16.272912062131535;-7.789456336208138;-12.262778357637881\n",
      "\n",
      "30;1;Spring;T16;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007741451263427734;0.32149387179744526;0.26469672085363805;0.9987597016560802;0.9987597016560802;6.556510925292969e-05;19.032997294337044;16.87193111431188;-0.6218313952047005;-6.56831703021899\n",
      "\n",
      "30;1;Spring;T16;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.000732421875;1.173222690031958;0.9099445087309606;0.9824828266229937;0.9824828266229937;6.699562072753906e-05;5.6223023558284835;5.28336894185167;0.9227760994090797;0.3395904589128542\n",
      "\n",
      "30;1;Spring;T16;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007390975952148438;2.0838935456720504;1.6665217923837448;0.93887022264888;0.93887022264888;6.461143493652344e-05;1.735390970726289;1.3901528439566713;0.9501699774637251;0.9370813187772493\n",
      "\n",
      "30;1;Spring;T16;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007731914520263672;1.940316464787071;1.529213668639594;0.9470182245333459;0.9470182245333459;7.009506225585938e-05;1.6827179203819806;1.411192629430152;0.9505760997915217;0.940842803547137\n",
      "\n",
      "30;1;Spring;T16;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007920265197753906;1.88953930078365;1.4534422984724797;0.9482636026095423;0.9482636026095423;6.794929504394531e-05;2.051728777146279;1.5258513383825445;0.9526772319132686;0.9120522275832141\n",
      "\n",
      "30;1;Spring;T16;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008006095886230469;2.026782680527521;1.5094990171059228;0.936465877878089;0.936465877878089;7.009506225585938e-05;2.366023452849303;1.9384481642544131;0.9388237232573374;0.8830438577979464\n",
      "\n",
      "30;1;Spring;T16;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007946491241455078;2.076465350903428;1.6232405440824549;0.9273406761627949;0.9273406761627949;6.985664367675781e-05;2.125040918699159;1.6805576721131723;0.9457058143301218;0.905654858906929\n",
      "\n",
      "30;1;Spring;T16;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008332729339599609;2.0641704364523266;1.632823668956189;0.9227904177858369;0.9227904177858369;6.699562072753906e-05;2.094162558189409;1.548072146017193;0.9480189591451894;0.9083767431279126\n",
      "\n",
      "30;1;Spring;T16;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008313655853271484;2.1040436158689295;1.6676431318375267;0.9252555759847372;0.9252555759847372;7.104873657226562e-05;2.4677330141415497;2.037886553764475;0.9593032157982382;0.8727724140646774\n",
      "\n",
      "30;1;Spring;T16;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008144378662109375;2.073864003547116;1.619490560781923;0.9232927969064654;0.9232927969064654;6.556510925292969e-05;2.2135381976699247;1.7481580196874407;0.9597376464520146;0.8976332333914183\n",
      "\n",
      "30;1;Spring;T16;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.000982522964477539;2.008899973723026;1.5592224317495427;0.9274408411398953;0.9274408411398953;0.00012111663818359375;2.007223560101095;1.532349738192767;0.9604764793465719;0.9158262964377729\n",
      "\n",
      "30;1;Spring;T16;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0010073184967041016;1.977309850557264;1.533136022996228;0.9253625538449157;0.9253625538449157;0.00011014938354492188;1.820254086113812;1.361942387418481;0.9580711697825359;0.930777230062504\n",
      "\n",
      "30;1;Spring;T16;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009775161743164062;1.9306418822748446;1.4956388415689705;0.9243658728051314;0.9243658728051314;0.0001087188720703125;1.8523451108200666;1.3774479025766417;0.9538449388943058;0.9283149238316979\n",
      "\n",
      "30;1;Spring;T31;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0956583023071289;0.04890267919653918;0.03264194667051231;0.9877405865883466;0.9877405043811882;0.013565540313720703;7.21181659285832;5.1410393838767225;0.037906355341114084;-0.5131068783912256\n",
      "\n",
      "30;1;Spring;T31;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09499335289001465;0.20720199713875412;0.12917008845676634;0.9988914097570409;0.9988890110733807;0.012697219848632812;1.994985899070878;1.5550201418156413;0.884213472653668;0.8842129686202771\n",
      "\n",
      "30;1;Spring;T31;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0961759090423584;0.2814044893220317;0.1846481346889662;0.997608085634062;0.9976063859552202;0.012814998626708984;3.0383207937837984;2.1388523130247887;0.7319461169014889;0.7314361770289286\n",
      "\n",
      "30;1;Spring;T31;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09716677665710449;0.2756015271887809;0.1734232987424436;0.9973206160368142;0.9973171522680759;0.01279139518737793;1.8991153482018095;1.44429872169097;0.8968456913080138;0.8950740413154661\n",
      "\n",
      "30;1;Spring;T31;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.21437859535217285;0.3459368473642417;0.24418827078050495;0.9956275879714068;0.9956249714724172;0.013305187225341797;1.6765068961204825;1.3701246824785027;0.9239185923663249;0.9182305743972465\n",
      "\n",
      "30;1;Spring;T31;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09983038902282715;0.4581085593594146;0.3264753178670887;0.9919396044445258;0.9919395991139964;0.012787818908691406;1.7424648115293788;1.419921642434559;0.9192043406190243;0.9116699875794176\n",
      "\n",
      "30;1;Spring;T31;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09983944892883301;0.5387602555827482;0.35954523272516786;0.9890034964777287;0.9890020903132685;0.012937545776367188;1.9201929748306519;1.5253959913965143;0.9171450321791783;0.8927320424247983\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Spring;T31;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09879231452941895;0.543972558386681;0.3567309191159006;0.9882107523616304;0.9882107519566229;0.013632535934448242;1.648028371458282;1.3354628625698237;0.9360022434638356;0.9209849850533255\n",
      "\n",
      "30;1;Spring;T31;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0996255874633789;0.5201977386781815;0.3440739833987564;0.9887729322747277;0.9887718923871487;0.012775421142578125;1.76060159864636;1.3651669720725763;0.9168459984937428;0.9098216172660414\n",
      "\n",
      "30;1;Spring;T31;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09910440444946289;0.566186474476553;0.38807348325038976;0.9870466485146678;0.9870466238171423;0.012857675552368164;1.7869073806663331;1.452720178880307;0.9137169816871002;0.907106710129671\n",
      "\n",
      "30;1;Spring;T31;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0992727279663086;0.6226678794818192;0.4540447659280842;0.9834876143915098;0.9834843324378323;0.013199090957641602;1.7928555077942379;1.4440106672314135;0.9233939303513145;0.9064872479989026\n",
      "\n",
      "30;1;Spring;T31;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09921550750732422;0.6681773565959168;0.48528634816396193;0.980427902974352;0.9804279029731058;0.013697624206542969;1.7363052249767366;1.3798859989923862;0.9229177859668818;0.9122933742926442\n",
      "\n",
      "30;1;Spring;T31;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10014963150024414;0.7212032456470886;0.5152987558758855;0.9760237324051184;0.9760223761136729;0.01322484016418457;1.7830417914362129;1.408197537117877;0.9245349398177485;0.9075081846557029\n",
      "\n",
      "30;1;Spring;T31;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09911108016967773;0.7310294677522783;0.5191486205773663;0.9740790844879549;0.9740741935367384;0.013154029846191406;1.7303513145366385;1.3633363789351522;0.925734071719153;0.912893847137527\n",
      "\n",
      "30;1;Spring;T46;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0012607574462890625;0.0685072305994579;0.05304733150911858;0.9140896483991765;0.9140896483991765;7.05718994140625e-05;7.199774719729434;5.780702050951772;-1.4982006642224217;-3.3583689761119837\n",
      "\n",
      "30;1;Spring;T46;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007803440093994141;0.43676592728829666;0.3316721171363069;0.9852547983635135;0.9852547983635135;6.604194641113281e-05;13.287522503464878;11.620812962599928;-2.540500393774061;-13.84479502749546\n",
      "\n",
      "30;1;Spring;T46;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007166862487792969;1.1044241630476865;0.8730957564264156;0.900029886921494;0.900029886921494;7.390975952148438e-05;2.2170770050853728;1.5606428758523647;0.6886441356665924;0.5867168162292395\n",
      "\n",
      "30;1;Spring;T46;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007731914520263672;1.1847715949174922;0.9006401360324584;0.8833675299014476;0.8833675299014476;6.67572021484375e-05;3.5754745795656158;3.0081757052691334;0.5768364037421025;-0.07486412671903486\n",
      "\n",
      "30;1;Spring;T46;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007765293121337891;1.1696425362958138;0.8973082837621873;0.895773419646736;0.895773419646736;7.224082946777344e-05;2.0587524554930585;1.672771727070821;0.7333752286179559;0.6436354956945336\n",
      "\n",
      "30;1;Spring;T46;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007619857788085938;1.2664178048434935;1.0022790608822938;0.8826951707261791;0.8826951707261791;6.67572021484375e-05;1.6451196151716294;1.3069328526728066;0.7726942092111438;0.772447746661337\n",
      "\n",
      "30;1;Spring;T46;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008044242858886719;1.366261328472281;1.0601314033800535;0.8747227484232477;0.8747227484232478;6.771087646484375e-05;1.7449446817860406;1.386936667897812;0.8368433555033815;0.7439943758858099\n",
      "\n",
      "30;1;Spring;T46;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008137226104736328;1.2883301290543518;0.969762474335687;0.8856880625598114;0.8856880625598114;7.796287536621094e-05;1.6232699228067071;1.2615871282483477;0.8481419371025054;0.7784520872295086\n",
      "\n",
      "30;1;Spring;T46;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008211135864257812;1.2508990650153067;0.9434555219237469;0.8910067174634623;0.8910067174634623;6.628036499023438e-05;1.5095241871900116;1.1623564544646225;0.863509814633926;0.8084128700571479\n",
      "\n",
      "30;1;Spring;T46;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008168220520019531;1.291408271994652;1.0040067441696867;0.8911864352166317;0.8911864352166317;6.818771362304688e-05;1.773456212899459;1.4995813390814061;0.8943301952136786;0.7355600176709484\n",
      "\n",
      "30;1;Spring;T46;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008232593536376953;1.3463847274619611;1.0451125012520095;0.8762589269545099;0.8762589269545099;6.699562072753906e-05;1.482811129151405;1.1780715617669264;0.883846598589135;0.8151336557204598\n",
      "\n",
      "30;1;Spring;T46;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009107589721679688;1.3309903172127189;1.034841415259558;0.8769200361849889;0.8769200361849889;0.0001087188720703125;1.383341063239321;1.0968374316835288;0.8731508496092513;0.8391041965419623\n",
      "\n",
      "30;1;Spring;T46;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009310245513916016;1.3228640077393;1.0225717506252199;0.8721068462048487;0.8721068462048487;0.00010657310485839844;1.3070655924265568;1.0212604287444493;0.8650832561595811;0.8563581652071766\n",
      "\n",
      "30;1;Spring;T46;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009598731994628906;1.2841433097859047;0.97966021500527;0.8734810095622327;0.8734810095622327;0.00021886825561523438;1.320785895980563;1.0243520956315135;0.8584667603276838;0.8533267130610617\n",
      "\n",
      "30;1;Spring;T76;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0005409717559814453;0.03579280373423816;0.028279297848611374;0.9824301285966777;0.9824301285966777;8.225440979003906e-05;3.3709635124260706;2.5947928652780177;-0.061158747559180826;-0.7049314927572814\n",
      "\n",
      "30;1;Spring;T76;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0004601478576660156;0.5982794483348806;0.4223368096623682;0.9564838273072528;0.9564838273072528;7.510185241699219e-05;1.4660091543461884;1.2565320990495599;0.7432458851718327;0.6775427143033821\n",
      "\n",
      "30;1;Spring;T76;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0005831718444824219;1.0463118088613648;0.7305443421038962;0.8654333309703748;0.8654333309703748;7.891654968261719e-05;2.756012936926231;1.946761029910494;0.11938102386549843;-0.1396233347943019\n",
      "\n",
      "30;1;Spring;T76;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0007188320159912109;1.0585769797665232;0.7676320972710743;0.8579004495075986;0.8579004495075986;7.700920104980469e-05;1.582164963646614;1.2206302527800748;0.7419006102689996;0.624420088247009\n",
      "\n",
      "30;1;Spring;T76;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0009210109710693359;1.1877213787653114;0.9061132760953321;0.8318437165249133;0.8318437165249133;9.72747802734375e-05;1.684880877445793;1.295956451020915;0.6723547254312794;0.5740709789642158\n",
      "\n",
      "30;1;Spring;T76;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0010423660278320312;1.2081827941067338;0.8884711959308664;0.8336217163732804;0.8336217163732805;7.843971252441406e-05;1.240946483918924;1.027679182759722;0.7857728283339023;0.7689505520220395\n",
      "\n",
      "30;1;Spring;T76;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0011227130889892578;1.2711006455427545;0.9463265601803346;0.8330754277327131;0.8330754277327131;7.82012939453125e-05;1.2647230816121984;1.0536718189415897;0.7811246642721461;0.7600118936968101\n",
      "\n",
      "30;1;Spring;T76;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0012717247009277344;1.2212407231683142;0.9147867267676091;0.840586443426623;0.8405864434266231;7.724761962890625e-05;1.293403594857485;1.0643538233923928;0.7882703037522306;0.7490039095610794\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Spring;T76;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0013985633850097656;1.2012895840468603;0.8998215994694172;0.843256495118415;0.8432564951184152;7.700920104980469e-05;1.2765043238453488;1.0632554113658517;0.7737202305114337;0.7555199588816026\n",
      "\n",
      "30;1;Spring;T76;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.001596689224243164;1.259070271445178;0.9713436922752547;0.8400484431244247;0.8400484431244247;7.843971252441406e-05;1.3259479635792826;1.0819355736081482;0.7730524125899246;0.7362139705720168\n",
      "\n",
      "30;1;Spring;T76;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.00170135498046875;1.2811048328020098;0.9920645178152068;0.8261077269164508;0.8261077269164508;7.867813110351562e-05;1.3247196289783223;1.0809495145754553;0.773156555034862;0.73670247756334\n",
      "\n",
      "30;1;Spring;T76;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0018110275268554688;1.317130699683131;1.0114549153760843;0.8119792039957482;0.8119792039957482;7.796287536621094e-05;1.276625411287879;1.0548985481644675;0.7775164552792531;0.7554735746015848\n",
      "\n",
      "30;1;Spring;T76;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0019328594207763672;1.307892058638039;1.0026361994090471;0.8036566147150643;0.8036566147150643;7.748603820800781e-05;1.301895115102045;1.0678375175157557;0.775440263806356;0.7456973859387246\n",
      "\n",
      "30;1;Spring;T76;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];DecisionTreeRegressor;0.0021932125091552734;1.2844241443057989;0.9756910150605984;0.8003605284723591;0.8003605284723591;8.106231689453125e-05;1.3056335285081595;1.0715004299656539;0.7748451490123863;0.7442348207148717\n",
      "\n",
      "30;1;Spring;T97;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0013403892517089844;0.04114788848224867;0.03177373648966636;0.9824116989898809;0.9824116989898809;6.937980651855469e-05;8.26720567903029;6.036016930350402;-7.229022241203916;-13.25685424903809\n",
      "\n",
      "30;1;Spring;T97;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007364749908447266;0.39326696237884545;0.29620522552758666;0.9725231417005717;0.9725231417005717;6.771087646484375e-05;11.63375837470087;10.139258330734423;-5.992134653085371;-27.23230282990056\n",
      "\n",
      "30;1;Spring;T97;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007207393646240234;1.0251916600368036;0.8155098449296165;0.8228153159973892;0.8228153159973892;6.628036499023438e-05;2.240238108672902;1.5899359708125655;0.09936050413354369;-0.04687441528464986\n",
      "\n",
      "30;1;Spring;T97;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007462501525878906;1.0730355708258235;0.8093171260596982;0.7961219102211333;0.7961219102211333;7.033348083496094e-05;4.02728608851387;3.446199565843707;-0.07609243316639125;-2.383227392967868\n",
      "\n",
      "30;1;Spring;T97;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007190704345703125;1.1085550419096855;0.8696916584064524;0.7928178905441163;0.7928178905441163;7.152557373046875e-05;2.0810512153451572;1.7316243657228907;0.4189506308851406;0.09661731632164527\n",
      "\n",
      "30;1;Spring;T97;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007767677307128906;1.1849887668258798;0.964608550525967;0.7762979141939488;0.7762979141939488;6.651878356933594e-05;1.585470906123699;1.3128036262857754;0.48604539285178705;0.47564818131439124\n",
      "\n",
      "30;1;Spring;T97;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007581710815429688;1.2652699698517285;0.9963778274257531;0.7715400197449651;0.7715400197449651;6.723403930664062e-05;1.526454388424915;1.208759876119212;0.6178836658751126;0.5139579004058268\n",
      "\n",
      "30;1;Spring;T97;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007700920104980469;1.196655121825407;0.9219476308650942;0.7873523516074945;0.7873523516074945;7.43865966796875e-05;1.4544148700555468;1.1392149711116324;0.6541485604857361;0.558751911527996\n",
      "\n",
      "30;1;Spring;T97;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007996559143066406;1.161177731041746;0.8988498456494279;0.7957893503568023;0.7957893503568023;6.985664367675781e-05;1.3893360339232668;1.093998429660525;0.6873327192952896;0.5973563779494201\n",
      "\n",
      "30;1;Spring;T97;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008006095886230469;1.202692426876298;0.9476852240637119;0.7993042523042065;0.7993042523042065;6.651878356933594e-05;1.6192987042089049;1.3565780208492269;0.7521995163529032;0.4530342135869695\n",
      "\n",
      "30;1;Spring;T97;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008232593536376953;1.261666424698586;0.9969456198540347;0.7677982754931277;0.7677982754931277;6.842613220214844e-05;1.35462151555619;1.1045374601800266;0.7262295307281981;0.6172262356628728\n",
      "\n",
      "30;1;Spring;T97;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0011181831359863281;1.2452622332836374;0.9881538643781257;0.7675044947998594;0.7675044947998594;0.0003266334533691406;1.2836493523311288;1.0587720592332617;0.7007577537805963;0.6562845637657357\n",
      "\n",
      "30;1;Spring;T97;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.000965118408203125;1.2494033960259066;0.985609717097074;0.7509553444141763;0.7509553444141763;0.00011157989501953125;1.23615698558686;1.0019940932969942;0.6835994388773203;0.6812475881793105\n",
      "\n",
      "30;1;Spring;T97;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0015411376953125;1.2115626909551236;0.9401409872264578;0.7523322630137694;0.7523322630137694;0.00011396408081054688;1.2587445480239507;1.0210086068752497;0.6695211320204244;0.6694924158365234\n",
      "\n",
      "\n",
      "30;1;Summer;T0;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0690774917602539;0.34773086842959305;0.17390507584621256;0.9878869818392548;0.987677674126797;0.01120138168334961;1.815230996441205;1.4439560710509054;0.772389122985821;0.6276675077534788\n",
      "\n",
      "30;1;Summer;T0;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.0723111629486084;0.2699291246478464;0.15763144215030886;0.9881937523467258;0.9881850916477932;0.011160135269165039;0.7251072562515585;0.5576486319828208;0.9432484711303678;0.9405883461526166\n",
      "\n",
      "30;1;Summer;T0;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07257413864135742;0.200176880078592;0.12882815720345042;0.9922832933154042;0.9922831987826435;0.010524749755859375;0.6820267507825547;0.5280309274207977;0.9498391030766428;0.9474382328654097\n",
      "\n",
      "30;1;Summer;T0;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.072967529296875;0.22825064949858587;0.16085928696140728;0.9872683862574625;0.9872249471847987;0.010783672332763672;0.6140217821122373;0.47010151371068604;0.9573978544948554;0.9573975382466225\n",
      "\n",
      "30;1;Summer;T0;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07266664505004883;0.21935171454230223;0.16103527020085148;0.9903552471672038;0.9903547528288847;0.011195898056030273;0.6122795449619659;0.4766761355208311;0.9576916769131453;0.9576389573363021\n",
      "\n",
      "30;1;Summer;T0;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07288599014282227;0.24303775315513462;0.17966960615349875;0.9906432898633576;0.9906431881039128;0.010927677154541016;0.5607398825228415;0.44133767558367;0.9673770991597198;0.964470423386239\n",
      "\n",
      "30;1;Summer;T0;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07363629341125488;0.2594111273373154;0.19178181841070013;0.9899812199019687;0.9899808541771816;0.010890007019042969;0.571211860408677;0.44982192395948944;0.965591980875675;0.9631309818307001\n",
      "\n",
      "30;1;Summer;T0;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07442021369934082;0.2681390434541266;0.1968179499263282;0.9894791261233622;0.9894774353277567;0.011032819747924805;0.5476932441205636;0.43329718340792983;0.9661592930467385;0.9661045107137813\n",
      "\n",
      "30;1;Summer;T0;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07507443428039551;0.2942692521060405;0.21291295568402993;0.9864141057650081;0.9864119728435127;0.011195659637451172;0.5439873391030782;0.4366395840994479;0.9665916462989428;0.9665616589091994\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Summer;T0;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07404351234436035;0.29425243214045826;0.2173720146150758;0.9872259246142675;0.9872258781653911;0.011050701141357422;0.564130627078724;0.45888120832484386;0.9641499410150891;0.9640394358172641\n",
      "\n",
      "30;1;Summer;T0;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07613468170166016;0.29142642892565906;0.21523726485422393;0.9882297505566572;0.9882294494482704;0.01573920249938965;0.565928707336164;0.4590221450090529;0.9638344540957148;0.963809832869637\n",
      "\n",
      "30;1;Summer;T0;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07462024688720703;0.3331939611913694;0.24099681882767382;0.9852975315397646;0.9852974872208068;0.01651740074157715;0.703640197465763;0.5230506703358209;0.9442140629270769;0.9440540795756017\n",
      "\n",
      "30;1;Summer;T0;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07352375984191895;0.36867736121903544;0.27246041997797654;0.9826886683447715;0.9826883779155208;0.011027812957763672;0.6552242288569962;0.4931626388746753;0.9525793876798805;0.9514882394035813\n",
      "\n",
      "30;1;Summer;T0;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.07344484329223633;0.36936395628854185;0.27664373589273955;0.9836170611189432;0.9836170230501506;0.01095271110534668;0.6221328561348707;0.4834449421831281;0.9570636666109986;0.9562645685155049\n",
      "\n",
      "30;1;Summer;T16;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09097790718078613;0.3382902419166016;0.18877049982283486;0.990007545895524;0.9899236806007446;0.013528108596801758;1.535693986019553;1.2086888679837826;0.8164768023609846;0.7358934747782742\n",
      "\n",
      "30;1;Summer;T16;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09508609771728516;0.4093118713631348;0.2374144559197086;0.9813408631593173;0.9813374968702939;0.012874841690063477;0.9479211622525732;0.7497301824948678;0.9195546397358483;0.8993730748374902\n",
      "\n",
      "30;1;Summer;T16;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09679651260375977;0.26808351255936413;0.1614171299765274;0.9895103470096038;0.9894855955476816;0.013940572738647461;1.0390043696932902;0.8232212981030428;0.9368280077543871;0.8791060653672849\n",
      "\n",
      "30;1;Summer;T16;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.21753239631652832;0.27296320587961403;0.15117414810730476;0.985821066857908;0.9857917721552008;0.013228893280029297;1.0755771583685965;0.8443591938537559;0.9236664916342388;0.8704453797305121\n",
      "\n",
      "30;1;Summer;T16;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09954142570495605;0.24281433764678134;0.14296327991315375;0.9906645580267492;0.990660976141739;0.01334238052368164;1.0459584590609736;0.8205344646954488;0.9281504023060206;0.8774823558042043\n",
      "\n",
      "30;1;Summer;T16;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10142111778259277;0.2336828833489815;0.14557258599911052;0.9929219444741059;0.9929088953739683;0.013266324996948242;1.0408724748650424;0.8210293281005482;0.923003023636648;0.8786709457067425\n",
      "\n",
      "30;1;Summer;T16;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10004782676696777;0.2465704768573987;0.14665106250357726;0.9921261917005492;0.992113074022257;0.013285636901855469;1.0425892829268215;0.8225165963982544;0.918403357587835;0.8782703769784702\n",
      "\n",
      "30;1;Summer;T16;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10401225090026855;0.2287327990811485;0.13846006742743072;0.9953489954294743;0.9953408650093961;0.013303041458129883;0.9941937707234286;0.7572484175241874;0.9235830764871974;0.8893091201190918\n",
      "\n",
      "30;1;Summer;T16;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10830116271972656;0.23020610550012768;0.13669776319971186;0.9961584855997723;0.9961472683221025;0.013171911239624023;1.0694306098276234;0.857104188095758;0.9192939085457972;0.8719218678247928\n",
      "\n",
      "30;1;Summer;T16;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10469698905944824;0.29683266750212794;0.168361382435095;0.9942296047825584;0.994228987270347;0.013266324996948242;0.9793778918323206;0.7592676779927047;0.9338216356180977;0.8925836584939439\n",
      "\n",
      "30;1;Summer;T16;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10358572006225586;0.25644605475443455;0.15627590073106193;0.9955107383919212;0.995506144205117;0.013385772705078125;0.884592032607595;0.694100008050841;0.9399199093094566;0.9123693947773948\n",
      "\n",
      "30;1;Summer;T16;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10844969749450684;0.2459904303710254;0.1521870823679174;0.9957159603884533;0.9957127977858068;0.01366281509399414;0.8924098936485098;0.7041098112070169;0.9439005688577617;0.910813624000189\n",
      "\n",
      "30;1;Summer;T16;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10661005973815918;0.2574631474540928;0.15519082135558745;0.9952180166905076;0.9952138590090555;0.013886451721191406;0.9274723472048766;0.7272440461972927;0.9346701816342984;0.9036677499930027\n",
      "\n",
      "30;1;Summer;T16;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10834693908691406;0.26050674975867943;0.1563586900724683;0.9952523053365825;0.9952507464366377;0.013099908828735352;0.8691144930232666;0.6615441142973291;0.936911421128107;0.9154090799549919\n",
      "\n",
      "30;1;Summer;T31;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09571075439453125;0.2528896208516208;0.13316657080615835;0.9949497970733018;0.9948352332767116;0.013424158096313477;1.4008720252494196;1.0649777376691019;0.7944584974213786;0.7730123175096612\n",
      "\n",
      "30;1;Summer;T31;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09567403793334961;0.32642276685506433;0.18997255839042973;0.9877102890905649;0.987705996871852;0.013107776641845703;0.784971134417962;0.6434037062119787;0.9294711193954146;0.928729018611316\n",
      "\n",
      "30;1;Summer;T31;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09831070899963379;0.24279646948271882;0.13674887234960362;0.9929299739686382;0.992900325143693;0.012983083724975586;1.0421041715253048;0.8416758788688493;0.8752182172736898;0.8743890112719197\n",
      "\n",
      "30;1;Summer;T31;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.09999275207519531;0.23178022026338116;0.11991364964627953;0.9963846863833865;0.996378057024667;0.013049125671386719;1.1005811317735916;0.8712544468088682;0.8611642425092085;0.8598963365399894\n",
      "\n",
      "30;1;Summer;T31;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.1018819808959961;0.21763936424113195;0.11937904898517987;0.9970602859956315;0.9970561989774229;0.013448715209960938;1.0294539188524063;0.8133633012202727;0.8777562848531216;0.8774201211626044\n",
      "\n",
      "30;1;Summer;T31;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10073113441467285;0.19498349127385264;0.11009176402375194;0.9976569762034794;0.9976507147215933;0.012932538986206055;1.0506600704575926;0.8457898960806449;0.8739089412402345;0.8723179577747072\n",
      "\n",
      "30;1;Summer;T31;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10500836372375488;0.190087388724507;0.1002302393581396;0.9976828908664862;0.9976826261499454;0.012316465377807617;0.9919285509699628;0.7867174208094082;0.8864654695250227;0.8861937411320384\n",
      "\n",
      "30;1;Summer;T31;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10284423828125;0.1738347816762417;0.09116346908732313;0.9981855814277769;0.9981853089071142;0.013353109359741211;1.0768180784305506;0.862947272560586;0.8659660463405984;0.8658810817984111\n",
      "\n",
      "30;1;Summer;T31;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10182595252990723;0.17055723966688094;0.0900714695134927;0.9982876566610523;0.9982874039714584;0.013219594955444336;1.1298261914746301;0.8794956283819215;0.8540282512363444;0.8523516350737047\n",
      "\n",
      "30;1;Summer;T31;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10290145874023438;0.1609750134518431;0.09385610036202721;0.9983295806344344;0.9983290209305337;0.01327824592590332;1.0453631390937952;0.8171562748549556;0.874812841829845;0.8736021374681724\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Summer;T31;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10444045066833496;0.16924445745895658;0.09367773836373557;0.9980659843687554;0.9980658307709721;0.014226198196411133;1.0557938372981202;0.8285997110821476;0.8717442551792829;0.8710671416638518\n",
      "\n",
      "30;1;Summer;T31;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10291457176208496;0.17619427837416196;0.1039513068629603;0.9978239234320236;0.9978237972790144;0.01323556900024414;1.0414814129410568;0.8220184038343371;0.8753406936060472;0.8745390959755889\n",
      "\n",
      "30;1;Summer;T31;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.10790586471557617;0.18545076341574696;0.1163061306165094;0.9975359697697491;0.9975359399543063;0.013596773147583008;1.0286332944461323;0.8291545629624462;0.877906905973466;0.8776154712316303\n",
      "\n",
      "30;1;Summer;T31;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];RandomForestRegressor;0.1053166389465332;0.2032333014445391;0.13486358482369956;0.9969891263083889;0.9969891243107855;0.012826204299926758;1.0584308878276034;0.8385830590960748;0.8721250209395889;0.8704222675199271\n",
      "\n",
      "30;1;Summer;T46;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.03530526161193848;0.09707576641498594;0.07526760918737638;0.9969064248029192;0.9969058790274821;0.0005815029144287109;3.961459318423525;3.113487165203962;-2.9494654711745674;-5.287765311066104\n",
      "\n",
      "30;1;Summer;T46;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.1901836395263672;0.12944364517266588;0.08655649455397152;0.9903512978256723;0.9903397351788132;0.00030231475830078125;1.4444503106739013;1.1780052898852864;0.1972770271065425;0.16402914624741116\n",
      "\n",
      "30;1;Summer;T46;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.19725775718688965;0.09985503239069317;0.07417160703264014;0.9942584847615002;0.9942576339570378;0.00026726722717285156;1.188776955814024;0.9492082595205616;0.5967833729429104;0.43377807974900684\n",
      "\n",
      "30;1;Summer;T46;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.15926814079284668;0.095436891150509;0.0709338554320638;0.9951063894696361;0.9951016374216498;0.000274658203125;0.8564346334049904;0.6818496367772063;0.7441003906892105;0.7061170669766406\n",
      "\n",
      "30;1;Summer;T46;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.4653148651123047;0.08453508293861679;0.05811860996744875;0.9961301481239478;0.9961158909676171;0.0003647804260253906;0.9431157575554407;0.8000518104307774;0.648089609090158;0.6436178653108261\n",
      "\n",
      "30;1;Summer;T46;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.3427703380584717;0.10102889068888563;0.07261229775079044;0.9938646905444647;0.993863463126857;0.00041556358337402344;0.9617572499356878;0.7698517829767453;0.6560413283822222;0.6293902331601492\n",
      "\n",
      "30;1;Summer;T46;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.38910579681396484;0.0978898925165685;0.07090507974285805;0.9942390629726491;0.9942390515673892;0.00032520294189453125;0.8659472941233093;0.7373362258979625;0.7055499056081624;0.6995523290984857\n",
      "\n",
      "30;1;Summer;T46;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.5097513198852539;0.09582775652279618;0.07159585991138516;0.9946109633459307;0.9946093786162575;0.0009086132049560547;0.9103438598751948;0.6617191437649949;0.6721722655819693;0.6679550669942311\n",
      "\n",
      "30;1;Summer;T46;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.5061841011047363;0.0757039818616157;0.05189300451543739;0.9966110967705388;0.9966087411178589;0.0002970695495605469;1.046460467651003;0.7230490489186019;0.6239093369411195;0.5612354052260642\n",
      "\n",
      "30;1;Summer;T46;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.9188475608825684;0.09793974775907353;0.06472906506064045;0.9944642632776162;0.9944642616044357;0.0002815723419189453;0.9330050693274101;0.6437981312814378;0.6602101702808838;0.6512181077503847\n",
      "\n",
      "30;1;Summer;T46;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.9176146984100342;0.08771613170667727;0.0629944239951318;0.9954898070083461;0.9954774563685601;0.00032019615173339844;1.000189054449272;0.6846919614947343;0.6100891638909964;0.5991793225033559\n",
      "\n",
      "30;1;Summer;T46;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;0.846064567565918;0.0951355557308634;0.06739016957154176;0.9947217279109131;0.9944572398722067;0.0003209114074707031;1.1588885119224048;0.8339756045996232;0.6685932227254154;0.46189226091964763\n",
      "\n",
      "30;1;Summer;T46;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;1.0870258808135986;0.09022598505541801;0.06478169022819355;0.9951624236121789;0.9951601499776258;0.00028228759765625;1.08473329777131;0.7551872354051894;0.5310647435266214;0.5285540966616105\n",
      "\n",
      "30;1;Summer;T46;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];MLPRegressor;1.2876977920532227;0.08068105846204544;0.05747994319557986;0.9962298219199194;0.9962294068018531;0.0002884864807128906;0.9912822877580637;0.679434298953845;0.6259872710901471;0.6062862201400645\n",
      "\n",
      "30;1;Summer;T76;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0012853145599365234;0.029505531148371206;0.023718718137490668;0.9983399379728014;0.9983399379728014;6.985664367675781e-05;0.6254701268836409;0.5470369640329937;0.6794553377623109;-0.3566229648300352\n",
      "\n",
      "30;1;Summer;T76;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.000736236572265625;0.06103009480886743;0.04867036216182922;0.995054618107894;0.995054618107894;6.723403930664062e-05;0.3717490103582686;0.3215669639356646;0.6092847130139569;0.5207681626872849\n",
      "\n",
      "30;1;Summer;T76;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007090568542480469;0.06202601694188523;0.05072429217665313;0.9943449695526327;0.9943449695526327;6.532669067382812e-05;0.30549138140491133;0.25144063715619713;0.6763976254692314;0.6763735980231601\n",
      "\n",
      "30;1;Summer;T76;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007452964782714844;0.061454443666596345;0.04941805396305804;0.9939353608740227;0.9939353608740227;6.651878356933594e-05;0.30189843705863256;0.24548394949703384;0.7107890070904446;0.6839412999510637\n",
      "\n",
      "30;1;Summer;T76;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007195472717285156;0.17464656995471176;0.13135647769740963;0.950325351389348;0.950325351389348;6.651878356933594e-05;0.6894200035337144;0.6002269666715861;0.6011149004087875;-0.6482146910207487\n",
      "\n",
      "30;1;Summer;T76;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007762908935546875;0.2012586874493112;0.15978160895072618;0.9323985838425762;0.9323985838425762;6.914138793945312e-05;0.43964865311367984;0.3404579302214851;0.6786151058455367;0.3297180658085578\n",
      "\n",
      "30;1;Summer;T76;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0012688636779785156;0.22585334539790328;0.18008835605264636;0.9232857017085037;0.9232857017085037;6.628036499023438e-05;0.3553828035661228;0.27234715289448225;0.6312811088697625;0.5620355715734529\n",
      "\n",
      "30;1;Summer;T76;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007715225219726562;0.23473356315674174;0.1874280432986553;0.9184768766368507;0.9184768766368507;6.651878356933594e-05;0.3296946416136435;0.2730830769537814;0.6235718286597804;0.623062107728257\n",
      "\n",
      "30;1;Summer;T76;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007722377777099609;0.23447719453965676;0.18402833095410312;0.9147728814041861;0.9147728814041861;6.842613220214844e-05;0.32044196195835295;0.2664504423865549;0.6440881169592886;0.6439222931584746\n",
      "\n",
      "30;1;Summer;T76;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008230209350585938;0.2457252042467759;0.19985660976179168;0.905004915849316;0.905004915849316;6.628036499023438e-05;0.35249857107527954;0.28352896126395954;0.5917763618690205;0.5691156288425615\n",
      "\n",
      "30;1;Summer;T76;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008170604705810547;0.24214314074799956;0.1957476127954085;0.9076475160550501;0.9076475160550501;6.794929504394531e-05;0.36967942212187327;0.2850936179812632;0.5950417907334635;0.5260892355247038\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Summer;T76;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009119510650634766;0.2456073219063933;0.19682939482269055;0.900251431755618;0.900251431755618;0.00010776519775390625;0.36796898423893437;0.27253831819750973;0.655028890140416;0.5304644841340165\n",
      "\n",
      "30;1;Summer;T76;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009462833404541016;0.2440007860807302;0.1958334400359371;0.9110202393999625;0.9110202393999625;0.00010895729064941406;0.3409898493001792;0.2569451469305768;0.663609747213487;0.5967922025823398\n",
      "\n",
      "30;1;Summer;T76;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009529590606689453;0.2400199971573782;0.1914433994776593;0.9215129932379325;0.9215129932379325;0.00010776519775390625;0.3240137492341219;0.25143645060222686;0.6696610860348632;0.6359400549727788\n",
      "\n",
      "30;1;Summer;T97;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0006392002105712891;0.026823560698025026;0.021542830194220423;0.9988206734506819;0.9988206734506819;6.389617919921875e-05;0.6845613312522425;0.5944915237489077;0.3688303909005073;-1.3903402432619831\n",
      "\n",
      "30;1;Summer;T97;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007135868072509766;0.037944060849152604;0.026321465063080633;0.9979969190028101;0.9979969190028101;6.985664367675781e-05;0.3791504253619287;0.2531849505678041;0.5000224734051377;0.2667398366375012\n",
      "\n",
      "30;1;Summer;T97;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007162094116210938;0.05193251540939585;0.04144316850661982;0.9956379435961118;0.9956379435961118;6.508827209472656e-05;0.6588225804986811;0.5472838331512747;0.3138068382158633;-1.213971100764622\n",
      "\n",
      "30;1;Summer;T97;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007309913635253906;0.07638425586956718;0.05962837839501452;0.9891643650591877;0.9891643650591877;6.961822509765625e-05;0.6226435894971122;0.543955846864787;0.5317650307942956;-0.9774888270847335\n",
      "\n",
      "30;1;Summer;T97;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0012357234954833984;0.11226448382305865;0.08763119150673508;0.9747111602400863;0.9747111602400863;7.033348083496094e-05;0.8522295030511481;0.787298684449382;0.45699383298085006;-2.7046580344521822\n",
      "\n",
      "30;1;Summer;T97;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007729530334472656;0.16335898623430917;0.12642184106922938;0.9479788005728285;0.9479788005728285;7.05718994140625e-05;0.5060707289081166;0.4114426255393177;0.5530390725620032;-0.30634363357320216\n",
      "\n",
      "30;1;Summer;T97;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009634494781494141;0.2103921766010317;0.16179376894057293;0.9225059642276323;0.9225059642276323;7.104873657226562e-05;0.380022073242424;0.2864690349255188;0.42173492967254755;0.2633645042877586\n",
      "\n",
      "30;1;Summer;T97;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008203983306884766;0.21096753991982278;0.15901376819315197;0.9253521689600497;0.9253521689600497;7.43865966796875e-05;0.3380109559078801;0.23722458453404824;0.4841247697753951;0.41723085386226677\n",
      "\n",
      "30;1;Summer;T97;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.000789642333984375;0.20067519816520388;0.1478258739511771;0.9297122451666759;0.9297122451666759;6.461143493652344e-05;0.34036634764780893;0.23096491469221572;0.496720342464263;0.4090806315860468\n",
      "\n",
      "30;1;Summer;T97;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007863044738769531;0.20571475025552147;0.16020262815297626;0.9247661884175756;0.9247661884175756;6.604194641113281e-05;0.3896420819869849;0.2734482407568224;0.4180791426915509;0.22559757074654108\n",
      "\n",
      "30;1;Summer;T97;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008039474487304688;0.21213251699870764;0.16551425350333834;0.9213344924731642;0.9213344924731642;7.295608520507812e-05;0.44219327087303695;0.3147829984972353;0.3988030528735932;0.002623216065097078\n",
      "\n",
      "30;1;Summer;T97;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009059906005859375;0.21514267611352483;0.16804618115980782;0.9161805285783939;0.9161805285783939;0.000110626220703125;0.46185181808205916;0.34249595867656635;0.48036832461984424;-0.08802859964814669\n",
      "\n",
      "30;1;Summer;T97;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008504390716552734;0.2213504763680356;0.1742494269285932;0.9223175175376768;0.9223175175376768;0.00011014938354492188;0.4271452873883386;0.2982270296468917;0.4690180406779514;0.06935031514907775\n",
      "\n",
      "30;1;Summer;T97;14years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009243488311767578;0.22427506606962977;0.1786170788893437;0.9293198612474574;0.9293198612474574;0.00010752677917480469;0.4063782542391018;0.2810068767798116;0.4531594749960801;0.1576435284511708\n",
      "\n",
      "\n",
      "30;1;Autumn;T0;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0006361007690429688;0.23170289601417968;0.20009702271821328;0.9983444070659999;0.9983444070659999;6.29425048828125e-05;1.761829290030928;1.5047068568678674;0.9367940093075926;0.9310941926560996\n",
      "\n",
      "30;1;Autumn;T0;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0006990432739257812;0.28007823977431573;0.22997481958082083;0.9989917435974;0.9989917435974;7.367134094238281e-05;2.9405224106556926;2.498058605580419;0.9315576758517227;0.8080548419010853\n",
      "\n",
      "30;1;Autumn;T0;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007102489471435547;0.5562786564019531;0.4103445021599777;0.9956192883521089;0.9956192883521089;8.630752563476562e-05;2.254321225919307;1.7587393430215617;0.8873544961274454;0.8871868373791353\n",
      "\n",
      "30;1;Autumn;T0;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0006926059722900391;1.2243328423719486;0.9228731431086753;0.9758197227195964;0.9758197227195964;6.556510925292969e-05;3.4787405633227335;2.9099068211037973;0.7773542387703767;0.7313590240698447\n",
      "\n",
      "30;1;Autumn;T0;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007295608520507812;1.3486936244944283;1.0666561415184692;0.965751503537138;0.965751503537138;6.67572021484375e-05;2.8858863615626746;2.383712390604325;0.8299147838266607;0.8151214077696057\n",
      "\n",
      "30;1;Autumn;T0;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009739398956298828;1.4049834060495947;1.0822586106556602;0.9633687041760954;0.9633687041760954;7.05718994140625e-05;3.032944016188685;2.406246371868261;0.8844447063185453;0.7957994244252831\n",
      "\n",
      "30;1;Autumn;T0;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008480548858642578;1.602113463143516;1.2988313486836203;0.9522808904416602;0.9522808904416602;6.651878356933594e-05;2.6659696506340973;2.17427726876921;0.8451540829666082;0.8422248592658533\n",
      "\n",
      "30;1;Autumn;T0;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008025169372558594;2.0556700222018587;1.6276647764573853;0.9184485148692441;0.9184485148692441;6.699562072753906e-05;3.0808668296636843;2.5511800522648116;0.8194274389640024;0.7892953952763346\n",
      "\n",
      "30;1;Autumn;T0;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007834434509277344;2.0689607712615614;1.6499181189232814;0.9101746986040927;0.9101746986040927;7.152557373046875e-05;2.9313931367758044;2.450913680825212;0.8298436174884294;0.8092448343646791\n",
      "\n",
      "30;1;Autumn;T0;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007929801940917969;2.1689962446935613;1.7223621414277643;0.8997266512562346;0.8997266512562346;6.532669067382812e-05;2.795729880402132;2.2962949161318122;0.8281875227408323;0.8264923649261149\n",
      "\n",
      "30;1;Autumn;T0;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008063316345214844;2.0938064835040477;1.625618481256164;0.9053461135753111;0.9053461135753111;6.628036499023438e-05;2.773197531230043;2.24367820199692;0.8293065277232453;0.8292778844306061\n",
      "\n",
      "30;1;Autumn;T0;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009281635284423828;2.2826587141533268;1.7243082612607283;0.8988109668061601;0.8988109668061601;0.00010776519775390625;2.5239555944995367;2.0132375360563213;0.8644376727233134;0.8585862647589055\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Autumn;T0;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009067058563232422;2.2412999598347714;1.7051776231450082;0.8981820321707514;0.8981820321707514;0.00011086463928222656;2.5369160251099347;2.0139882512831773;0.8599327918636808;0.8571302260334109\n",
      "\n",
      "30;1;Autumn;T16;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0007307529449462891;2.2657301003209933;1.6555230251555475;0.6417575391550845;0.6295399633442242;0.000881195068359375;5.4369812864297025;4.215407044550133;0.24148763161416265;-0.11493633610161047\n",
      "\n",
      "30;1;Autumn;T16;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0013401508331298828;4.654731933257233;3.399713605807731;0.5466102736498132;0.5349186122934333;0.001888275146484375;5.022011321457257;3.9603477811239705;0.15989733976541043;0.048760736601921706\n",
      "\n",
      "30;1;Autumn;T16;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0027649402618408203;4.179237325029981;2.820676283621876;0.6009048816189535;0.5992118463439979;0.003060579299926758;4.86171838281142;3.717408539358636;0.3029399808440634;0.10851510275409215\n",
      "\n",
      "30;1;Autumn;T16;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.004469871520996094;3.810587404256802;2.6219017660808146;0.6292348324978156;0.6127293014325518;0.003922224044799805;5.096984071565946;3.805159460961968;0.3751631836203082;0.020146956653051884\n",
      "\n",
      "30;1;Autumn;T16;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.0067021846771240234;3.475272976081071;2.2688451831905336;0.6244140585382361;0.61227074973707;0.00475621223449707;4.713144001970022;3.505657121764753;0.4444736603941831;0.1621701847525746\n",
      "\n",
      "30;1;Autumn;T16;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.009970903396606445;3.3579940840730673;2.3378175643030548;0.6512063024255951;0.6486066294425556;0.005840778350830078;4.4631710312957384;3.3139519273650535;0.4917542495698862;0.24868605077526185\n",
      "\n",
      "30;1;Autumn;T16;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.013816356658935547;3.2859720936342707;2.2140979729052375;0.6565231052909939;0.6351341919625311;0.006940603256225586;4.622652580291059;3.398213494303377;0.49430176682970606;0.19403366319272253\n",
      "\n",
      "30;1;Autumn;T16;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.017186880111694336;3.1077103588831205;2.0414226717211847;0.6645715141150847;0.6438529525760864;0.007812738418579102;4.499531494529293;3.308985532779494;0.5184673982915868;0.23639461028286735\n",
      "\n",
      "30;1;Autumn;T16;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.021581411361694336;2.9612312536508614;1.8884445386613509;0.6658209174920924;0.6456050721444464;0.008598089218139648;4.484724726635184;3.296564968641795;0.5326731287007695;0.2414119879302793\n",
      "\n",
      "30;1;Autumn;T16;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.027604341506958008;3.0523945560306407;2.0424534901180946;0.633770675637779;0.6152025437249236;0.009963750839233398;4.437615943030262;3.263178942560629;0.5447817514995579;0.25726512026167103\n",
      "\n",
      "30;1;Autumn;T16;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.03335976600646973;2.928301291801176;2.013000102710121;0.6497177007787369;0.6417652398444121;0.010911941528320312;4.523498682749863;3.322987792083742;0.5395380431278618;0.22823810936411415\n",
      "\n",
      "30;1;Autumn;T16;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.04069948196411133;2.8585802727289953;1.9590511104108461;0.6665498700596271;0.6592853720462645;0.011717557907104492;4.655935986536089;3.4225130143545854;0.5276358923383586;0.18238585859097345\n",
      "\n",
      "30;1;Autumn;T16;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];SVR;0.047034502029418945;2.7497054400132024;1.8902030453078744;0.6714178040518775;0.6644284150739341;0.012842178344726562;4.592236073548678;3.378994086632644;0.5455337006406018;0.20460509517700953\n",
      "\n",
      "30;1;Autumn;T31;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.09218120574951172;2.015576108107971;1.658734787170057;0.0;0.0;0.0007495880126953125;3.6719644226003685;3.0174757814976747;0.0;-0.5327326836073787\n",
      "\n",
      "30;1;Autumn;T31;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.18722200393676758;1.0454143020288102;0.813159280278418;0.8952377856881446;0.8952377856881446;0.0009534358978271484;1.8828451175710843;1.4641681000056863;0.6524812628910291;0.5970059055859824\n",
      "\n",
      "30;1;Autumn;T31;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.24203133583068848;0.4715610621445028;0.3202505857651543;0.9748319683611243;0.9748319683611243;0.0011382102966308594;1.7368385616230255;1.2893424127374258;0.7122131284118556;0.6570834904080192\n",
      "\n",
      "30;1;Autumn;T31;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.3278985023498535;0.3753130345696232;0.2489679882116113;0.9797522580806328;0.9797522580806328;0.0011584758758544922;1.8079198204593545;1.3128412219079848;0.7200651075353537;0.6284409695848254\n",
      "\n",
      "30;1;Autumn;T31;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.40553736686706543;0.3597774728467653;0.22439963334898455;0.9772693745910889;0.9772693745910889;0.0012371540069580078;1.8116516569653094;1.2937470244198674;0.7145190540066199;0.6269054715854165\n",
      "\n",
      "30;1;Autumn;T31;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.48038506507873535;0.2769357557241361;0.1784929580372511;0.9864668407122253;0.9864668407122253;0.0015828609466552734;1.65308586659262;1.167877837205933;0.7384440405804261;0.689357899256455\n",
      "\n",
      "30;1;Autumn;T31;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.5904593467712402;0.2580778368770466;0.16624058564133432;0.9868666444487227;0.9868666444487227;0.002038717269897461;1.7506420219979617;1.2415675739085523;0.6954319435223252;0.6516111984461921\n",
      "\n",
      "30;1;Autumn;T31;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.8545966148376465;0.24406192506779617;0.15346903081780008;0.9874475945102785;0.9874475945102785;0.001566171646118164;1.775334328060615;1.2772118679112947;0.6935785465082669;0.6417140397202623\n",
      "\n",
      "30;1;Autumn;T31;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.7026050090789795;0.2313217804963481;0.14198651709418825;0.9877370934589068;0.9877370934589068;0.0014538764953613281;1.7621595522648252;1.2565313825337021;0.7054590201023294;0.647011996601633\n",
      "\n",
      "30;1;Autumn;T31;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.7819373607635498;0.22662776623894054;0.1367983239172415;0.9875998910742572;0.9875998910742572;0.0015556812286376953;1.6872204149435206;1.2091233219802813;0.7183996818473024;0.676396558155143\n",
      "\n",
      "30;1;Autumn;T31;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.8708884716033936;0.22715929008175748;0.13002940717619735;0.9867936140350181;0.9867936140350181;0.0015707015991210938;1.6677613240798872;1.201009433684593;0.7202164461276293;0.683817895879244\n",
      "\n",
      "30;1;Autumn;T31;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;4.324755430221558;0.20147777980647136;0.12335623174368426;0.9902441094990692;0.9902441094990692;0.0017161369323730469;1.7577965244579399;1.2370482917973211;0.7028644441075247;0.6487577975054627\n",
      "\n",
      "30;1;Autumn;T31;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;2.8988001346588135;0.19982106074604294;0.12007927414003608;0.9896987363205494;0.9896987363205494;0.0016655921936035156;1.7194932865615673;1.2072289151832007;0.7145868994548179;0.6638984943802666\n",
      "\n",
      "30;1;Autumn;T46;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0013475418090820312;0.08381518360339237;0.06869373542856179;0.983574007397617;0.983574007397617;8.20159912109375e-05;0.8077860315373357;0.6211792422199369;0.13764721383794587;0.13667894036588168\n",
      "\n",
      "30;1;Autumn;T46;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008170604705810547;0.14350525524544455;0.11407199190494886;0.9828574856425609;0.9828574856425609;6.771087646484375e-05;1.5818871164045476;1.432639624812454;0.3635671321877708;-2.3107823795004245\n",
      "\n",
      "30;1;Autumn;T46;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007851123809814453;0.20832397938001418;0.15540418644174153;0.9583947977283291;0.9583947977283291;6.341934204101562e-05;1.381120521359379;1.21992812235672;0.31183770613801753;-1.5237295552358163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Autumn;T46;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007774829864501953;0.30071896775989543;0.22343127504041832;0.8893212104403281;0.8893212104403281;6.842613220214844e-05;0.8581351516551918;0.7041489316215379;0.30669784513064036;0.025703725866146\n",
      "\n",
      "30;1;Autumn;T46;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0007891654968261719;0.31121976231096465;0.2502548560715115;0.8565762569499811;0.8565762569499811;6.651878356933594e-05;0.7060759996813522;0.5943761791859908;0.382360631102021;0.34039710002972523\n",
      "\n",
      "30;1;Autumn;T46;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008180141448974609;0.49054630750979183;0.38007185059701687;0.7696945223209971;0.7696945223209971;6.723403930664062e-05;0.7356412926261834;0.587956985753115;0.7026072381639363;0.28400192469438656\n",
      "\n",
      "30;1;Autumn;T46;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0010449886322021484;0.5073065132197446;0.3795760948341296;0.725184737062667;0.725184737062667;7.43865966796875e-05;0.4634752478526082;0.3387210268311089;0.747723203544344;0.7157943235024076\n",
      "\n",
      "30;1;Autumn;T46;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008950233459472656;0.5124925938975903;0.39156488793276223;0.7186306739324471;0.7186306739324471;6.866455078125e-05;0.46128370073247005;0.3274406583102387;0.7729228473282831;0.7184757074958195\n",
      "\n",
      "30;1;Autumn;T46;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0009043216705322266;0.5054581071456905;0.3866376099383656;0.7102400944392306;0.7102400944392306;7.319450378417969e-05;0.46404232711493876;0.3263150030769915;0.7824297186150373;0.7150984255310096\n",
      "\n",
      "30;1;Autumn;T46;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008533000946044922;0.49379275594159466;0.3747883916084309;0.7140332121837457;0.7140332121837458;6.437301635742188e-05;0.43347070394549253;0.30144234320833446;0.7949637458832316;0.7514011247855045\n",
      "\n",
      "30;1;Autumn;T46;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0008635520935058594;0.4930712023318943;0.3630782704714995;0.6938213690628576;0.6938213690628576;6.628036499023438e-05;0.43269175173428537;0.2926306672334315;0.7874491086143358;0.7522937924979256\n",
      "\n",
      "30;1;Autumn;T46;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0010652542114257812;0.5494725293597176;0.4010035729407149;0.6756755475455372;0.6756755475455372;9.393692016601562e-05;0.4573003553612374;0.312000837928944;0.8065112784544867;0.7233168350244346\n",
      "\n",
      "30;1;Autumn;T46;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LinearRegression;0.0010311603546142578;0.5409050602294889;0.39197484210648875;0.6608766025069206;0.6608766025069206;9.655952453613281e-05;0.4723321166304746;0.33029172204352825;0.7962390251107494;0.7048283718678143\n",
      "\n",
      "30;1;Autumn;T76;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0006835460662841797;0.0;0.0;1.0;1.0;0.01322174072265625;0.1178315570224407;0.06696318472211787;0.5468844935270234;0.34238414309131304\n",
      "\n",
      "30;1;Autumn;T76;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0004131793975830078;0.0;0.0;1.0;1.0;0.013136148452758789;0.12024544856082382;0.07460343038583381;0.3429035838556954;0.31516438584909\n",
      "\n",
      "30;1;Autumn;T76;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.00041985511779785156;0.0;0.0;1.0;1.0;0.012928962707519531;0.12708363864151312;0.07723317981090652;0.256453050931187;0.23505831929720777\n",
      "\n",
      "30;1;Autumn;T76;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0004527568817138672;0.0;0.0;1.0;1.0;0.01320028305053711;0.12240979920916138;0.07413487935544193;0.31532281133898976;0.2902891988465981\n",
      "\n",
      "30;1;Autumn;T76;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0005247592926025391;0.0;0.0;1.0;1.0;0.01247549057006836;0.11619408062618843;0.07063531916882905;0.3851086922012794;0.36053459863228277\n",
      "\n",
      "30;1;Autumn;T76;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0005502700805664062;0.0;0.0;1.0;1.0;0.012435197830200195;0.10907317645314259;0.06604361020261648;0.4562371551912131;0.43651161875136124\n",
      "\n",
      "30;1;Autumn;T76;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0006115436553955078;0.0;0.0;1.0;1.0;0.012028932571411133;0.09935869203331192;0.059905485660161906;0.5476771241728586;0.5324147796854254\n",
      "\n",
      "30;1;Autumn;T76;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0006160736083984375;0.0;0.0;1.0;1.0;0.017196178436279297;0.09341056223328326;0.055858470574109406;0.5977206179432437;0.5867232097071019\n",
      "\n",
      "30;1;Autumn;T76;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0006887912750244141;0.0;0.0;1.0;1.0;0.015126705169677734;0.08762496317612735;0.05155908349132195;0.6449879160942095;0.6363322909721221\n",
      "\n",
      "30;1;Autumn;T76;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0006916522979736328;0.0;0.0;1.0;1.0;0.012317419052124023;0.08267631556070917;0.048367838536705926;0.6833963443489266;0.6762488854770184\n",
      "\n",
      "30;1;Autumn;T76;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0008504390716552734;0.0;0.0;1.0;1.0;0.013331174850463867;0.07908334218408207;0.04594420907926135;0.7092399628334256;0.7037767969047799\n",
      "\n",
      "30;1;Autumn;T76;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0008409023284912109;0.0;0.0;1.0;1.0;0.01249074935913086;0.07648959021444293;0.044691901174865836;0.7273311956815627;0.722889034045086\n",
      "\n",
      "30;1;Autumn;T76;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];KNeighborsRegressor;0.0008845329284667969;0.0;0.0;1.0;1.0;0.012297868728637695;0.07363336743524221;0.04268005747806179;0.7457753506205043;0.7431980197673693\n",
      "\n",
      "30;1;Autumn;T97;1years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.2581319808959961;0.009755216932850938;0.00895403328632427;1.1102230246251565e-16;-2.5091040356528538e-14;0.0006933212280273438;0.03922429774470922;0.020073288001074593;1.1102230246251565e-16;-0.18091194005060607\n",
      "\n",
      "30;1;Autumn;T97;2years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.16411709785461426;0.0613888340714513;0.03181485226839247;0.4578482832095042;0.4578482832095043;0.0008058547973632812;0.06284553343162538;0.04916015750475881;-1.1842257229129407;-2.0314889787080985\n",
      "\n",
      "30;1;Autumn;T97;3years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.25490689277648926;0.04276848805592692;0.020866908847054357;0.6346157486518179;0.634615748651818;0.001049041748046875;0.03517375450141015;0.025721330257768873;0.05388321189276302;0.05039145040941284\n",
      "\n",
      "30;1;Autumn;T97;4years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.36226558685302734;0.0358470503099812;0.014327131639884041;0.8290660862721773;0.8290660862721773;0.0011394023895263672;0.03635178896054901;0.02494613916472028;-0.014270900199186753;-0.014282043017990409\n",
      "\n",
      "30;1;Autumn;T97;5years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.7183096408843994;0.03012154198723111;0.011552891321951034;0.867065411899233;0.8670654118992329;0.0013613700866699219;0.04229721832124635;0.02702739007738184;-0.3632031840524159;-0.37319044787773925\n",
      "\n",
      "30;1;Autumn;T97;6years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.5056097507476807;0.07594547159926676;0.024967966546919792;0.7563453240857746;0.7563453240857747;0.0014190673828125;0.052382249988314165;0.029351877543436607;-1.0773779024783199;-1.1060828789566877\n",
      "\n",
      "30;1;Autumn;T97;7years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.6599259376525879;0.058046986260869404;0.018827449833948653;0.8463811543177543;0.8463811543177543;0.0014297962188720703;0.049696911045186094;0.03314136184640582;-0.7721421747966317;-0.8956840288087922\n",
      "\n",
      "30;1;Autumn;T97;8years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.6788063049316406;0.06230638617632912;0.02225427571238343;0.8848781599928367;0.8848781599928367;0.0016574859619140625;0.0969474682399366;0.05096683171722358;-6.01894448238099;-6.214065879999993\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30;1;Autumn;T97;9years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.784651517868042;0.060317188862223814;0.022142239643182134;0.9011681609059907;0.9011681609059907;0.0016355514526367188;0.0886167544772924;0.045687152451351215;-4.879406187421106;-5.027522503387584\n",
      "\n",
      "30;1;Autumn;T97;10years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;0.8427295684814453;0.0625067380055147;0.021953553328675417;0.8825256194175397;0.8825256194175397;0.0017786026000976562;0.08060305808457947;0.04126964553928346;-3.89459761744957;-3.9866650552094676\n",
      "\n",
      "30;1;Autumn;T97;11years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;1.0204036235809326;0.05970232659697259;0.021396600349133228;0.8821067758112444;0.8821067758112444;0.00213623046875;0.07552188308383753;0.03853246553220724;-3.3753794733479934;-3.377768383240471\n",
      "\n",
      "30;1;Autumn;T97;12years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;1.0653564929962158;0.08095816010646854;0.02889508348942059;0.8563855810277465;0.8563855810277464;0.0017397403717041016;0.08148377116587198;0.03867465862892656;-3.9605635448422714;-4.0962344678493725\n",
      "\n",
      "30;1;Autumn;T97;13years;['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP'];LGBMRegressor;1.3328032493591309;0.0687357531151287;0.024932497489881942;0.8894006431769765;0.8894006431769765;0.002084493637084961;0.05928981879002557;0.03214975855749476;-1.6971909782126904;-1.6981581752244068\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# sequence length\n",
    "sequence_length = 1\n",
    "# resolution\n",
    "res = 30\n",
    "loc = 'Toolik'\n",
    "print(f'Res;seqLen;season;target;trSize;feature;model;trainDur;RMSE;MAE;Var;R2;testDur;RMSE;MAE;Var;R2')\n",
    "for season in ['Winter','Spring','Summer','Autumn']:\n",
    "        for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "\n",
    "            features, model_final = BestModelInput_switch(trgt, season)\n",
    "\n",
    "            get_optimal_sequence_length(loc, season, trgt)        \n",
    "            \n",
    "            # Get all data for the season\n",
    "            X_all, y_all, df_all = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "            train_size = int(len(X_all) * 0.8)\n",
    "\n",
    "            # Slice down to only the training set\n",
    "            X_all_train = X_all[:train_size]\n",
    "            y_all_train = y_all[:train_size]\n",
    "            df_all_train = df_all.iloc[:train_size]\n",
    "\n",
    "            unique_years = df_all_train['Year'].nunique()\n",
    "\n",
    "            for tr_size in range(1, unique_years + 1):  # Use 1 year, 2 years, ... up to all years for training\n",
    "                # Get all data for the season - do this everytime to standardize the test set similarly to the training set\n",
    "                X_all, y_all, df_all = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "                train_size = int(len(X_all) * 0.8)\n",
    "\n",
    "                # Slice down to only the training set\n",
    "                X_all_train = X_all[:train_size]\n",
    "                y_all_train = y_all[:train_size]\n",
    "                df_all_train = df_all.iloc[:train_size]\n",
    "\n",
    "                X_test = X_all[train_size:]\n",
    "                y_test = y_all[train_size:]\n",
    "\n",
    "                unique_years = df_all_train['Year'].nunique()\n",
    "\n",
    "                print(f'{res};{sequence_length};{season};{trgt};{tr_size}years;{features}',end=';')\n",
    "                print(str(model_final).split('(')[0],end=';')\n",
    "                \n",
    "                indices = get_training_indices(df_all_train, tr_size)\n",
    "                X_train = X_all_train[indices]\n",
    "                y_train = y_all_train[indices]\n",
    "\n",
    "                # Scale features\n",
    "                X_train_scaled, X_test_scaled = scale_features(X_train, X_test)   \n",
    "\n",
    "                # Train model\n",
    "                start_train = time.time()\n",
    "                model_final.fit(X_train_scaled, y_train)\n",
    "                end_train = time.time()\n",
    "\n",
    "                # Evaluate training performance\n",
    "                y_pred_train = model_final.predict(X_train_scaled)\n",
    "                rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "                r2_train = r2_score(y_train, y_pred_train)\n",
    "                mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "                explained_variance_train = explained_variance_score(y_train, y_pred_train)\n",
    "                train_duration = end_train - start_train\n",
    "                print(f'{train_duration};{rmse_train};{mae_train};{explained_variance_train};{r2_train}',end=';')\n",
    "\n",
    "                # Test model\n",
    "                start_test = time.time()\n",
    "                y_pred_test = model_final.predict(X_test_scaled)\n",
    "                end_test = time.time()\n",
    "\n",
    "                # Evaluate testing performance\n",
    "                rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "                r2_test = r2_score(y_test, y_pred_test)\n",
    "                mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "                explained_variance_test = explained_variance_score(y_test, y_pred_test)\n",
    "                test_duration = end_test - start_test\n",
    "\n",
    "                print(f'{test_duration};{rmse_test};{mae_test};{explained_variance_test};{r2_test}')\n",
    "                RES = '_res'+ str(res) + 'D'\n",
    "                model_name = str(model_final).split('(')[0]\n",
    "                \n",
    "                # Write results to a CSV file\n",
    "                formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test[:,-3:]])\n",
    "                formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "                y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "                y_preds = pd.DataFrame(y_pred_test, columns=['preds'])\n",
    "                results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "                results.to_csv(f'Results3/TrainSize_{loc}_{RES}_{trgt}_{season}_{tr_size}years_results.csv', index=False)\n",
    "\n",
    "                print(f'')\n",
    "        print(f'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a354463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6403a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813faa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e8606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa2fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe13dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f60980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da313535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa1fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e83816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a984fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bd972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afbc4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res:TrainSize:seqLen:Season:Feature:RMSE:R2\n",
      "7:1:1:Winter:Time:10.019852996004552:-0.6844820053562999:6.6977770280749125:-0.72247352587151:4.462820856770888:-0.6238184731030998:2.11571845660505:0.5086088956048325:1.8051706082349497:0.6306458137880641:1.6766852632295486:0.6290854938233288:\n",
      "7:0.75:1:Winter:Time:9.814186408440088:-0.6160406517767063:7.266330887114187:-1.0273163612891807:4.853563247731016:-0.9206134080035726:2.2470396271791007:0.44571515602419054:1.7344283762082493:0.659027574772886:1.515013542411485:0.6971666129494432:\n",
      "7:0.5:1:Winter:Time:10.314572255599117:-0.7850324731073441:6.883111405617229:-0.8191176266857303:3.647207958873599:-0.08452513940192774:3.408776397914864:-0.2755825494218085:3.4945489162020418:-0.3841678773151225:3.274924091914998:-0.415054609319494:\n",
      "7:0.25:1:Winter:Time:9.888751174425062:-0.6406901632719688:6.647931787744446:-0.696931430412804:4.107488315613764:-0.3755337257817264:2.294540115246412:0.4220332552576017:1.8221978425744112:0.6236451003222434:1.6632312632788233:0.6350141703920565:\n",
      "7:0:1:Winter:Time:10.010119629605391:-0.6812109559488915:7.511220643491321:-1.1662681961554862:4.023504744508384:-0.3198592037128454:3.113211074397706:-0.06396826444098358:3.073720176290843:-0.07086610853129072:2.912688211123362:-0.11933148561199003:\n",
      "7:1:1:Spring:Time:5.896478671632455:0.5853793091283721:4.977903681351458:0.6039444560120215:3.9210579436269284:0.6505441754407673:2.337491170230591:0.611566197857496:2.0291003612688403:0.46993001776870946:1.9052099424930917:0.34630697362711993:\n",
      "7:0.75:1:Spring:Time:6.500859167006667:0.4960272873912187:4.9384784598874845:0.6101931681006825:4.029284631756956:0.6309870070822611:2.527436516733159:0.5458727425467476:2.0821052794701327:0.44187493697709246:1.841068709865:0.38958082535140814:\n",
      "7:0.5:1:Spring:Time:5.556026598760201:0.6318759940512851:4.871627092811864:0.6206752389731313:4.77601833123797:0.4815370421154084:3.2562054339317923:0.24622687116816855:2.949451565148856:-0.11997593741018098:2.7594799000592327:-0.37133210452892795:\n",
      "7:0.25:1:Spring:Time:7.166344926055162:0.38756370283466735:4.6826245035159095:0.649537313666125:4.145368054222873:0.609418242165302:2.2334621914717796:0.6453709832402885:1.7775856566310428:0.5931941130050271:1.6156076353630333:0.5299327794195376:\n",
      "7:0:1:Spring:Time:6.504332784644182:0.4954885659202546:6.440224535877584:0.3370738708598847:6.085986393867039:0.15812479784243205:3.9131562306473535:-0.08860777626626559:3.324007359801491:-0.42249290169392983:3.0348385974876906:-0.6586676373114058:\n",
      "7:1:1:Summer:Time:4.044843693704527:0.03695082696881624:4.294389863836907:-0.08387870829673805:3.8707056043748023:0.04698961708373284:1.7462971668333747:0.1955081978194838:0.40243574496370205:0.5443590319338036:0.4435041158562948:-0.014352507983414808:\n",
      "7:0.75:1:Summer:Time:3.857845750780784:0.12393830102847803:3.99560055658492:0.06169965318918369:6.237885243060233:-1.4750963597491737:2.5466132956912406:-0.7108476141343181:0.5464015342791567:0.16004989389938074:0.2552115046007395:0.6641123050419182:\n",
      "7:0.5:1:Summer:Time:3.7293534919769566:0.18132396725660438:5.0503749869656405:-0.49908064778364913:8.92440202612024:-4.066121508650384:3.4759082313824265:-2.187292553514335:0.7146271667010028:-0.43677483553965835:0.7353533762533968:-1.7885971532901443:\n",
      "7:0.25:1:Summer:Time:4.392849790326402:-0.1358936760887175:5.327086776345271:-0.6678511534295215:7.840630869773423:-2.9103836279186446:3.450087478522897:-2.1401148884015124:0.5128354125478563:0.26007843967558864:0.49643463230068646:-0.27091861410785323:\n",
      "7:0:1:Summer:Time:4.9354731888965055:-0.4338461988114526:4.2095669378462945:-0.04148396188559689:8.778272755884581:-3.9015731873380437:3.592588339591543:-2.404867657127557:0.8117177654648244:-0.8537013715679396:0.9655358272152584:-3.807622786459202:\n",
      "7:1:1:Autumn:Time:6.448304364301045:0.2940376702548162:4.3055308031059685:0.48610249274874273:2.935121630881696:0.21711653987285506:0.7549311652677996:0.2345888996593989:0.21946868901326025:-1.8789667660833103:0.02153985047947224:-0.0499081780940569:\n",
      "7:0.75:1:Autumn:Time:5.6672153796978755:0.4547070192318625:6.393696547108963:-0.13325371201592873:3.792732964447127:-0.3072222896787147:0.7655018816791028:0.21300391373337657:0.11669080145175514:0.18611178867586908:0.17529098149848713:-68.53205988839215:\n",
      "7:0.5:1:Autumn:Time:6.3029487773291:0.3255060996718472:6.447552285883391:-0.1524254885924281:3.0886383521293577:0.13307998166335555:0.6685374933395631:0.3997508070879322:0.36820010486860083:-7.103251689547102:0.34616444980585226:-270.1632808383491:\n",
      "7:0.25:1:Autumn:Time:6.79228118366795:0.21671143985088637:6.242096746733374:-0.08015007678000852:3.6167500040048064:-0.1887263593092221:0.8127624359527377:0.11282908789546742:0.12239535495773902:0.10459114484649124:0.17653477669527232:-69.52230443104624:\n",
      "7:0:1:Autumn:Time:6.776323527806449:0.2203876028251931:7.615302911617817:-0.6076721016037081:3.8702161999432643:-0.3611793901564786:0.8359707393626451:0.061439655868299914:0.15221639861683228:-0.3848871885814831:0.2739555127680859:-168.83454741790985:\n"
     ]
    }
   ],
   "source": [
    "#Baseline: TIME only\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# List of features to use\n",
    "features = ''\n",
    "# sequence length\n",
    "sequence_length = 1\n",
    "# resolution\n",
    "res = 7\n",
    "loc = 'Toolik'\n",
    "print(f'Res:seqLen:Feature:TrainSize:Season:RMSE:R2')\n",
    "# Prepare sequences\n",
    "for season in ['Winter','Spring','Summer','Autumn']:\n",
    "    for tr_size in [1,3/4,1/2,1/4,0]:\n",
    "        print(f'{res}:{sequence_length}:Time:{tr_size}:{season}',end=':')\n",
    "        for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "            X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            if tr_size == 0:\n",
    "                X_train, X_test = X[:int(np.floor(90))], X[train_size:]\n",
    "                y_train, y_test = y[:int(np.floor(90))], y[train_size:]                \n",
    "            else:\n",
    "                X_train, X_test = X[:int(np.floor(train_size*tr_size))], X[train_size:]\n",
    "                y_train, y_test = y[:int(np.floor(train_size*tr_size))], y[train_size:]\n",
    "\n",
    "            # Train final LightGBM model using fused features\n",
    "            model_final = LGBMRegressor(verbose=-1,force_col_wise=True)\n",
    "            model_final.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model_final.predict(X_test.reshape(X_test.shape[0], -1))  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Evaluate the model\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f'{rmse_test}:{r2}',end=':')\n",
    "\n",
    "        print(f'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9223cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res:seqLen:Feature:TrainSize:Season:RMSE:R2\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:1:Winter:3.7198342251353087:0.7678381542046916:3.433747181385108:0.5472819437475749:2.956878108759022:0.2871709115967733:1.9053306268912458:0.6014780580712136:2.304307811187044:0.3981510569188952:2.1062300298967758:0.41469487738677335:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.75:Winter:5.81268206954176:0.43311258593464863:5.806778308264995:-0.29467772510915125:4.2671460878776335:-0.48454577201379845:1.9210362212357426:0.5948809663536121:1.6301678016461663:0.6987887835907982:1.4467059598323737:0.7238587634566358:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.5:Winter:6.278892695527048:0.33853053883306816:5.507588966228792:-0.16470040591638346:2.6409345911462423:0.4313646434479559:2.6972628067856945:0.20134710812202028:3.1546604371789244:-0.1280069014080194:3.0027836686599225:-0.18964891948319518:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.25:Winter:6.201371298056012:0.3547631744990527:6.653996304100395:-0.7000288636329322:3.974685318537791:-0.28802434267989474:1.969956081042251:0.5739852571244517:1.6920864510526559:0.6754724178545968:1.4754735676674613:0.7127674911589983:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0:Winter:8.003629559336467:-0.0747751960498586:7.0884361525285975:-0.9292657225774095:3.557599383199773:-0.03188821369360584:2.7584127773298572:0.16472390458435804:2.9263176878408044:0.02937949262631201:2.8129167462890643:-0.04396149644241776:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:1:Spring:2.3613940579319523:0.9335029408181816:2.9744363666437823:0.8585924872634989:2.7511415814622575:0.8279670466373965:1.5531471679979787:0.8285086275225644:1.510109440907522:0.7064088230730873:1.441628203377383:0.6257216436896018:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.75:Spring:2.4298235756839373:0.9295931377342561:2.7422855675257405:0.879804426286746:3.1974426124332527:0.7676239954540468:1.8899714505049614:0.7460620872419345:1.5712312422679613:0.682161599459014:1.5072955163572606:0.5908477083445036:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.5:Spring:2.9413679979295417:0.8968274337662552:3.15416358817995:0.8409873971312127:3.6130089153758664:0.7032957073637004:2.080499207000278:0.6922825201107383:2.180721822454293:0.3877529606507667:2.1677327500408197:0.1537481105505587:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.25:Spring:5.3048173243680745:0.6644120382032848:3.54115096881512:0.7995749487374966:3.5973935123077956:0.705854873416732:1.5977803083153577:0.8185106326579372:1.5300091394177646:0.6986201553797529:1.471949512139993:0.6098119168527074:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0:Spring:5.597199604709211:0.6263998029848303:6.17318542846478:0.3909095874223616:5.816570464755403:0.23101166733529832:3.699276314992925:0.02713943050453249:3.224645568116577:-0.33872116047689826:2.968107189343699:-0.5865265167437674:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:1:Summer:0.9756285685044762:0.9439708552737754:1.246748446671864:0.908644348620015:1.3256145992835657:0.888223139712488:1.6177315005125221:0.3096040922390554:0.38177812362141317:0.5899359022718622:0.4603210795129372:-0.0927361840867158:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.75:Summer:1.122672400735785:0.9258090268215418:1.1285382165135764:0.9251468143720714:5.921053314333987:-1.2300535550486367:2.196447476334872:-0.2727027104607822:0.5215494339752494:0.23471953786622457:0.3566127238290279:0.3441760578616956:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.5:Summer:1.304179304616757:0.8998802897837743:4.108630288916104:0.007862479895922814:8.329647054264749:-3.413372140710252:3.221125282764633:-1.7371623489643748:0.6680172405068898:-0.25546615996826416:0.693388182872546:-1.47939927541079:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.25:Summer:1.313076968991156:0.8985095114083825:2.1531429764371306:0.727526992179266:7.377946572903157:-2.462488675202992:3.1988311879876306:-1.6994044995005275:0.5174536762810703:0.24669192557398256:0.5077969569010021:-0.32976159546862704:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0:Summer:3.023766894267623:0.4618026054457596:3.8740181780905067:0.11793405430703008:8.55602983072872:-3.6565249022297674:3.480621153893698:-2.1959416003203884:0.7419708536096872:-0.5488286014631318:0.9296309203731025:-3.4567135253702066:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:1:Autumn:2.3618423808319764:0.9052907726236205:3.073619620821217:0.7381073262388321:2.0665740991394377:0.6118971108632126:0.6152029634748579:0.4917037740529683:0.14966367988069462:-0.338826658366459:0.028741109412393336:-0.8692741748708925:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.75:Autumn:4.9760714066487965:0.5795990596172682:5.821572658930347:0.06048492221234636:3.5815186683970497:-0.1656800136288945:0.6639303375980236:0.4079954086799974:0.18528161782236163:-1.0518999509696374:0.2285789728814436:-117.23294404093726:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.5:Autumn:5.33917717016142:0.5160069384292638:6.452643619260263:-0.15424624124217656:3.32446399092062:-0.004357156063822476:0.6724137549785204:0.39276999223555364:0.3112702251395495:-4.79117478009764:0.32063683681764127:-231.64449525352745:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0.25:Autumn:3.5582752537130897:0.7850341106981396:6.092213143128282:-0.028900286629839567:3.5756674263977426:-0.16187430721998664:0.6054708350606854:0.5076584321143046:0.29480286741376754:-4.194633590433054:0.26641204953220754:-159.61040501487986:\n",
      "7:1:['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:0:Autumn:6.172493493526217:0.3531378309015456:7.576710944171326:-0.5914190326683852:3.837537930011367:-0.3382901265761653:0.8142299742798055:0.10962241229830472:0.15220089005678236:-0.38460500463547986:0.2750102874821826:-170.14484804666964:\n"
     ]
    }
   ],
   "source": [
    "#Baseline: select features\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# List of features to use\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "# sequence length\n",
    "sequence_length = 1\n",
    "# resolution\n",
    "res = 7\n",
    "loc = 'Toolik'\n",
    "print(f'Res:seqLen:Feature:TrainSize:Season:RMSE:R2')\n",
    "# Prepare sequences\n",
    "for season in ['Winter','Spring','Summer','Autumn']:\n",
    "    for tr_size in [1,3/4,1/2,1/4,0]:\n",
    "        print(f'{res}:{sequence_length}:{features}:{tr_size}:{season}',end=':')\n",
    "        for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "            X, y, _ = prepare_sequences(loc, sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            if tr_size == 0:\n",
    "                X_train, X_test = X[:int(np.floor(90))], X[train_size:]\n",
    "                y_train, y_test = y[:int(np.floor(90))], y[train_size:]                \n",
    "            else:\n",
    "                X_train, X_test = X[:int(np.floor(train_size*tr_size))], X[train_size:]\n",
    "                y_train, y_test = y[:int(np.floor(train_size*tr_size))], y[train_size:]\n",
    "\n",
    "            # Train final LightGBM model using fused features\n",
    "            model_final = LGBMRegressor(verbose=-1,force_col_wise=True)\n",
    "            model_final.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model_final.predict(X_test.reshape(X_test.shape[0], -1))  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Evaluate the model\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f'{rmse_test}:{r2}',end=':')\n",
    "\n",
    "            # Write results to a CSV file\n",
    "    #         formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test])\n",
    "    #         formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "    #         y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "    #         y_preds = pd.DataFrame(y_pred, columns=['preds'])\n",
    "    #         results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "    #         results.to_csv(f'Results2/Preliminary{loc}_{trgt}_res{res}D_{time}{sequence_length}_LGBMRegressor_{season}_results.csv', index=False)\n",
    "        print(f'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672ff89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41a7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Len:Target:Feature:Season:RMSEtesting\n",
      "70:T0(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 3.7810662351265405\n",
      "70:T0(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 2.0453983357076178\n",
      "70:T0(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 1.3241806649452\n",
      "70:T0(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 2.3962388597994617\n",
      "70:T16(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 3.9520605785677874\n",
      "70:T16(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 2.4227915133197953\n",
      "70:T16(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 1.4901969207222545\n",
      "70:T16(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 3.2788997428741755\n",
      "70:T31(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 2.728084109155547\n",
      "70:T31(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 2.6080250119825816\n",
      "70:T31(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 1.6721107911247897\n",
      "70:T31(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 2.1090294904168108\n",
      "70:T46(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 2.0220050040800666\n",
      "70:T46(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 1.5785322265789006\n",
      "70:T46(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 2.0184596113208744\n",
      "70:T46(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 0.4985494525748675\n",
      "70:T76(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 2.137809466461471\n",
      "70:T76(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 1.3758278683331084\n",
      "70:T76(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 0.4346203505028054\n",
      "70:T76(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 0.1306954662247218\n",
      "70:T97(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Winter: 1.928491099704652\n",
      "70:T97(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Spring: 1.2769619499536453\n",
      "70:T97(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Summer: 0.41622581303033307\n",
      "70:T97(res=14D):['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']:Autumn: 0.03896427631816716\n"
     ]
    }
   ],
   "source": [
    "#Baseline: features for different lengths...\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# List of features to use\n",
    "features = ['SNODP', 'SWGDN', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "# resolution\n",
    "res = 7\n",
    "# sequence length\n",
    "for sequence_length in [70]:#[1,4,7,10,14,21,28,35,49,56,63,70]\n",
    "    print(f'\\n\\n\\n\\n\\nLen:Target:Feature:Season:RMSEtesting')\n",
    "    # Prepare sequences\n",
    "    for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "        for season in ['Winter','Spring','Summer','Autumn']:\n",
    "\n",
    "            X, y, _ = prepare_sequences(sequence_length, trgt, features, season, res)\n",
    "        #     print(X.shape)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "            # Train final LightGBM model using fused features\n",
    "            model_final = LGBMRegressor(verbose=0,force_col_wise=True)\n",
    "            model_final.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model_final.predict(X_test.reshape(X_test.shape[0], -1))  # Reshape the data to 2D if needed\n",
    "\n",
    "            # Evaluate the model\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            print(f'{sequence_length}:{trgt}(res={res}D):{features}:{season}: {rmse_test}')\n",
    "\n",
    "            # Write results to a CSV file\n",
    "            formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test])\n",
    "            formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "            y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "            y_preds = pd.DataFrame(y_pred, columns=['preds'])\n",
    "            results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "            if res == 1:\n",
    "                RES = ''\n",
    "            else:\n",
    "                RES = '_res'+ str(res) + 'D'\n",
    "            results.to_csv(f'Results2/Preliminary_{trgt}{RES}_SNODP{sequence_length}_LGBMRegressor_{season}_results.csv', index=False)\n",
    "    # #PLOT\n",
    "    # # Assuming model_final is your trained LightGBM model\n",
    "    # get_lgbm_varimp(model_final, sequence_length, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46daa22",
   "metadata": {},
   "source": [
    "### decomposed process many features : proposed technique searching for best features using GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfc2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modellearning(sequence_length, trgt, selected_features, season, res):\n",
    "    X, y, _ = prepare_sequences(sequence_length, trgt, selected_features, season, res)\n",
    "#     print(X.shape)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Train final LightGBM model using fused features\n",
    "    model_final = LGBMRegressor(verbose=0,force_col_wise=True)\n",
    "    model_final.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Reshape the data to 2D if needed\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model_final.predict(X_test.reshape(X_test.shape[0], -1))  # Reshape the data to 2D if needed\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888ff4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Start optimization: 14, Autumn\n",
      "14, Autumn, T0, ['T2M', 'GHTSKIN', 'TLML', 'EVPSOIL', 'SLP'], 1.5020732696048098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Autumn, T16, ['Tair', 'SWGDN', 'SWLAND', 'GHTSKIN', 'TLML', 'EVPSOIL'], 2.666874994938504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Autumn, T31, ['SWGDN', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP'], 1.5929082535204033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Autumn, T46, ['Tair', 'SWGDN', 'SPEED', 'QV2M', 'SLP'], 0.4307669851797162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Autumn, T76, ['Tair', 'SWGDN', 'T2M', 'GHTSKIN', 'SPEED', 'EVPSOIL'], 0.08540888786574889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Autumn, T97, ['Tair', 'SWGDN', 'SWLAND', 'GHTSKIN', 'TLML', 'EVPSOIL'], 0.018879247746467896\n",
      "*** Start optimization: 14, Spring"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Spring, T0, ['SNODP', 'SWGDN', 'LWGAB', 'T2M', 'SWLAND', 'TLML', 'EVPSOIL', 'SLP'], 1.67961768484642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14, Spring, T16, ['LWGAB', 'T2M', 'SWLAND', 'TLML', 'LWLAND', 'QV2M', 'SLP'], 2.0510703472759326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# !pip install deap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Dictionary to store RMSE for evaluated feature combinations\n",
    "evaluated_combinations = {}\n",
    "\n",
    "# Objective function for GA\n",
    "def evalOneMax(individual, all_feature_names, target, sequence_length, season, res):\n",
    "    selected_features = [all_feature_names[i] for i, is_selected in enumerate(individual) if is_selected]\n",
    "    selected_features_tuple = tuple(sorted(selected_features))\n",
    "    \n",
    "    # Ensure at least 1 feature are selected\n",
    "    if len(selected_features) < 0:\n",
    "        return float('inf'),\n",
    "    \n",
    "    # Always proceed with model training, bypassing the cache check\n",
    "    rmse = modellearning(sequence_length, target, selected_features, season, res)\n",
    "\n",
    "    return rmse,\n",
    "\n",
    "def optimize_features(target, sequence_length, season, res, nmbr_population, nmbr_generation, known_good_features=None):\n",
    "    best_features = []  # Initialize to an empty list\n",
    "    best_rmse = float('inf')  # Initialize to positive infinity\n",
    "    \n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(all_feature_names))\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    \n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evalOneMax, all_feature_names=all_feature_names, target=target, sequence_length=sequence_length, season=season, res=res)\n",
    "\n",
    "    population = toolbox.population(n=nmbr_population)\n",
    "\n",
    "    # Seed the population with a known good individual, if provided\n",
    "    if known_good_features:\n",
    "        good_individual = [0] * len(all_feature_names)\n",
    "        for feature in known_good_features:\n",
    "            idx = all_feature_names.index(feature)\n",
    "            good_individual[idx] = 1\n",
    "        population[0] = creator.Individual(good_individual)\n",
    "        population[0].fitness.values = evalOneMax(population[0], all_feature_names, target, sequence_length, season, res)\n",
    "\n",
    "    algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=nmbr_generation, verbose=False)\n",
    "\n",
    "    # Extract the best individual\n",
    "    best_ind = tools.selBest(population, k=1)[0]\n",
    "    best_features = [all_feature_names[i] for i, is_selected in enumerate(best_ind) if is_selected]\n",
    "    best_rmse = best_ind.fitness.values[0]\n",
    "    \n",
    "    return best_features, best_rmse\n",
    "\n",
    "# Your data loading here\n",
    "res = 14 #change this: 1, 7, 14, 30\n",
    "sequence_length = 1\n",
    "all_feature_names = ['Tair', 'SNODP', 'SWGDN', 'LWGAB', 'T2M', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'TSH', 'EVPSOIL', 'LWLAND', 'TS', 'QV2M', 'SLP']\n",
    "known_good_features = ['SNODP','Tair']  # Replace with your own known good features\n",
    "nmbr_population = 500\n",
    "nmbr_generation = 10\n",
    "\n",
    "## Run optimization\n",
    "for season in ['Autumn', 'Spring', 'Summer', 'Winter']:#\n",
    "    print(f'*** Start optimization: {res}, {season}', end='')\n",
    "    for target in ['T0', 'T16', 'T31', 'T46', 'T76', 'T97']:\n",
    "        best_features, best_rmse = optimize_features(target, sequence_length, season, res, nmbr_population, nmbr_generation, known_good_features)\n",
    "        print(f\"\\n{res}, {season}, {target}, {best_features}, {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e469e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Start optimization: 7, Summer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T0, ['Tair', 'LWGAB', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'TSH', 'TS', 'QV2M'], 0.6777126076769224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T16, ['SNODP', 'SWGDN', 'LWGAB', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TS', 'QV2M'], 0.868811191537385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T31, ['Tair', 'LWGAB', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TSH', 'TS'], 1.166465731297291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T46, ['GHTSKIN', 'LWLAND', 'QV2M'], 1.3579162669544054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T76, ['Tair', 'SNODP', 'SWGDN', 'GHTSKIN', 'HFLUX', 'EVPSOIL', 'LWLAND', 'SLP'], 0.33132696426984953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/mohamed.ahajjam/.local/lib/python3.9/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7, Summer, T97, ['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'LWLAND'], 0.4177147049853185\n"
     ]
    }
   ],
   "source": [
    "# !pip install deap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Dictionary to store RMSE for evaluated feature combinations\n",
    "evaluated_combinations = {}\n",
    "\n",
    "# Objective function for GA\n",
    "def evalOneMax(individual, all_feature_names, target, sequence_length, season, res):\n",
    "    selected_features = [all_feature_names[i] for i, is_selected in enumerate(individual) if is_selected]\n",
    "    selected_features_tuple = tuple(sorted(selected_features))\n",
    "    \n",
    "    # Ensure at least 1 feature are selected\n",
    "    if len(selected_features) < 0:\n",
    "        return float('inf'),\n",
    "    \n",
    "    # Always proceed with model training, bypassing the cache check\n",
    "    rmse = modellearning(sequence_length, target, selected_features, season, res)\n",
    "\n",
    "    return rmse,\n",
    "\n",
    "def optimize_features(target, sequence_length, season, res, nmbr_population, nmbr_generation, known_good_features=None):\n",
    "    best_features = []  # Initialize to an empty list\n",
    "    best_rmse = float('inf')  # Initialize to positive infinity\n",
    "    \n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(all_feature_names))\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    \n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evalOneMax, all_feature_names=all_feature_names, target=target, sequence_length=sequence_length, season=season, res=res)\n",
    "\n",
    "    population = toolbox.population(n=nmbr_population)\n",
    "\n",
    "    # Seed the population with a known good individual, if provided\n",
    "    if known_good_features:\n",
    "        good_individual = [0] * len(all_feature_names)\n",
    "        for feature in known_good_features:\n",
    "            idx = all_feature_names.index(feature)\n",
    "            good_individual[idx] = 1\n",
    "        population[0] = creator.Individual(good_individual)\n",
    "        population[0].fitness.values = evalOneMax(population[0], all_feature_names, target, sequence_length, season, res)\n",
    "\n",
    "    algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=nmbr_generation, verbose=False)\n",
    "\n",
    "    # Extract the best individual\n",
    "    best_ind = tools.selBest(population, k=1)[0]\n",
    "    best_features = [all_feature_names[i] for i, is_selected in enumerate(best_ind) if is_selected]\n",
    "    best_rmse = best_ind.fitness.values[0]\n",
    "    \n",
    "    return best_features, best_rmse\n",
    "\n",
    "# Your data loading here\n",
    "res = 7 #change this: 1, 7, 14, 30\n",
    "sequence_length = 1\n",
    "all_feature_names = ['Tair', 'SNODP', 'SWGDN', 'LWGAB', 'T2M', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'TSH', 'EVPSOIL', 'LWLAND', 'TS', 'QV2M', 'SLP']\n",
    "known_good_features = ['SNODP','Tair']  # Replace with your own known good features\n",
    "nmbr_population = 500\n",
    "nmbr_generation = 10\n",
    "\n",
    "## Run optimization\n",
    "for season in ['Summer']:#['Autumn', 'Spring', 'Summer']:#, 'Winter'\n",
    "    print(f'*** Start optimization: {res}, {season}', end='')\n",
    "    for target in ['T0', 'T16', 'T31', 'T46', 'T76', 'T97']:\n",
    "        best_features, best_rmse = optimize_features(target, sequence_length, season, res, nmbr_population, nmbr_generation, known_good_features)\n",
    "        print(f\"\\n{res}, {season}, {target}, {best_features}, {best_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc39e3",
   "metadata": {},
   "source": [
    "### Results using optimized features with different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22616a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestfeatures_Weekly(month, trgt):\n",
    "    if month == 'Autumn':\n",
    "        if trgt == 'T0':\n",
    "            features = ['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']\n",
    "            return features\n",
    "        elif trgt == 'T16':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T31':\n",
    "            features = ['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T46':\n",
    "            features = ['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T76':\n",
    "            features = ['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']\n",
    "            return features\n",
    "        elif trgt == 'T97':\n",
    "            features = ['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']\n",
    "            return features\n",
    "\n",
    "    if month == 'Spring':\n",
    "        if trgt == 'T0':\n",
    "            features = ['Tair', 'SNODP', 'LWGAB', 'T2M', 'TSH', 'EVPSOIL', 'LWLAND', 'TS', 'QV2M']\n",
    "            return features\n",
    "        elif trgt == 'T16':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'LWGAB', 'T2M', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'LWLAND', 'QV2M', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T31':\n",
    "            features = ['T2M', 'SPEED', 'TLML', 'EVPSOIL', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T46':\n",
    "            features = ['Tair', 'SNODP', 'LWGAB', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL', 'LWLAND', 'TS', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T76':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'TSH', 'EVPSOIL', 'TS', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T97':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'EVPSOIL', 'LWLAND', 'TS', 'SLP']\n",
    "            return features   \n",
    "\n",
    "\n",
    "    if month == 'Summer':\n",
    "        if trgt == 'T0':\n",
    "            features = ['Tair', 'LWGAB', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'TSH', 'TS', 'QV2M']\n",
    "            return features\n",
    "        elif trgt == 'T16':\n",
    "            features = ['SNODP', 'SWGDN', 'LWGAB', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TS', 'QV2M']\n",
    "            return features\n",
    "        elif trgt == 'T31':\n",
    "            features = ['Tair', 'LWGAB', 'SWLAND', 'GHTSKIN', 'HFLUX', 'SPEED', 'TSH', 'TS']\n",
    "            return features\n",
    "        elif trgt == 'T46':\n",
    "            features = ['GHTSKIN', 'LWLAND', 'QV2M']\n",
    "            return features\n",
    "        elif trgt == 'T76':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'GHTSKIN', 'HFLUX', 'EVPSOIL', 'LWLAND', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T97':\n",
    "            features = ['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'LWLAND']\n",
    "            return features   \n",
    "    \n",
    "    if month == 'Winter':\n",
    "        if trgt == 'T0':\n",
    "            features = ['SWGDN', 'SWLAND', 'TLML', 'EVPSOIL', 'TS', 'QV2M']\n",
    "            return features\n",
    "        elif trgt == 'T16':\n",
    "            features = ['SWGDN', 'SWLAND', 'TSH', 'EVPSOIL', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T31':\n",
    "            features = ['Tair', 'SWGDN', 'LWGAB', 'SPEED', 'LWLAND', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T46':\n",
    "            features = ['SNODP', 'LWGAB', 'T2M', 'SWLAND', 'TLML', 'SLP']\n",
    "            return features\n",
    "        elif trgt == 'T76':\n",
    "            features = ['SLP']\n",
    "            return features\n",
    "        elif trgt == 'T97':\n",
    "            features = ['Tair', 'HFLUX', 'SPEED', 'LWLAND', 'SLP']\n",
    "            return features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d74e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution:Season:Target:Feature:RMSEtesting\n",
      "Autumn:1:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:1.6364512974271255:0.9545329642938375\n",
      "Autumn:1:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.4547241138174876:0.8329569205896457\n",
      "Autumn:1:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:1.7737432503970871:0.7140919236479909\n",
      "Autumn:1:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.4656884548847183:0.7087466093234069\n",
      "Autumn:1:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.08994768173619504:0.5164165137828638\n",
      "Autumn:1:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.020939451954835916:0.007806056824202101\n",
      "Autumn:4:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.019706677816514:0.9307245820261683\n",
      "Autumn:4:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.6874445734490737:0.8003556167558348\n",
      "Autumn:4:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:1.841529150071638:0.6928765907336413\n",
      "Autumn:4:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.48420182110306925:0.686204198835308\n",
      "Autumn:4:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.07855188325016618:0.6324242439619276\n",
      "Autumn:4:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.02769446852816747:-0.7296664292759647\n",
      "Autumn:7:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.2407634089551696:0.9146626052631199\n",
      "Autumn:7:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.6152337565208743:0.8114415059865776\n",
      "Autumn:7:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:1.848244783694616:0.6916890843947329\n",
      "Autumn:7:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.5175743492171516:0.6426807780703278\n",
      "Autumn:7:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.08170115810795633:0.6037003636322176\n",
      "Autumn:7:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.04995106882375811:-4.607559085809798\n",
      "Autumn:10:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.285531783616953:0.9112186177063879\n",
      "Autumn:10:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.6226141556849467:0.8103757499304693\n",
      "Autumn:10:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:1.9091976859312156:0.6710183145416306\n",
      "Autumn:10:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.46935834268894583:0.7061538711940578\n",
      "Autumn:10:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.10134919711246652:0.3901712800844258\n",
      "Autumn:10:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.034987667355658995:-1.7511499696803892\n",
      "Autumn:14:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.5862770931038495:0.886069334271854\n",
      "Autumn:14:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.745227372050807:0.7927168984479281\n",
      "Autumn:14:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:2.046555451668079:0.6232578509429438\n",
      "Autumn:14:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.4468002896856224:0.7346243301826734\n",
      "Autumn:14:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.1279993324423646:0.030597591689531356\n",
      "Autumn:14:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.0357479474750816:-1.8622626861236764\n",
      "Autumn:21:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.475997758401405:0.8950704664925205\n",
      "Autumn:21:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:2.8461276657402568:0.7775565921134701\n",
      "Autumn:21:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:2.0951961885813457:0.6064322977703953\n",
      "Autumn:21:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.5004404806222121:0.6681991148429127\n",
      "Autumn:21:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.15387665085672722:-0.3961749115060631\n",
      "Autumn:21:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.021460282783520378:-0.028035484758414997\n",
      "Autumn:28:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.3371994294181095:0.9048086035992096\n",
      "Autumn:28:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:3.083805078182595:0.7388766150026489\n",
      "Autumn:28:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:2.2617426755015715:0.5440588990884412\n",
      "Autumn:28:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.5461872471162754:0.607301422331092\n",
      "Autumn:28:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.13251421587318427:-0.02825364840730482\n",
      "Autumn:28:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.023824496258197412:-0.2588719043220289\n",
      "Autumn:35:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.421805930495645:0.8965579767664643\n",
      "Autumn:35:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:3.4249139985107195:0.6774611901631349\n",
      "Autumn:35:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:2.2234337773470627:0.560475381096593\n",
      "Autumn:35:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.6131747316939244:0.5065452607972092\n",
      "Autumn:35:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.16237019637585792:-0.5383971345053564\n",
      "Autumn:35:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.020748955102942924:0.048217181274723964\n",
      "Autumn:42:T0:['Tair', 'SNODP', 'T2M', 'GHTSKIN', 'HFLUX', 'SPEED', 'TLML', 'EVPSOIL']:2.4367817191450727:0.8922520857756072\n",
      "Autumn:42:T16:['Tair', 'SNODP', 'SWGDN', 'SWLAND', 'GHTSKIN', 'SPEED', 'TLML', 'TSH', 'SLP']:3.700026757760439:0.6217564939348459\n",
      "Autumn:42:T31:['T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:2.250570854342353:0.5516009118647756\n",
      "Autumn:42:T46:['Tair', 'T2M', 'SWLAND', 'GHTSKIN', 'TLML', 'QV2M', 'SLP']:0.6158175200305182:0.5049380869660522\n",
      "Autumn:42:T76:['Tair', 'SWGDN', 'T2M', 'SWLAND', 'GHTSKIN', 'TS']:0.14481252475724835:-0.21513360549422278\n",
      "Autumn:42:T97:['Tair', 'SNODP', 'LWGAB', 'GHTSKIN', 'TSH', 'LWLAND', 'SLP']:0.033106016154934303:-1.4065368004041758\n"
     ]
    }
   ],
   "source": [
    "#Baseline: TIME+Tair only\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# List of features to use\n",
    "# sequence length\n",
    "sequence_length = 1\n",
    "# resolution\n",
    "res = 7\n",
    "season = 'Autumn'\n",
    "\n",
    "print(f'resolution:Season:Target:Feature:RMSEtesting')\n",
    "for sequence_length in [1,4,7,10,14,21,28,35,42]:#,49,56,63,70\n",
    "    for trgt in ['T0','T16','T31','T46','T76','T97']:\n",
    "    # Prepare sequences\n",
    "        features = bestfeatures_Weekly(season, trgt)\n",
    "        X, y, _ = prepare_sequences(sequence_length, trgt, features, season, res)\n",
    "    #     print(X.shape)\n",
    "\n",
    "        # Split into train and test sets\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        # Train final LightGBM model using fused features\n",
    "        model_final = LGBMRegressor(verbose=0,force_col_wise=True)\n",
    "        model_final.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Reshape the data to 2D if needed\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model_final.predict(X_test.reshape(X_test.shape[0], -1))  # Reshape the data to 2D if needed\n",
    "\n",
    "        # Evaluate the model\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f'{season}:{sequence_length}:{trgt}:{features}:{rmse_test}:{r2}')\n",
    "        # Write results to a CSV file\n",
    "        formatted_dates = np.array(['-'.join(map(str, row)) for row in X_test])\n",
    "        formatted_dates_df = pd.DataFrame(formatted_dates, columns=['Date'])\n",
    "        y_test_df = pd.DataFrame(y_test, columns=['actual'])\n",
    "        y_preds = pd.DataFrame(y_pred, columns=['preds'])\n",
    "        results = pd.concat([formatted_dates_df, y_test_df, y_preds.reset_index(drop=True)], axis=1)\n",
    "        results.to_csv(f'Results2/Preliminary_{trgt}_res{res}D_BestFeatures{sequence_length}_LGBMRegressor_{season}_results.csv', index=False)\n",
    "# #PLOT\n",
    "# # Assuming model_final is your trained LightGBM model\n",
    "# get_lgbm_varimp(model_final, sequence_length, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
